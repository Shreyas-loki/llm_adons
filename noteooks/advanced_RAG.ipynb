{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "fd6bbfc1c2f84fe7af6be70cd43f6a0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bbfd14f0a7964202a54e811869b09d5b",
       "IPY_MODEL_e67629a555c54ef89a50f4a75380361f",
       "IPY_MODEL_c24ea28f20d64610b3e1dc1e6b975d0a"
      ],
      "layout": "IPY_MODEL_350f7f16a16b499897b2007ad39e29ad"
     }
    },
    "bbfd14f0a7964202a54e811869b09d5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f414967bb9c14c989f75b5b754e69be7",
      "placeholder": "​",
      "style": "IPY_MODEL_5fd86c86cc2c47628678c685118cc25f",
      "value": "modules.json: 100%"
     }
    },
    "e67629a555c54ef89a50f4a75380361f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_841c59911f9048cab77fc8d43854fc9f",
      "max": 349,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3c71069535654245bcefcebaf09c1662",
      "value": 349
     }
    },
    "c24ea28f20d64610b3e1dc1e6b975d0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_16311bcf2eb74476997e4980bbc5bd25",
      "placeholder": "​",
      "style": "IPY_MODEL_cf00a122f6a049aba2f4bbdac61266d1",
      "value": " 349/349 [00:00&lt;00:00, 19.8kB/s]"
     }
    },
    "350f7f16a16b499897b2007ad39e29ad": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f414967bb9c14c989f75b5b754e69be7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5fd86c86cc2c47628678c685118cc25f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "841c59911f9048cab77fc8d43854fc9f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c71069535654245bcefcebaf09c1662": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "16311bcf2eb74476997e4980bbc5bd25": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf00a122f6a049aba2f4bbdac61266d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dfc722475cec486a81a57eb646525f15": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_57e3a96a9d424e34a0a991ea6976c0e2",
       "IPY_MODEL_dd8d2fd2afd84491a7621c88ee9c7979",
       "IPY_MODEL_76c450cb72874c7c884edc1318762b57"
      ],
      "layout": "IPY_MODEL_c99795ebd870403eb98edfc13cb1f5f8"
     }
    },
    "57e3a96a9d424e34a0a991ea6976c0e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_51f0327f2e734de78e88456b1f65abd5",
      "placeholder": "​",
      "style": "IPY_MODEL_136702ae14604d36a0fc44c0de6779a8",
      "value": "config_sentence_transformers.json: 100%"
     }
    },
    "dd8d2fd2afd84491a7621c88ee9c7979": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae78914f4dfe4290ae3c2ace3d34410e",
      "max": 116,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4a4271c2cdde4c33a55e6a206fc7483d",
      "value": 116
     }
    },
    "76c450cb72874c7c884edc1318762b57": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_006f585814114e60865b8b9bacaca2cf",
      "placeholder": "​",
      "style": "IPY_MODEL_36b0c1a956494f709b6a468307a33583",
      "value": " 116/116 [00:00&lt;00:00, 2.91kB/s]"
     }
    },
    "c99795ebd870403eb98edfc13cb1f5f8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51f0327f2e734de78e88456b1f65abd5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "136702ae14604d36a0fc44c0de6779a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ae78914f4dfe4290ae3c2ace3d34410e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a4271c2cdde4c33a55e6a206fc7483d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "006f585814114e60865b8b9bacaca2cf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "36b0c1a956494f709b6a468307a33583": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1d67315d670d4f0eab8ec1cad84427c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e7ba9903f81841608486361821e875b2",
       "IPY_MODEL_0e5fc71856304fb7bd922505671edc34",
       "IPY_MODEL_440f8e1c7bbe417bb06c22bc4c75af35"
      ],
      "layout": "IPY_MODEL_98758434fab64a7da1bea788df9b40f7"
     }
    },
    "e7ba9903f81841608486361821e875b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_955bdfc10aeb46719c4d69d152b4613c",
      "placeholder": "​",
      "style": "IPY_MODEL_8da4c3a3c9924529a4dffe91cb655bd5",
      "value": "README.md: 100%"
     }
    },
    "0e5fc71856304fb7bd922505671edc34": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_88d3d7d195134a8795b0597e9a67a9dc",
      "max": 10659,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4ce904ca105842b78c5a4881d49dc583",
      "value": 10659
     }
    },
    "440f8e1c7bbe417bb06c22bc4c75af35": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_85042b987a4c4f3aae4db8e32d5200fe",
      "placeholder": "​",
      "style": "IPY_MODEL_67c2dcd549504ee3be0178c63e2c3c77",
      "value": " 10.7k/10.7k [00:00&lt;00:00, 504kB/s]"
     }
    },
    "98758434fab64a7da1bea788df9b40f7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "955bdfc10aeb46719c4d69d152b4613c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8da4c3a3c9924529a4dffe91cb655bd5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "88d3d7d195134a8795b0597e9a67a9dc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ce904ca105842b78c5a4881d49dc583": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "85042b987a4c4f3aae4db8e32d5200fe": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "67c2dcd549504ee3be0178c63e2c3c77": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f4bb31c90a5943b38db70b143b94d37e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_39eeff9971674f009292e7e868e71ed1",
       "IPY_MODEL_6fdbf16ad8e746a78bc16732e8997f2a",
       "IPY_MODEL_387e83c31b824c3fab50cdcd5cc4f40a"
      ],
      "layout": "IPY_MODEL_1ed10c13aac24d12a2c2524ee7f61270"
     }
    },
    "39eeff9971674f009292e7e868e71ed1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c202fcaa8b3644b2afeee48a2cc483bb",
      "placeholder": "​",
      "style": "IPY_MODEL_33073b6485c443ba8f03f4a94b123efb",
      "value": "sentence_bert_config.json: 100%"
     }
    },
    "6fdbf16ad8e746a78bc16732e8997f2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_51529750ed0a47b09bc79809a533ba3a",
      "max": 53,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c89294a8f1ff4ab0bdfa396de708b9c8",
      "value": 53
     }
    },
    "387e83c31b824c3fab50cdcd5cc4f40a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_08c114cb38df47c9bee0ba89341cc1b1",
      "placeholder": "​",
      "style": "IPY_MODEL_4d98188eeb8047edbea5dd203ff26127",
      "value": " 53.0/53.0 [00:00&lt;00:00, 2.99kB/s]"
     }
    },
    "1ed10c13aac24d12a2c2524ee7f61270": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c202fcaa8b3644b2afeee48a2cc483bb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33073b6485c443ba8f03f4a94b123efb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "51529750ed0a47b09bc79809a533ba3a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c89294a8f1ff4ab0bdfa396de708b9c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "08c114cb38df47c9bee0ba89341cc1b1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d98188eeb8047edbea5dd203ff26127": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e96dad20b25b43b0b183b7e6a88172e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9192cecf70f544c3aca44b9a9fafce46",
       "IPY_MODEL_5e0735841d8849d38cac572938c032d4",
       "IPY_MODEL_5a422e4c4a3e43cd9b80d54493e8ca86"
      ],
      "layout": "IPY_MODEL_ccf2a6bc73a3405e9f5e05c39a0d45dc"
     }
    },
    "9192cecf70f544c3aca44b9a9fafce46": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e240c3f00ffe40a6962fbe2dbd8a1207",
      "placeholder": "​",
      "style": "IPY_MODEL_253abe4840384150a0c417e01d1df19e",
      "value": "config.json: 100%"
     }
    },
    "5e0735841d8849d38cac572938c032d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_36fdaad9e9ac4b14b0a1dfb503d5a8f0",
      "max": 612,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_54547bca3cc04ba08ac5ffeabafd00ca",
      "value": 612
     }
    },
    "5a422e4c4a3e43cd9b80d54493e8ca86": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_06de9438f3bf477f90657ba8599d6e17",
      "placeholder": "​",
      "style": "IPY_MODEL_3d25117c9462491da85ead1ec5de1500",
      "value": " 612/612 [00:00&lt;00:00, 31.7kB/s]"
     }
    },
    "ccf2a6bc73a3405e9f5e05c39a0d45dc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e240c3f00ffe40a6962fbe2dbd8a1207": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "253abe4840384150a0c417e01d1df19e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "36fdaad9e9ac4b14b0a1dfb503d5a8f0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54547bca3cc04ba08ac5ffeabafd00ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "06de9438f3bf477f90657ba8599d6e17": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d25117c9462491da85ead1ec5de1500": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "645c3773bd2542b38ecc125dcbce0669": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_81c4c77e97ec42ada9f37bb7d7564bdc",
       "IPY_MODEL_87d6fe7544f44ab8a65889c410275365",
       "IPY_MODEL_5a0ba428ea274147923513f279640093"
      ],
      "layout": "IPY_MODEL_a7d6659804864b53a2f955da97963967"
     }
    },
    "81c4c77e97ec42ada9f37bb7d7564bdc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_09ca5e9c1a464051b2074557beedc73a",
      "placeholder": "​",
      "style": "IPY_MODEL_34ba3676c2e14d96910459adb5750133",
      "value": "model.safetensors: 100%"
     }
    },
    "87d6fe7544f44ab8a65889c410275365": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9da3d525e8114ff7934d453d6b662ed3",
      "max": 90868376,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e2d3447517e44dce981028170a706a5c",
      "value": 90868376
     }
    },
    "5a0ba428ea274147923513f279640093": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_277eb9eedb4d48bb81cceac853eb82b7",
      "placeholder": "​",
      "style": "IPY_MODEL_a82c1c36eefd4ecaa8d228edba0f71b7",
      "value": " 90.9M/90.9M [00:01&lt;00:00, 110MB/s]"
     }
    },
    "a7d6659804864b53a2f955da97963967": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "09ca5e9c1a464051b2074557beedc73a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34ba3676c2e14d96910459adb5750133": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9da3d525e8114ff7934d453d6b662ed3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2d3447517e44dce981028170a706a5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "277eb9eedb4d48bb81cceac853eb82b7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a82c1c36eefd4ecaa8d228edba0f71b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6933abdfb57b4c1ea45b69c6bc3c425d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_088e3b247415450894c23cc9125b9feb",
       "IPY_MODEL_cabeb0e3c3e3423393675d59d8daef26",
       "IPY_MODEL_5d728bd96baa4fab9a1afc92512ce88d"
      ],
      "layout": "IPY_MODEL_ba090554ff1d445991e9df60b132dbd1"
     }
    },
    "088e3b247415450894c23cc9125b9feb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e71b1f3f95d8496ead20d05dd1601994",
      "placeholder": "​",
      "style": "IPY_MODEL_19ef243bc1bf4426af0785639073c144",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "cabeb0e3c3e3423393675d59d8daef26": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_704ff26ea3ff40c0b8278b03efbb8094",
      "max": 350,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2509797a30c34bf492cb67c4dd140873",
      "value": 350
     }
    },
    "5d728bd96baa4fab9a1afc92512ce88d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_87641872309b41b6a1c6b877326210f8",
      "placeholder": "​",
      "style": "IPY_MODEL_3fa3614c16e6448cadd5b2bdabd48262",
      "value": " 350/350 [00:00&lt;00:00, 14.6kB/s]"
     }
    },
    "ba090554ff1d445991e9df60b132dbd1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e71b1f3f95d8496ead20d05dd1601994": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "19ef243bc1bf4426af0785639073c144": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "704ff26ea3ff40c0b8278b03efbb8094": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2509797a30c34bf492cb67c4dd140873": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "87641872309b41b6a1c6b877326210f8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3fa3614c16e6448cadd5b2bdabd48262": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ca9385e899de4a50b10bc98b970dee40": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ce42191af3384974a3413da9a6115402",
       "IPY_MODEL_e042677cfa444564965fcf2b2b5870d2",
       "IPY_MODEL_32c3f6e2390a4158b554f199063c0f30"
      ],
      "layout": "IPY_MODEL_b46aba827d954311939cc8f6ed855882"
     }
    },
    "ce42191af3384974a3413da9a6115402": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_53eae9e53ac14b38a7698d0adc6d4bec",
      "placeholder": "​",
      "style": "IPY_MODEL_ff0cefd718dc4175b50c799097f477ba",
      "value": "vocab.txt: 100%"
     }
    },
    "e042677cfa444564965fcf2b2b5870d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1f4b3ef0d9240b2b287a3e2599bc1e1",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_88fd24b60e05404699eb97ba9680da7b",
      "value": 231508
     }
    },
    "32c3f6e2390a4158b554f199063c0f30": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a395a321c964cc798574186293c634b",
      "placeholder": "​",
      "style": "IPY_MODEL_41c6b50a469344d3ab9854015c3dd61b",
      "value": " 232k/232k [00:00&lt;00:00, 2.58MB/s]"
     }
    },
    "b46aba827d954311939cc8f6ed855882": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "53eae9e53ac14b38a7698d0adc6d4bec": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff0cefd718dc4175b50c799097f477ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b1f4b3ef0d9240b2b287a3e2599bc1e1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88fd24b60e05404699eb97ba9680da7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2a395a321c964cc798574186293c634b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "41c6b50a469344d3ab9854015c3dd61b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0601195ae4f246789073707b21f1ba87": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7b91e17bf1274cdea4d6ef39ac56b0ca",
       "IPY_MODEL_7d1a82b0118d4fa5b307b5f9983d8840",
       "IPY_MODEL_aabfac739901432c8ce2345acc4f2b20"
      ],
      "layout": "IPY_MODEL_57a783a05109434192fee5d5fdaa7139"
     }
    },
    "7b91e17bf1274cdea4d6ef39ac56b0ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c329e86b6ce41779055c1fa5a9f647e",
      "placeholder": "​",
      "style": "IPY_MODEL_33be9a3d6cdb4ba1a4eb786e475e4198",
      "value": "tokenizer.json: 100%"
     }
    },
    "7d1a82b0118d4fa5b307b5f9983d8840": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_18e9492e0f9c4557884e66396875d916",
      "max": 466247,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9c982054a5994fc89e96d23f24abf0f4",
      "value": 466247
     }
    },
    "aabfac739901432c8ce2345acc4f2b20": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_41919a4b393c4dcfbed6ec1483dc129e",
      "placeholder": "​",
      "style": "IPY_MODEL_8110e278bf294a88a7f1eaf2988a80aa",
      "value": " 466k/466k [00:00&lt;00:00, 4.90MB/s]"
     }
    },
    "57a783a05109434192fee5d5fdaa7139": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c329e86b6ce41779055c1fa5a9f647e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33be9a3d6cdb4ba1a4eb786e475e4198": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "18e9492e0f9c4557884e66396875d916": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c982054a5994fc89e96d23f24abf0f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "41919a4b393c4dcfbed6ec1483dc129e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8110e278bf294a88a7f1eaf2988a80aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0766ba76efd841b7a978fe3771642d5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_621ed5b45f024e5ea321dd91a4489389",
       "IPY_MODEL_f2181b6079de4dda98c1bb47606f03b6",
       "IPY_MODEL_4918e01da5064b8e883c9b2497df08bc"
      ],
      "layout": "IPY_MODEL_592383deb2824399ae8085d40d003739"
     }
    },
    "621ed5b45f024e5ea321dd91a4489389": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_64cb91226b414540af0feabadfca321c",
      "placeholder": "​",
      "style": "IPY_MODEL_dccef02b6f764fc2a55b70a837d0b4d7",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "f2181b6079de4dda98c1bb47606f03b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c6f50d812e464b6dac31742a7a49f1bd",
      "max": 112,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_eaa49650da5e42c09372b897d34b8292",
      "value": 112
     }
    },
    "4918e01da5064b8e883c9b2497df08bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c1c37fe438c34e94babdfc3c953562e4",
      "placeholder": "​",
      "style": "IPY_MODEL_d67fc01ccd624721b23dbe197ba9bd3f",
      "value": " 112/112 [00:00&lt;00:00, 5.26kB/s]"
     }
    },
    "592383deb2824399ae8085d40d003739": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64cb91226b414540af0feabadfca321c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dccef02b6f764fc2a55b70a837d0b4d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c6f50d812e464b6dac31742a7a49f1bd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eaa49650da5e42c09372b897d34b8292": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c1c37fe438c34e94babdfc3c953562e4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d67fc01ccd624721b23dbe197ba9bd3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0525b20f7a574ebf8b454eed686140de": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4e6223bc924c466694b0a10892ca1e35",
       "IPY_MODEL_13e2d5c7cac14e0daee1c01a722ebfa8",
       "IPY_MODEL_00d967272fd142eaa5117f6f6d84eee1"
      ],
      "layout": "IPY_MODEL_da71d698eb854fe586e051eb342e6718"
     }
    },
    "4e6223bc924c466694b0a10892ca1e35": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_60dbea28a0cb401fa6603576caeb928a",
      "placeholder": "​",
      "style": "IPY_MODEL_1d705681c9f543599b2088177fc2b87a",
      "value": "1_Pooling/config.json: 100%"
     }
    },
    "13e2d5c7cac14e0daee1c01a722ebfa8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d050f7886a249d3bdf5808dd8a96342",
      "max": 190,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_70ff369cf7ea4264ab0d6374aa5fe2fd",
      "value": 190
     }
    },
    "00d967272fd142eaa5117f6f6d84eee1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ebdf1e4e5cd3489f93ee71bc82077607",
      "placeholder": "​",
      "style": "IPY_MODEL_184e556219604fdab040433511d090be",
      "value": " 190/190 [00:00&lt;00:00, 8.45kB/s]"
     }
    },
    "da71d698eb854fe586e051eb342e6718": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "60dbea28a0cb401fa6603576caeb928a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d705681c9f543599b2088177fc2b87a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5d050f7886a249d3bdf5808dd8a96342": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70ff369cf7ea4264ab0d6374aa5fe2fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ebdf1e4e5cd3489f93ee71bc82077607": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "184e556219604fdab040433511d090be": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "G8FbuyAUUCyJ",
    "outputId": "3fb21923-144c-42e5-a598-dbc0ef9d582a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.1.21-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting langchainhub\n",
      "  Downloading langchainhub-0.1.21-py3-none-any.whl.metadata (659 bytes)\n",
      "Collecting langchain-chroma\n",
      "  Downloading langchain_chroma-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting langchain-text-splitters\n",
      "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting langchain-cohere\n",
      "  Downloading langchain_cohere-0.2.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-1.40.6-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting pypdf\n",
      "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting ragas\n",
      "  Downloading ragas-0.1.13-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting xmltodict\n",
      "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting rank_bm25\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.29 (from langchain-openai)\n",
      "  Downloading langchain_core-0.2.30-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchainhub) (24.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchainhub) (2.32.3)\n",
      "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
      "  Downloading types_requests-2.32.0.20240712-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting chromadb<0.6.0,>=0.4.0 (from langchain-chroma)\n",
      "  Downloading chromadb-0.5.5-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting fastapi<1,>=0.95.2 (from langchain-chroma)\n",
      "  Downloading fastapi-0.112.0-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-chroma) (1.26.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.32)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.10.1)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting langchain<0.3.0,>=0.2.13 (from langchain-community)\n",
      "  Downloading langchain-0.2.13-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.0 (from langchain-community)\n",
      "  Downloading langsmith-0.1.99-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-community)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting cohere<6.0,>=5.5.6 (from langchain-cohere)\n",
      "  Downloading cohere-5.8.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting langchain-experimental>=0.0.6 (from langchain-cohere)\n",
      "  Downloading langchain_experimental-0.0.64-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: pandas>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from langchain-cohere) (2.1.4)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from langchain-cohere) (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.42.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.5)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
      "Collecting datasets (from ragas)\n",
      "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting pysbd>=0.3.4 (from ragas)\n",
      "  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ragas) (1.6.0)\n",
      "Collecting appdirs (from ragas)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.3.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.2.1)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading posthog-3.5.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_api-1.26.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.47b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_sdk-1.26.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.19.1)\n",
      "Collecting pypika>=0.48.9 (from chromadb<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m67.3/67.3 kB\u001B[0m \u001B[31m3.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build wheel ... \u001B[?25l\u001B[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting overrides>=7.3.1 (from chromadb<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (6.4.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.64.1)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.12.3)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading kubernetes-30.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting orjson>=3.9.12 (from chromadb<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m50.4/50.4 kB\u001B[0m \u001B[31m3.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting boto3<2.0.0,>=1.34.0 (from cohere<6.0,>=5.5.6->langchain-cohere)\n",
      "  Downloading boto3-1.34.160-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting fastavro<2.0.0,>=1.9.4 (from cohere<6.0,>=5.5.6->langchain-cohere)\n",
      "  Downloading fastavro-1.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting httpx-sse==0.4.0 (from cohere<6.0,>=5.5.6->langchain-cohere)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting parameterized<0.10.0,>=0.9.0 (from cohere<6.0,>=5.5.6->langchain-cohere)\n",
      "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.10/dist-packages (from cohere<6.0,>=5.5.6->langchain-cohere) (2.20.1)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting starlette<0.38.0,>=0.37.2 (from fastapi<1,>=0.95.2->langchain-chroma)\n",
      "  Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.6.1)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.29->langchain-openai)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.3->langchain-cohere) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.3->langchain-cohere) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.3->langchain-cohere) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (2.0.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.1)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets->ragas)\n",
      "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->ragas)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting xxhash (from datasets->ragas)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->ragas)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Collecting botocore<1.35.0,>=1.34.160 (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5.6->langchain-cohere)\n",
      "  Downloading botocore-1.34.160-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5.6->langchain-cohere)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5.6->langchain-cohere)\n",
      "  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.1.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb<0.6.0,>=0.4.0->langchain-chroma) (2.0.1)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.29->langchain-openai)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.16.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (2.27.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.3.1)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (3.2.2)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain-chroma) (24.3.25)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain-chroma) (3.20.3)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting importlib-metadata<=8.0.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading importlib_metadata-8.0.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.63.2)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_proto-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.47b0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting opentelemetry-instrumentation==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_instrumentation-0.47b0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-util-http==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading opentelemetry_util_http-0.47b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (71.0.4)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.16.0)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (13.7.1)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading watchfiles-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (5.4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (4.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.0.0,>=6.0->opentelemetry-api>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (3.19.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (2.16.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.6.0)\n",
      "Downloading langchain_openai-0.1.21-py3-none-any.whl (49 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m49.8/49.8 kB\u001B[0m \u001B[31m3.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading langchainhub-0.1.21-py3-none-any.whl (5.2 kB)\n",
      "Downloading langchain_chroma-0.1.2-py3-none-any.whl (9.3 kB)\n",
      "Downloading langchain_community-0.2.12-py3-none-any.whl (2.3 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.3/2.3 MB\u001B[0m \u001B[31m31.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
      "Downloading langchain_cohere-0.2.2-py3-none-any.whl (43 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m43.3/43.3 kB\u001B[0m \u001B[31m3.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m52.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading openai-1.40.6-py3-none-any.whl (361 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m361.3/361.3 kB\u001B[0m \u001B[31m21.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m295.8/295.8 kB\u001B[0m \u001B[31m20.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m227.1/227.1 kB\u001B[0m \u001B[31m17.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading ragas-0.1.13-py3-none-any.whl (162 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m162.1/162.1 kB\u001B[0m \u001B[31m11.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Downloading chromadb-0.5.5-py3-none-any.whl (584 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m584.3/584.3 kB\u001B[0m \u001B[31m32.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.4/2.4 MB\u001B[0m \u001B[31m64.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading cohere-5.8.0-py3-none-any.whl (207 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m207.8/207.8 kB\u001B[0m \u001B[31m14.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading fastapi-0.112.0-py3-none-any.whl (93 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m93.1/93.1 kB\u001B[0m \u001B[31m6.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m75.6/75.6 kB\u001B[0m \u001B[31m5.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m77.9/77.9 kB\u001B[0m \u001B[31m5.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m318.9/318.9 kB\u001B[0m \u001B[31m20.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading langchain-0.2.13-py3-none-any.whl (997 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m997.8/997.8 kB\u001B[0m \u001B[31m43.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading langchain_core-0.2.30-py3-none-any.whl (384 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m384.8/384.8 kB\u001B[0m \u001B[31m22.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading langchain_experimental-0.0.64-py3-none-any.whl (204 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m204.3/204.3 kB\u001B[0m \u001B[31m13.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading langsmith-0.1.99-py3-none-any.whl (140 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m140.4/140.4 kB\u001B[0m \u001B[31m9.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m71.1/71.1 kB\u001B[0m \u001B[31m4.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Downloading types_requests-2.32.0.20240712-py3-none-any.whl (15 kB)\n",
      "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m527.3/527.3 kB\u001B[0m \u001B[31m29.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl (273 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m273.8/273.8 kB\u001B[0m \u001B[31m17.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading boto3-1.34.160-py3-none-any.whl (139 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m139.2/139.2 kB\u001B[0m \u001B[31m10.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m116.3/116.3 kB\u001B[0m \u001B[31m8.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading fastavro-1.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/3.1 MB\u001B[0m \u001B[31m51.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m54.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m49.2/49.2 kB\u001B[0m \u001B[31m3.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m67.6/67.6 kB\u001B[0m \u001B[31m4.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.8/6.8 MB\u001B[0m \u001B[31m83.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading opentelemetry_api-1.26.0-py3-none-any.whl (61 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m61.5/61.5 kB\u001B[0m \u001B[31m4.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl (17 kB)\n",
      "Downloading opentelemetry_proto-1.26.0-py3-none-any.whl (52 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m52.5/52.5 kB\u001B[0m \u001B[31m3.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading opentelemetry_instrumentation_fastapi-0.47b0-py3-none-any.whl (11 kB)\n",
      "Downloading opentelemetry_instrumentation-0.47b0-py3-none-any.whl (29 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.47b0-py3-none-any.whl (15 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl (138 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m138.0/138.0 kB\u001B[0m \u001B[31m10.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading opentelemetry_util_http-0.47b0-py3-none-any.whl (6.9 kB)\n",
      "Downloading opentelemetry_sdk-1.26.0-py3-none-any.whl (109 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m109.5/109.5 kB\u001B[0m \u001B[31m6.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m141.9/141.9 kB\u001B[0m \u001B[31m9.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
      "Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m41.3/41.3 kB\u001B[0m \u001B[31m2.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m39.9/39.9 MB\u001B[0m \u001B[31m15.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m71.9/71.9 kB\u001B[0m \u001B[31m4.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.8/62.8 kB\u001B[0m \u001B[31m4.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m134.8/134.8 kB\u001B[0m \u001B[31m9.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m194.1/194.1 kB\u001B[0m \u001B[31m13.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading botocore-1.34.160-py3-none-any.whl (12.5 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.5/12.5 MB\u001B[0m \u001B[31m64.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m58.3/58.3 kB\u001B[0m \u001B[31m4.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m341.4/341.4 kB\u001B[0m \u001B[31m23.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading importlib_metadata-8.0.0-py3-none-any.whl (24 kB)\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m82.7/82.7 kB\u001B[0m \u001B[31m6.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.4/3.4 MB\u001B[0m \u001B[31m70.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading watchfiles-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m427.7/427.7 kB\u001B[0m \u001B[31m24.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m130.2/130.2 kB\u001B[0m \u001B[31m9.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m46.0/46.0 kB\u001B[0m \u001B[31m2.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hUsing cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m86.8/86.8 kB\u001B[0m \u001B[31m5.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hBuilding wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53726 sha256=7108b8822e9616cb96f511a6226c76838f6e2bc1fc4b6fb894a1c59c4d641217\n",
      "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, monotonic, mmh3, appdirs, xxhash, xmltodict, websockets, uvloop, types-requests, tenacity, rank_bm25, python-dotenv, pysbd, pypdf, pyarrow, parameterized, overrides, orjson, opentelemetry-util-http, opentelemetry-proto, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, marshmallow, jsonpointer, jmespath, jiter, importlib-metadata, humanfriendly, httpx-sse, httptools, h11, fastavro, dill, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, uvicorn, typing-inspect, tiktoken, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, langchainhub, jsonpatch, httpcore, coloredlogs, botocore, s3transfer, opentelemetry-semantic-conventions, opentelemetry-instrumentation, onnxruntime, nvidia-cusolver-cu12, langsmith, kubernetes, httpx, fastapi, dataclasses-json, opentelemetry-sdk, opentelemetry-instrumentation-asgi, openai, langchain-core, datasets, boto3, sentence-transformers, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-grpc, langchain-text-splitters, langchain-openai, cohere, langchain, chromadb, langchain-community, langchain-chroma, ragas, langchain-experimental, langchain-cohere\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 9.0.0\n",
      "    Uninstalling tenacity-9.0.0:\n",
      "      Successfully uninstalled tenacity-9.0.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 14.0.2\n",
      "    Uninstalling pyarrow-14.0.2:\n",
      "      Successfully uninstalled pyarrow-14.0.2\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib_metadata 8.2.0\n",
      "    Uninstalling importlib_metadata-8.2.0:\n",
      "      Successfully uninstalled importlib_metadata-8.2.0\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
      "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed appdirs-1.4.4 asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.0 boto3-1.34.160 botocore-1.34.160 chroma-hnswlib-0.7.6 chromadb-0.5.5 cohere-5.8.0 coloredlogs-15.0.1 dataclasses-json-0.6.7 datasets-2.21.0 deprecated-1.2.14 dill-0.3.8 fastapi-0.112.0 fastavro-1.9.5 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 httpx-sse-0.4.0 humanfriendly-10.0 importlib-metadata-8.0.0 jiter-0.5.0 jmespath-1.0.1 jsonpatch-1.33 jsonpointer-3.0.0 kubernetes-30.1.0 langchain-0.2.13 langchain-chroma-0.1.2 langchain-cohere-0.2.2 langchain-community-0.2.12 langchain-core-0.2.30 langchain-experimental-0.0.64 langchain-openai-0.1.21 langchain-text-splitters-0.2.2 langchainhub-0.1.21 langsmith-0.1.99 marshmallow-3.21.3 mmh3-4.1.0 monotonic-1.6 multiprocess-0.70.16 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 onnxruntime-1.18.1 openai-1.40.6 opentelemetry-api-1.26.0 opentelemetry-exporter-otlp-proto-common-1.26.0 opentelemetry-exporter-otlp-proto-grpc-1.26.0 opentelemetry-instrumentation-0.47b0 opentelemetry-instrumentation-asgi-0.47b0 opentelemetry-instrumentation-fastapi-0.47b0 opentelemetry-proto-1.26.0 opentelemetry-sdk-1.26.0 opentelemetry-semantic-conventions-0.47b0 opentelemetry-util-http-0.47b0 orjson-3.10.7 overrides-7.7.0 parameterized-0.9.0 posthog-3.5.0 pyarrow-17.0.0 pypdf-4.3.1 pypika-0.48.9 pysbd-0.3.4 python-dotenv-1.0.1 ragas-0.1.13 rank_bm25-0.2.2 s3transfer-0.10.2 sentence-transformers-3.0.1 starlette-0.37.2 tenacity-8.5.0 tiktoken-0.7.0 types-requests-2.32.0.20240712 typing-inspect-0.9.0 uvicorn-0.30.6 uvloop-0.19.0 watchfiles-0.23.0 websockets-12.0 xmltodict-0.13.0 xxhash-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-openai \\\n",
    "  langchainhub \\\n",
    "  langchain-chroma \\\n",
    "  langchain-community \\\n",
    "  langchain-text-splitters \\\n",
    "  langchain-cohere \\\n",
    "  tiktoken \\\n",
    "  openai \\\n",
    "  python-dotenv \\\n",
    "  pypdf \\\n",
    "  sentence-transformers \\\n",
    "  ragas \\\n",
    "  xmltodict \\\n",
    "  rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Azure OpenAI credentials\n",
    "AZURE_OPENAI_ENDPOINT = \"https://azureopenai16.openai.azure.com/\"\n",
    "AZURE_OPENAI_API_KEY = \"75db73a3b9da40b0b6e0e98273a6029f\"\n",
    "AZURE_OPENAI_API_VERSION = \"2024-02-01\"\n",
    "\n",
    "LLM_MODEL = \"gpt35turbo\"\n",
    "EMBEDDING_MODEL = \"ada0021_6\"\n",
    "\n",
    "COHERE_API_KEY = \"BWR8YyveaadqsWa8Ty0FM0vEIAysgJgnjhZVkRP1\"\n",
    "\n",
    "storage_path = \"./vectordb\"\n",
    "\n",
    "LANGCHAIN_TRACING_V2 = \"true\"\n",
    "LANGCHAIN_API_KEY = \"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\""
   ],
   "metadata": {
    "id": "zkfyAC7aUfkI",
    "ExecuteTime": {
     "end_time": "2024-08-16T07:15:56.294324Z",
     "start_time": "2024-08-16T07:15:56.264294Z"
    }
   },
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "of7sPJ9sWIcl",
    "ExecuteTime": {
     "end_time": "2024-08-16T06:30:49.885804Z",
     "start_time": "2024-08-16T06:30:49.866772Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import chromadb\n",
    "from datasets import Dataset\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from ragas import RunConfig\n",
    "from ragas.metrics import (context_precision, context_recall)\n",
    "\n",
    "from langchain_openai.chat_models import AzureChatOpenAI\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "from ragas import evaluate\n",
    "\n",
    "import os\n",
    "from langchain_chroma import Chroma\n",
    "from datasets import load_dataset\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain_core.documents.base import Document"
   ],
   "metadata": {
    "id": "HGM4igQTWIVF",
    "ExecuteTime": {
     "end_time": "2024-08-16T06:30:55.547890Z",
     "start_time": "2024-08-16T06:30:49.893764Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "  openai_api_type=\"azure\",\n",
    "  openai_api_version=\"2024-02-01\",\n",
    "  openai_api_key=AZURE_OPENAI_API_KEY,\n",
    "  azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "  model=LLM_MODEL,\n",
    "  temperature=0\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528,
     "referenced_widgets": [
      "fd6bbfc1c2f84fe7af6be70cd43f6a0e",
      "bbfd14f0a7964202a54e811869b09d5b",
      "e67629a555c54ef89a50f4a75380361f",
      "c24ea28f20d64610b3e1dc1e6b975d0a",
      "350f7f16a16b499897b2007ad39e29ad",
      "f414967bb9c14c989f75b5b754e69be7",
      "5fd86c86cc2c47628678c685118cc25f",
      "841c59911f9048cab77fc8d43854fc9f",
      "3c71069535654245bcefcebaf09c1662",
      "16311bcf2eb74476997e4980bbc5bd25",
      "cf00a122f6a049aba2f4bbdac61266d1",
      "dfc722475cec486a81a57eb646525f15",
      "57e3a96a9d424e34a0a991ea6976c0e2",
      "dd8d2fd2afd84491a7621c88ee9c7979",
      "76c450cb72874c7c884edc1318762b57",
      "c99795ebd870403eb98edfc13cb1f5f8",
      "51f0327f2e734de78e88456b1f65abd5",
      "136702ae14604d36a0fc44c0de6779a8",
      "ae78914f4dfe4290ae3c2ace3d34410e",
      "4a4271c2cdde4c33a55e6a206fc7483d",
      "006f585814114e60865b8b9bacaca2cf",
      "36b0c1a956494f709b6a468307a33583",
      "1d67315d670d4f0eab8ec1cad84427c5",
      "e7ba9903f81841608486361821e875b2",
      "0e5fc71856304fb7bd922505671edc34",
      "440f8e1c7bbe417bb06c22bc4c75af35",
      "98758434fab64a7da1bea788df9b40f7",
      "955bdfc10aeb46719c4d69d152b4613c",
      "8da4c3a3c9924529a4dffe91cb655bd5",
      "88d3d7d195134a8795b0597e9a67a9dc",
      "4ce904ca105842b78c5a4881d49dc583",
      "85042b987a4c4f3aae4db8e32d5200fe",
      "67c2dcd549504ee3be0178c63e2c3c77",
      "f4bb31c90a5943b38db70b143b94d37e",
      "39eeff9971674f009292e7e868e71ed1",
      "6fdbf16ad8e746a78bc16732e8997f2a",
      "387e83c31b824c3fab50cdcd5cc4f40a",
      "1ed10c13aac24d12a2c2524ee7f61270",
      "c202fcaa8b3644b2afeee48a2cc483bb",
      "33073b6485c443ba8f03f4a94b123efb",
      "51529750ed0a47b09bc79809a533ba3a",
      "c89294a8f1ff4ab0bdfa396de708b9c8",
      "08c114cb38df47c9bee0ba89341cc1b1",
      "4d98188eeb8047edbea5dd203ff26127",
      "e96dad20b25b43b0b183b7e6a88172e7",
      "9192cecf70f544c3aca44b9a9fafce46",
      "5e0735841d8849d38cac572938c032d4",
      "5a422e4c4a3e43cd9b80d54493e8ca86",
      "ccf2a6bc73a3405e9f5e05c39a0d45dc",
      "e240c3f00ffe40a6962fbe2dbd8a1207",
      "253abe4840384150a0c417e01d1df19e",
      "36fdaad9e9ac4b14b0a1dfb503d5a8f0",
      "54547bca3cc04ba08ac5ffeabafd00ca",
      "06de9438f3bf477f90657ba8599d6e17",
      "3d25117c9462491da85ead1ec5de1500",
      "645c3773bd2542b38ecc125dcbce0669",
      "81c4c77e97ec42ada9f37bb7d7564bdc",
      "87d6fe7544f44ab8a65889c410275365",
      "5a0ba428ea274147923513f279640093",
      "a7d6659804864b53a2f955da97963967",
      "09ca5e9c1a464051b2074557beedc73a",
      "34ba3676c2e14d96910459adb5750133",
      "9da3d525e8114ff7934d453d6b662ed3",
      "e2d3447517e44dce981028170a706a5c",
      "277eb9eedb4d48bb81cceac853eb82b7",
      "a82c1c36eefd4ecaa8d228edba0f71b7",
      "6933abdfb57b4c1ea45b69c6bc3c425d",
      "088e3b247415450894c23cc9125b9feb",
      "cabeb0e3c3e3423393675d59d8daef26",
      "5d728bd96baa4fab9a1afc92512ce88d",
      "ba090554ff1d445991e9df60b132dbd1",
      "e71b1f3f95d8496ead20d05dd1601994",
      "19ef243bc1bf4426af0785639073c144",
      "704ff26ea3ff40c0b8278b03efbb8094",
      "2509797a30c34bf492cb67c4dd140873",
      "87641872309b41b6a1c6b877326210f8",
      "3fa3614c16e6448cadd5b2bdabd48262",
      "ca9385e899de4a50b10bc98b970dee40",
      "ce42191af3384974a3413da9a6115402",
      "e042677cfa444564965fcf2b2b5870d2",
      "32c3f6e2390a4158b554f199063c0f30",
      "b46aba827d954311939cc8f6ed855882",
      "53eae9e53ac14b38a7698d0adc6d4bec",
      "ff0cefd718dc4175b50c799097f477ba",
      "b1f4b3ef0d9240b2b287a3e2599bc1e1",
      "88fd24b60e05404699eb97ba9680da7b",
      "2a395a321c964cc798574186293c634b",
      "41c6b50a469344d3ab9854015c3dd61b",
      "0601195ae4f246789073707b21f1ba87",
      "7b91e17bf1274cdea4d6ef39ac56b0ca",
      "7d1a82b0118d4fa5b307b5f9983d8840",
      "aabfac739901432c8ce2345acc4f2b20",
      "57a783a05109434192fee5d5fdaa7139",
      "9c329e86b6ce41779055c1fa5a9f647e",
      "33be9a3d6cdb4ba1a4eb786e475e4198",
      "18e9492e0f9c4557884e66396875d916",
      "9c982054a5994fc89e96d23f24abf0f4",
      "41919a4b393c4dcfbed6ec1483dc129e",
      "8110e278bf294a88a7f1eaf2988a80aa",
      "0766ba76efd841b7a978fe3771642d5b",
      "621ed5b45f024e5ea321dd91a4489389",
      "f2181b6079de4dda98c1bb47606f03b6",
      "4918e01da5064b8e883c9b2497df08bc",
      "592383deb2824399ae8085d40d003739",
      "64cb91226b414540af0feabadfca321c",
      "dccef02b6f764fc2a55b70a837d0b4d7",
      "c6f50d812e464b6dac31742a7a49f1bd",
      "eaa49650da5e42c09372b897d34b8292",
      "c1c37fe438c34e94babdfc3c953562e4",
      "d67fc01ccd624721b23dbe197ba9bd3f",
      "0525b20f7a574ebf8b454eed686140de",
      "4e6223bc924c466694b0a10892ca1e35",
      "13e2d5c7cac14e0daee1c01a722ebfa8",
      "00d967272fd142eaa5117f6f6d84eee1",
      "da71d698eb854fe586e051eb342e6718",
      "60dbea28a0cb401fa6603576caeb928a",
      "1d705681c9f543599b2088177fc2b87a",
      "5d050f7886a249d3bdf5808dd8a96342",
      "70ff369cf7ea4264ab0d6374aa5fe2fd",
      "ebdf1e4e5cd3489f93ee71bc82077607",
      "184e556219604fdab040433511d090be"
     ]
    },
    "id": "Du615qMLV7kl",
    "outputId": "c3cf2214-4b3c-4c0e-c0b2-b8bdabce5e6e",
    "ExecuteTime": {
     "end_time": "2024-08-16T06:31:08.217346Z",
     "start_time": "2024-08-16T06:30:55.899302Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hammer\\Anaconda3\\envs\\llama\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "loader = DirectoryLoader(\"C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\", glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "docs = loader.load()\n",
    "print('.....document_loaded.....')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OBAqdV00VO8U",
    "outputId": "960c1946-1dc7-4e2a-dd32-eb0e0043aa10",
    "ExecuteTime": {
     "end_time": "2024-08-16T06:31:10.007833Z",
     "start_time": "2024-08-16T06:31:08.237348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....document_loaded.....\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "docs"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "QVp9YAbyW2bV",
    "outputId": "7c1a0e51-2a1e-4966-cfdf-c20316f733f6",
    "ExecuteTime": {
     "end_time": "2024-08-16T06:31:12.707119Z",
     "start_time": "2024-08-16T06:31:12.666121Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 0}, page_content='Fully automated hand tracking for Parkinson’s Disease\\nDiagnosis\\nCallum Macpherson\\nStudent ID: 201022895\\nSupervised by Luisa Cutillo, Samuel Relton, and Hui Fang\\nSubmitted in accordance with the requirements for the\\nmodule MATH5872M: Dissertation in Data Science and Analytics\\nas part of the degree of\\nMaster of Science in Data Science and Analytics\\nThe University of Leeds, School of Mathematics\\nSeptember 2021\\nThe candidate conﬁrms that the work submitted is his/her own and that appropriate\\ncredit has been given where reference has been made to the work of others.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 1}, page_content=''),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 2}, page_content='Acknowledgements\\nI would like to thank my supervisors, Samuel Relton, Luisa Cutillo, and Hui Fang, for their\\nguidance and support over the last three months. Their support and enthusiasm have been ex-\\ncellent. Furthermore, I would like to thank ﬁve additional MSc students on this course who\\nassisted with labelling the data discussed in Section 4.4.1: Hanhu Hong, Jiachen Bai, Kunhao\\nLiang, and Siyu An.\\nI would also like to thank Luisa Cutillo, acting as programme leader and the assessors of this\\nmodule, for granting formal permission for the report’s length to exceed 60 pages. Additional\\npages were required as this project used image data, which required additional pages to visualise\\ndata quality issues. Furthermore, to ensure that the study is well explained and repeatable,\\nadditional pages were needed to explain how the multiple data sets were created. Lastly, as\\na multi-stage machine learning pipeline has been proposed, the various algorithms considered\\nneeded to be reviewed and explained in a sufﬁcient level of detail. There are a total of 75 pages\\nin this report, including 4 blank pages between chapters.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 3}, page_content=''),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 4}, page_content='Abstract\\nBackground\\nThe global prevalence of Parkinson’s is on the rise. There is a global shortage of trained neu-\\nrologists who can effectively diagnose the disease, requiring an objective tool for diagnosis.\\nFurther, the need for remote and objective assessment of the underlying symptoms of Parkin-\\nson’s disease has been exacerbated since the COVID-19 pandemic. State-of-the-art methods\\nhave been demonstrated to diagnose Parkinson’s Disease using videos of ﬁnger tapping in con-\\njunction with a pose estimation tool named DeepLabCut. However, currently, a user needs to\\nmanually label 150 video frames before use which serves as a barrier for clinical deployment.\\nMethods\\nAn automatic labelling system is proposed in this study. This solution consists of a machine\\nlearning pipeline that utilises two existing state-of-the-art algorithms which are combined. Firstly,\\nan object detection model is used that operates on a 1080p image and locates the position of a\\nhand using a bounding box. Secondly, a key point regression model operates on a cropped image\\nusing the bounding box coordinates and outputs (x,y) coordinates for the tip of the index ﬁnger\\nand thumb. By reviewing existing state-of-the-art algorithms, the selected models for the object\\ndetection model were YOLOv4 and YOLOv4-tiny. Similarly, the selected feature extractors\\nselected for the key point regression model were EfﬁcientNet-B0 and EfﬁcientNet-B4.\\nResults\\nBoth YOLOv4 and YOLOv4-tiny achieve a mean average precision (mAP) of ∼100% and\\nmean intersection over union (IOU) of ∼85%. YOLOv4-tiny was the preferred object detection\\nmodel with a detection speed of 330 FPS which is 6.68 ×faster than YOLOv4. The EfﬁcientNet-\\nB0 key point regression model achieved a mean pixel error of 13.41 on validation data with an\\ninference speed of 439 FPS. The EfﬁcientNet-B4 model achieved a mean pixel error of 9.95 on\\nvalidation data with an inference speed of 19 FPS. The preferred feature extractor for the key\\npoint regression model was EfﬁcientNet-B4 as it produced a lower pixel error.\\nConclusion\\nThe ﬁnal automatic labelling system pipeline for this report uses YOLOv4-tiny for object detec-\\ntion and an EfﬁcientNet-B4 backbone pre-trained on ImageNet with a customised top for key\\npoint regression to predict coordinates for the tip of the index ﬁnger and thumb. The ﬁnal mean\\npixel error from the pipeline is 9.95 px which is excellent considering that DeepLabCut achieves\\n8.39 px on the same data set. Most of the pixel error caused by the pipeline produced in this\\nreport is likely due to human variability in labelling and scaling down input images.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 5}, page_content=''),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 6}, page_content='Contents\\n1 Introduction 1\\n1.1 Existing Literature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n1.2 Aims and Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n2 Technical Overview 7\\n2.1 Convolutional Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n2.1.1 Convolution Layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.1.2 Pooling Layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\n2.1.3 Fully Connected Layers . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n2.1.4 CNN for multi-class classiﬁcation example . . . . . . . . . . . . . . . 13\\n2.1.5 Activation Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n2.2 Loss Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n2.2.1 Mean Square Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n2.2.2 Intersection over Union (IOU) . . . . . . . . . . . . . . . . . . . . . . 17\\n2.2.3 Mean average Precision (mAP) . . . . . . . . . . . . . . . . . . . . . 18\\n2.3 Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n2.3.1 Generalization Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n2.3.2 Early stopping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n3 Computer Vision: Review of methods 23\\n3.1 Pose Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n3.1.1 Data sets and Data Augmentation . . . . . . . . . . . . . . . . . . . . 24\\n3.1.2 Model Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\n3.1.3 Loss Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\n3.1.4 Optimization Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . 28\\n3.1.5 Chosen method for this study . . . . . . . . . . . . . . . . . . . . . . 29\\n3.2 A brief history of encoder network architectures . . . . . . . . . . . . . . . . . 29\\n3.2.1 LeNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\n3.2.2 AlexNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n3.2.3 VGG-16 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n3.2.4 ResNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\\n3.2.5 EfﬁcientNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n3.3 Object Detection Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\\n3.3.1 Two-stage detection models . . . . . . . . . . . . . . . . . . . . . . . 35\\n3.3.2 One-stage detection models . . . . . . . . . . . . . . . . . . . . . . . 36\\ni'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 7}, page_content='4 Methods 41\\n4.1 Automatic Labelling System: Pipeline . . . . . . . . . . . . . . . . . . . . . . 42\\n4.2 Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\\n4.3 Data Collection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\n4.4 Data Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\\n4.4.1 Labelling key points for DLC . . . . . . . . . . . . . . . . . . . . . . 46\\n4.4.2 Pitfalls of labelling approach and data quality issues . . . . . . . . . . 47\\n4.4.3 Converting DLC Keypoints to Bounding Box Labels . . . . . . . . . . 51\\n4.4.4 Converting DLC Keypoints to data set for Key Point Regression model 56\\n4.5 Bounding Box models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\\n4.5.1 Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\\n4.5.2 YOLOv4: Network Architecture . . . . . . . . . . . . . . . . . . . . . 60\\n4.5.3 YOLOv4-tiny: Network Architecture . . . . . . . . . . . . . . . . . . 61\\n4.5.4 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\\n4.5.5 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\\n4.6 Key point regression models . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\\n4.6.1 Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\\n4.6.2 EfﬁcientNet-B0: Network Architecture . . . . . . . . . . . . . . . . . 63\\n4.6.3 EfﬁcientNet-B4: Network Architecture . . . . . . . . . . . . . . . . . 64\\n4.6.4 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\\n4.6.5 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\\n5 Results 67\\n5.1 Object Detection models for bounding box coordinates . . . . . . . . . . . . . 67\\n5.2 Key Point Regression Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\\n5.2.1 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\\n6 Conclusion 73\\n6.1 Limitations of Study and Suggestions for Future Work . . . . . . . . . . . . . 73\\n6.2 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\\nA Appendix 77\\nA.1 YOLOv4 and YOLOv4-tiny conﬁguration instructions . . . . . . . . . . . . . 77\\nA.2 EfﬁcientNet-B4 Key point regression model predictions . . . . . . . . . . . . . 78\\nii'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 8}, page_content='List of Figures\\n2.1 A basic CNN architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n2.2 The operations of a convolutional layer. The computation on the RHS of the\\nconvoluted feature details the element-wise multiplication between the input\\ndata and the kernel, of which the summation is taken to produce the convoluted\\nfeature (Yin 2018). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.3 How ﬁlters learn visual features from an input image to store in feature maps\\n(DeepLizard 2021). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n2.4 Feature maps at different network depths (Dertat 2017) . . . . . . . . . . . . . 11\\n2.5 Max pooling operation example (Dertat 2017) . . . . . . . . . . . . . . . . . . 12\\n2.6 CNN example for mutli-class classiﬁcation (Swapna 2021) . . . . . . . . . . . 13\\n2.7 The sigmoid function and its derivative . . . . . . . . . . . . . . . . . . . . . . 14\\n2.8 Common activation functions (Kızrak 2020) . . . . . . . . . . . . . . . . . . . 16\\n2.9 Intersection over Union (IOU) . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n2.10 Model Fit: Underﬁtting, Balanced, Overﬁtting (Bhande 2018) . . . . . . . . . 20\\n2.11 Early Stopping Point (PapersWithCode n.d.) . . . . . . . . . . . . . . . . . . . 21\\n3.1 Image Classiﬁcation on ImageNet, ( PaperswithCode 2021 a) . . . . . . . . . . 25\\n3.2 LeNet and AlexNet network architectures . . . . . . . . . . . . . . . . . . . . 30\\n3.3 The residual block (He et al. 2016) . . . . . . . . . . . . . . . . . . . . . . . . 32\\n3.4 EfﬁcientNet performance on ImageNet (Tan & Le 2019). . . . . . . . . . . . . 33\\n3.5 YOLO Method (Redmon et al. 2016). . . . . . . . . . . . . . . . . . . . . . . 37\\n4.1 Current and proposed DeepLabCut Workﬂow . . . . . . . . . . . . . . . . . . 41\\n4.2 Proposed ML Pipeline for the Automatic Labelling System . . . . . . . . . . . 43\\n4.3 Workﬂow of this project . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\\n4.4 Hand Labelling Protocol . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\n4.5 Human variability in labelling (Mathis et al. 2018) . . . . . . . . . . . . . . . . 48\\n4.6 Data quality issues. Left: Soft Tissue Artefact. Right: Self-occlusion. . . . . . 49\\n4.7 Additional data quality issues. Left: Blur. Right: Motion Blur. . . . . . . . . . 50\\n4.8 Frequency distribution of Euclidean distance between index ﬁnger and thumb . 51\\n4.9 Different proportion of original image occupied by hand: Left: Large hand rel-\\native to 1080p image space. Right: Small hand relative to image space. . . . . . 52\\n4.10 Example bounding box ground truth annotation ‘.txt’ ﬁle . . . . . . . . . . . . 53\\n4.11 Deﬁning bounding box dimensions using existing DLC key point labels . . . . 54\\n4.12 Pandas dataframe named ‘yolo df’ containing formatted bounding box values . 55\\n4.13 Issue caused by 768 ×768 bounding box: Index ﬁnger outside of box . . . . . . 56\\n4.14 Converting key points coordinates from original image to cropped image . . . . 58\\n4.15 New coordinates relative to origin of cropped image . . . . . . . . . . . . . . . 59\\niii'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 9}, page_content='5.1 Object Detection Models: Training loss and validation mAP. Left: YOLOv4-\\ntiny. Right: YOLOv4. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\\n5.2 Predictions made by object detection models. Left: YOLOv4-tiny. Right:\\nYOLOv4. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\\n5.3 Key Point Regression Models: Training loss and validation loss. Left: EfﬁcientNet-\\nB0. Right: EfﬁcientNet-B4. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\\nA.1 EfﬁcientNet-B4 Key point regression model: Predictions on Training Data . . . 78\\nA.2 EfﬁcientNet-B4 Key point regression model: Predictions on Test Data . . . . . 79\\niv'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 10}, page_content='List of Tables\\n1.1 Summary of the UPDRS Item 3.4 (Finger Tapping) rating scale (Goetz et al. 2008) 2\\n3.1 Comparing ResNet and Darknet Variants (Redmon & Farhadi 2018) . . . . . . 39\\n4.1 Hand labelling protocol. Each key point is visualised in Figure 4.4. . . . . . . . 47\\n4.2 Bounding box ground truth annotation format . . . . . . . . . . . . . . . . . . 53\\n4.3 Comparison of object detection models used in this study . . . . . . . . . . . . 61\\n4.4 EfﬁcientNet-B0 Network Architecture with custom top for regression. . . . . . 64\\n4.5 EfﬁcientNet-B4 Network Architecture with custom top for regression. . . . . . 65\\nv'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 11}, page_content='vi'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 12}, page_content='Chapter 1\\nIntroduction\\nParkinson’s Disease (PD) is a progressive, complex neurodegenerative disorder that inhibits\\nmovement. Speciﬁcally, it is characterized by several primary motor symptoms, including\\nbradykinesia (slowness of movement), muscle rigidity, a resting tremor, postural instability.\\nWhile the exact cause of the disease is typically unknown, it can be caused by carbon monox-\\nide poisoning, strokes or even drug-induced. The disease is a result of genetic mutations that\\ncause brain cell degeneration, primarily in the midbrain region (Miller 2002). Approximately\\n10 million patients are affected by PD globally, and that the prevalence of this disease is in-\\ncreasing, resulting in an increased demand for trained neurologists to carry out ongoing motor\\nassessments of patients (Zhao et al. 2020, Shahid & Singh 2020). Additionally, there is a global\\nshortage of trained neurologists, resulting in a decreased supply of clinicians to accurately di-\\nagnose PD. Furthermore, since the COVID-19 pandemic, many patients have been unable to\\ncarry out routine in-person assessments as PD patients are typically older and considered ‘high\\nrisk’ and have been formally advised to shield (Sibley et al. 2021). Therefore, there is an urgent\\nneed to develop new tools to objectively measure PD to enable effective widespread diagnosis,\\nmonitoring and future research into the disease (Zhao et al. 2020).\\nFor diagnosing PD, Williams et al. (2020) explains that a principal method used in current\\nclinical practice involves trained neurologists visually observing patients performing a tapping\\nexercise with their index ﬁnger and thumb. This method assesses for bradykinesia, which is\\na cardinal motor feature of the condition that is associated with the slowness of movement,\\ndecreased size and speed of movement, or sudden halts as movements are continued (Williams\\net al. 2020). A patient’s ability to perform this tapping exercise is assessed using two validated\\nclinical rating scales: Item 3.4 of the Uniﬁed Parkinson’s Disease Rating Scale (UPDRS) and the\\nModiﬁed Bradykinesia Rating Scale (MBRS). The UPDRS classiﬁes the severity of PD based\\non the criteria given in Table 1.1, considering ﬁnger tapping speeds, amplitude, and rhythm\\nsimultaneously, resulting in a single score from 0 (no PD) to 4 (severe PD). On the other hand,\\nthe MBRS is made up of three separate scores for speed, amplitude and rhythm (Williams et al.\\n2020).\\n1'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 13}, page_content='Score Description\\n0 - Normal No problems\\n1 - SlightAny of the following:\\n(a) Regular rhythm broken with 1-2 interruptions,\\n(b) slight slowing in tapping speed,\\n(c) amplitude decreases towards end of exercise.\\n2 - MildAny of the following:\\n(a) Regular rhythm broken with 3-5 interruptions,\\n(b) mild slowing in tapping speed,\\n(c) amplitude decreases midway through exercise.\\n3 - ModerateAny of the following:\\n(a) Regular rhythm broken with 6+ interruptions,\\n(b) moderate slowing in tapping speed,\\n(c) amplitude decreases from start.\\n4 - Severe Cannot perform the exercise due to slowing, interruptions or decrements.\\nTable 1.1: Summary of the UPDRS Item 3.4 (Finger Tapping) rating scale (Goetz et al. 2008)\\nZhao et al. (2020) highlights that this method is inherently subjective as the distance between\\nthe index ﬁnger and thumb is not measured and is therefore not objective. Further, due to the\\nlack of objective measures in classifying patients severity of PD, there are high levels of inter-\\nvariability between clinicians (Goetz et al. 2008, Heldman et al. 2011). Several studies have\\nbeen published which objectively measure bradykinesia from ﬁnger-tapping exercises. How-\\never, these include the use of wearable equipment such as gyroscopes (Heldman et al. 2011),\\nelectromagnetic sensors (Sano et al. 2016), infrared camera markers (R ˇziˇcka et al. 2016), or pa-\\ntient interaction with an app (Lee et al. 2016). However, the use of wearable sensors can hinder\\na patients ability to perform the exercise, serving as a barrier for clinical deployment (Zhao et al.\\n2020).\\nModern advances in a subset of machine learning (ML) called deep learning have facilitated\\nmany new technologies within the research area of image analysis, such as image classiﬁca-\\ntion, object detection and pose estimation using convolutional neural networks (CNNs). These\\ntechnologies have been applied to many ﬁelds of medical imaging, such as diagnostics in der-\\nmatology, radiology, and pathology (Esteva et al. 2019). Existing studies have shown that deep\\nlearning tools can be used as a contactless method to quantify measures related to Parkinson’s\\nbradykinesia. These will be explored in the following section.\\n2'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 14}, page_content='1.1 Existing Literature\\nFour recent studies have shown that propose contactless methods to measure PD bradykine-\\nsia in ﬁnger tapping exercises objectively. The study looked at ﬁnger tapping for 13 patients\\nwith advanced PD and 6 controls that simultaneously tracks both the left and right hand using\\na pipeline that is dependent on a face detection system (Khan et al. 2014). A drawback of this\\napproach is that the face detection stage in the ML pipeline is not essential to tracking hands,\\nas demonstrated by later papers. Furthermore, tracking both hands simultaneously may prevent\\npatients from monitoring their own hands if the system were to be deployed for private use. If\\nthe pipeline only tracked one hand, then a patient could hold their phone with their other hand,\\nwhich would not be possible with this system.\\nThe second recent study implemented a computer vision approach that uses segmented im-\\nages to produce an optic ﬂow ﬁeld that identiﬁes areas of the hand that are moving and can\\nextract key point features that are clinically relevant such as the tip of the index ﬁnger and\\nthumb (Williams et al. 2020). This method improves the previous method as it is more practical\\nfor self-monitoring as it only detects and tracks key points. Furthermore, the results in the study\\ncan be considered more robust as they validate the model on a larger cohort. One potential lim-\\nitation of this method is that it requires that neurologists have a background in programming to\\nimplement. The method suggested in the following paragraph does not require a background in\\nprogramming to output objectively measured hand tracking.\\nThe third study considered ﬁnger tapping, as well as hand clasping and hand pro/supination\\n(twisting at the wrists) (Liu et al. 2019). For ﬁnger tapping bradykinesia, the study reported\\nstrong correlations between the MDS-UPRDS and video measures of tapping frequency (r=0.91),\\nfrequency variation (r=0.82), and amplitude (r=-0.94). On the other hand, the study reported a\\nweak correlation between MDS-UPDRS and video measure for amplitude variation (r=0.39).\\nLimitations of this study are that only two trained clinicians gave assessments that were used\\nas ‘ground truth’ for comparison of their computer vision methods. Therefore the ground truth\\ncomparison is less robust than other more recent studies. Furthermore, 24 FPS videos were used\\nthat resulted in motion blur, causing issues in tracking the hand. Also, there were no MBRS rat-\\nings used for comparisons, meaning that a cornerstone of PD bradykinesia assessment has not\\nbeen considered (Zhao et al. 2020).\\nLastly, the fourth study to consider ﬁnger tapping uses deep learning-based pose estimation\\ntechnologies such as DeepLabCut as a contactless method to track key points such as ﬁngertips.\\nThis study used 137 videos of hands (77 PD and 60 control) with 20 frames of each video had\\nkey points labelled such as the tip of the index ﬁnger and thumb. Therefore, approximately\\n2,740 labelled frames were required to train the pose estimation model in DeepLabCut, which\\nachieved a mean absolute error of 8.39 pixels for 1080p videos. A total of 22 trained movement\\n3'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 15}, page_content='disorder neurologists made clinical assessments on all participants based on the MBRS and\\nMDS-UPDRS, meaning that more robust ‘ground truth’ observations existed, and considering\\nboth standard assessment scales, which other studies had not achieved. The study reported good\\nSpearman’s correlation with clinical ratings -0.74 for tapping speed, 0.66 for amplitude, and\\n-0.65 for rhythm on the MBRS, and -0.69 combined for MDS-UPDRS. A further key beneﬁt\\nof this study compared to others is that it uses DeepLabCut, an open-source pose estimation\\nsoftware that uses a GUI without requiring users to produce sophisticated code. Currently, this\\nmakes this method widely available to neurologists that do not have a background in program-\\nming. However, a user currently needs to manually label the index ﬁnger and thumb in video\\nframes before use, which serves as a bottleneck and a barrier to deployment. This paper sug-\\ngests that 2,740 frames were labelled of 6 key points, which likely took several hours. However,\\nthe developers of DeepLabCut have demonstrated that DeepLabCut only requires less than 150\\nframes of human labelled data to track 3D hand movements of a mouse (Nath et al. 2019).\\nNonetheless, the labelling stage of the DeepLabCut workﬂow serves as a signiﬁcant barrier to\\ndeployment in clinical practice.\\nA more recent study has measured bradykinesia in facial expressions using Google’s pose\\nestimation package called ‘MediaPipe’. This study was conducted by Gomez et al. (2021)\\nThe facial expressions that patients were asked to perform were: Happy, Wink, Angry and\\nSurprised. Static and dynamic facial expressions were tracked using 468 3D facial landmarks to\\ndetect hypomimia, which reduces facial expressiveness caused by bradykinesia. There were a\\ntotal of 54 participants in the study, both PD and non-PD participants, and the visual landmark\\ndata was classiﬁed automatically using a support vector machine (SVM) in which a maximum\\nclassiﬁcation accuracy of 77.36% was achieved with the ‘Wink’ gesture (Gomez et al. 2021).\\n1.2 Aims and Objectives\\nThis project aims to automate the labelling stage of the DeepLabCut workﬂow for Parkinson’s\\nDisease Diagnosis. An automatic labelling tool will be designed and tested, which takes an\\nimage of a hand as input and output labels for the coordinates of the tip of the index ﬁnger and\\nthumb.\\nBy producing an automatic labelling system, clinicians will no longer need to manually label\\n150+ frames before using DeepLabCut for tracking the index ﬁnger and thumb for quantifying\\nParkinson’s bradykinesia. Manual labelling serves as a barrier to the widespread clinical deploy-\\nment of DeepLabCut. Producing an automatic labelling system may result in the deployment of\\na quantiﬁable, deep-learning-based system for objectively measuring Parkinson’s bradykinesia.\\nCurrent clinical methods involve visually observing patients performing the exercise, which is\\ntherefore not quantiﬁed and highly subjective. Different clinicians will often classify the same\\n4'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 16}, page_content='patient with a different level of severity of Parkinson’s Disease. Thus, producing a quantiﬁable\\nand objective diagnosis method should result in higher quality diagnosis and treatment plan.\\nThe proposed automatic labelling system will be underpinned by deep-learning-based com-\\nputer vision technology. Existing state-of-the-art open-source computer vision technologies\\nwill be reviewed to assess the feasibility of implementation for use in the proposed automatic\\nlabelling system. This will include considering existing pose estimation and object detection\\nalgorithms and reviewing modern convolutional neural networks as potential feature extractors\\nfor the proposed system. Once appropriate algorithms have been identiﬁed and selected, the\\nproposed system will be outlined. The following objectives need to be met to meet the aims of\\nthis project:\\n1.To give a technical overview of the relevant deep-learning-based computer vision\\nalgorithms. An overview of convolutional neural networks will be provided, including\\nthe underpinning core operations, constituent layers, and key terminology.\\n2.Review existing algorithms for the proposed use in the automatic labelling system. A\\nreview on markerless motion capture will be produced with a speciﬁc focus on how deep\\nlearning systems are used in pose estimation. Existing methods to estimate key-point co-\\nordinates from image data will be identiﬁed and discussed, with a method selected for\\nuse in this study. This will be followed by a review on inﬂuential convolutional neural\\nnetworks and design choices that have improved their performance in recent years. Exist-\\ning state-of-the-art convolutional neural networks will be selected for use in the automatic\\nlabelling system for this project. This will be followed by a review of object detection al-\\ngorithms used as part of the automatic labelling system. A method to estimate key-point\\ncoordinates from image data, along with two convolutional neural networks and object\\ndetection algorithms, will be selected once this objective is completed.\\n3.Outline Proposed System and Working Methods. Once the preferred algorithms and\\ntechnologies have been identiﬁed; the proposed automatic labelling system will be deﬁned\\nusing ﬂow diagrams. Furthermore, a workﬂow diagram will be produced for data pre-\\nprocessing, modelling, and evaluation. Any data quality issues will be identiﬁed and\\ndiscussed. Methods used to produce the automatic labelling system will be outlined to\\nensure that the study is repeatable.\\n4.Compare both object detection algorithms and key-point estimation algorithms. The\\nproposed algorithms that have been used in this study will be compared using deﬁned\\nevaluation metrics. A preferred algorithm will be selected for object detection and key\\npoint estimation in the ﬁnal automatic labelling system.\\n5'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 17}, page_content='6'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 18}, page_content='Chapter 2\\nTechnical Overview\\nIn 1959, the term Machine Learning was coined by Arthur Samuel, and it refers to the ﬁeld of\\nstudy that gives computers the ability to learn without being explicitly programmed (Samuel\\n1959). Interestingly, in many machine learning publications, people often cite this deﬁnition\\nas a direct quote of Arthur Samuel despite it never appearing in any of his publications. As\\nsummarised by Jordan & Mitchell (2015), Machine Learning is one of the fasting growing\\nﬁelds in technology and is underpinned by both statistical and computing theory. Despite some\\nof the key algorithmic theory being produced as early as 1943, there has been increasing and\\nongoing breakthroughs in the ﬁeld of machine learning due to new learning algorithms, an ex-\\nponential increase in the availability of open-source online data and low-cost computational\\npower (McCulloch & Pitts 1943). Data-driven machine learning methods are being widely\\nimplemented in many industries such as science, ﬁnance, technology, retail, and health care re-\\nsulting in evidence-driven decision making (Jordan & Mitchell 2015).\\nAyodele (2010) highlights that Machine Learning can be categorised into three core areas;\\nsupervised learning, unsupervised learning, and reinforcement learning. Supervised learning is\\nwhere algorithms learn based on labelled training data to make predictions on test data. The\\nlabelled training data is an input with an example labelled output that the model has to learn\\nthe underlying relationship. For example, in image classiﬁcation, a labelled data point would be\\nan image of a dog along with the label ‘dog’ that a particular algorithm can learn to associate\\nthe visual features of the image with the given label to then make future predictions on unseen\\nimages. In unsupervised learning, certain algorithms are used to learn patterns in unstructured\\ndata. This differs from supervised learning as the data used in these tasks does not have labels.\\nExamples of unsupervised learning are clustering algorithms such as k-means clustering. Rein-\\nforcement learning is when an algorithm learns a policy of how to behave based on observations\\nof a given environment. Each possible action the agent can take within the environment will\\nhave a reward or punishment incurred by the agent, which guides the algorithm in learning how\\nto interact with that environment. A common analogy for this is that when you are training a\\ndog, you reward it with a treat if it behaves well. If it misbehaves, then you punish it by saying\\n7'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 19}, page_content='‘no’ in a deep tone. This is the same logic that inspired reinforcement learning (Ayodele 2010).\\nThis project aims to build a fully automated hand tracker. This will be achieved by training\\na given ML model or system on labelled training data. This model will take image frames\\nof a video as input and output the coordinated (x,y) of the index ﬁnger and thumb position in\\nimage space. Therefore, this is a supervised learning task. An outline of the algorithms used in\\nmachine learning-based image analysis will be explained in this chapter.\\n2.1 Convolutional Neural Networks\\nA subset of machine learning known as deep learning has continued to outperform previous\\nstate-of-the-art machine learning models in several applications, particularly computer vision\\nand natural language processing (V oulodimos et al. 2018). The term deep learning refers to\\nartiﬁcial neural networks with multiple layers. Convolutional neural networks (CNN) are among\\nthe most popular in deep learning literature in the last decade (Albawi et al. 2017). The surge\\nin developments within this subset of machine learning was a result of extensive, open-source,\\nlabelled data sets along with more widely available low-cost Graphics Processing Units (GPUs),\\nallowing for parallel computational methods accelerating the process of training several millions\\nof learnable parameters (V oulodimos et al. 2018).\\nThe modern framework for the CNN was published in 1989, in which a deep learning sys-\\ntem named ‘LeNet-5’ was used to classify handwritten digits (LeCun et al. 1989). Gu et al.\\n(2018) explains that convolutional neural networks are inspired by biological visual perception\\nmechanisms that are used by many creatures. CNN’s mimic the behaviour of the visual cortex\\nin regards to detecting visual information in speciﬁc receptive ﬁelds. A CNN is constructed\\nfrom three primary neural layer categories: convolutional, pooling, and fully connected layers,\\nas shown in Figure 2.1. The convolution and pooling layers perform mathematical operations\\non the input image to extract learnable visual features. These learned features are then passed\\nonto fully connected layer(s), which can be used for classiﬁcation or regression (Gu et al. 2018).\\nFigure 2.1: A basic CNN architecture\\n8'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 20}, page_content='2.1.1 Convolution Layers\\nThe objective of the convolutional layer is explained by Gu et al. (2018) as to learn feature\\nrepresentations of visual features that are present in the input image. This is achieved by passing\\nﬁlters (also commonly referred to as kernels) over the pixels present in the input image using\\na sliding window operation to perform summation of element-wise multiplication to produce\\nthe feature representations, commonly referred to as ‘feature maps’ or ‘convoluted features’ as\\nshown in Figure 2.2 (Gu et al. 2018). Convolutional layers increase network efﬁciency when\\ncompared with fully connected layers by weight sharing. By applying the convolution operation,\\nthe same weights are used throughout all locations in an image which reduces computational\\noverhead (LeCun et al. 2015).\\nFigure 2.2: The operations of a convolutional layer. The computation on the RHS of the convo-\\nluted feature details the element-wise multiplication between the input data and the kernel, of\\nwhich the summation is taken to produce the convoluted feature (Yin 2018).\\nEach layer contains several ﬁlters, and these contain the learnable parameters in convolu-\\ntional layers, which are the weights of each ﬁlter. The early layers in the network learn simple\\npatterns in the input image, such as edges, corners, and curves. The weights of the ﬁlters are\\nlearned through a process known as back-propagation, in which the weights of the ﬁlter are\\niteratively updated to minimise the difference between predicted outputs and ground truth out-\\nputs. In hidden layers, the ﬁlters are convolved over feature maps from the previous layer, and\\nnew feature maps are produced that are more abstract and complex visual representations of the\\noriginal image (V oulodimos et al. 2018). In comparison to the earlier layers, hidden layers learn\\nvisual features that are more complex and often abstract when manually observing feature maps\\nfrom deep hidden layers. The ‘receptive ﬁeld’ is a window of pixels in the original input image\\nthat affect a particular pixel in a feature map (Gu et al. 2018).\\nThe size of the ﬁlter used in each layer is a crucial hyperparameter in convolutional layers.\\nThe ﬁlter size determines the receptive ﬁeld of the original image that the ﬁlter can observe. A\\n9'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 21}, page_content='larger ﬁlter such as 7 ×7 will be passed over a greater region of the original input image and\\nextract visual features that are typically larger and more general. A smaller ﬁlter will result in a\\nsmaller receptive ﬁeld for a given feature map, learning local and more detailed visual features.\\nTypically, smaller ﬁlters are used, such as a 3 ×3 ﬁlter, as most of the useful visual features\\nin an image are small; therefore, it makes sense to observe small subsections of the image in\\ndetail. Many useful visual features, such as vertical or horizontal edges, will be present in an\\nimage several times. Therefore, a single horizontal edge detecting ﬁlter can be passed over an\\nentire image extracting the location of all horizontal edges in the image. The presence, location\\nand proximity of all learned visual features are then used in making ﬁnal predictions of the\\nconsidered output for the task at hand. Other core hyperparameters related to the convolutional\\nlayers include the number of ﬁlters applied in a given layer. Adding additional ﬁlters is known\\nas making the network ‘wider’. Additionally, another hyperparameter is the number of layers in\\nthe network, where adding more layers is referred to as making the network ‘deeper’. Making\\na network wider or deeper increases the learning capacity of the network but can also make it\\nprone to overﬁtting.\\nA brief example is shown in Figure 2.3 to provide further intuition to convolutional layers.\\nThis example is elementary and assumes that the ﬁlters’ numerical weights have been learned\\nwithout manual hand-tuning. See the ﬁrst row of Figure 2.3, which shows a grey-scale hand-\\nwritten digit which is from the MNIST data set (LeCun & Cortes 2010). The input has been\\nrepeated four times as four ﬁlters will be used on this data. Each input image has dimensions\\n28x28 for the height and width of the image. The second row denoted by ‘Filter (numerical)’\\ncontains four 3x3 ﬁlters with weights that a CNN has learned. The numerical values have been\\nrepresented visually in the third row in which 1 corresponds to white, 0 corresponds to grey, and\\n-1 corresponds to black on a continuous grey scale.\\nFigure 2.3: How ﬁlters learn visual features from an input image to store in feature maps\\n(DeepLizard 2021).\\n10'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 22}, page_content='The bottom row in Figure 2.3 shows the feature maps that are outputted as a result of pass-\\ning each of the 3x3 ﬁlters over the 28x28 input image. Each of the feature maps clearly show\\nthe number 7 and are some form of a visual representation of the original input image. How-\\never, there are clear differences between each of the four feature maps due to being produced\\nby different ﬁlters. We can see in the feature maps that all of the ﬁlters are detecting edges. In\\nthe feature maps, the brightest pixels can be considered the visual feature ﬁltered and learned\\nof the original input image. In feature map (a), we can see that the brightest pixels are in the\\ntop horizontal edges of the 7 in the input image, indicating that the ﬁlter associated with this\\nfeature map is a top horizontal edge detector. Feature map (b) shows the brightest pixels on the\\nleft vertical edges, and therefore the ﬁlter associated with this feature map is a left vertical edge\\ndetector. Similarly, the ﬁlter associated with feature map (c) detects horizontal bottom edges,\\nand (d) detects right vertical edges. The ﬁlters given in this example are elementary and would\\nlikely be present in the early layers of a CNN. In reality, many different types of ﬁlters become\\nincreasingly more sophisticated and abstract in deeper hidden layers of a given CNN.\\nFor example, see Figure 2.4, in which ﬁve feature maps are displayed produced from a CNN,\\nanalysing an image of a cat. As explained by Dertat (2017) the images are from different layers\\nat increasing depth through the network. The left-hand side image is from the shallowest layer,\\nwhereas the far right image is a feature map from the deepest layer. It is clear that the deeper in\\nthe network that the feature map is extracted from, the less the feature map looks like the original\\nimage. For example, in the feature maps associated with block4 and block5, there is arguably\\nno cat present in the image. Deeper feature maps encode higher-level visual features such as\\n‘cat eyes’ and ‘cat nose’ that are more sophisticated than general shape detectors which are\\npresent in the early layers of the network. Therefore, a helpful way to think about deeper feature\\nmaps is that they contain abstract representations of visual features associated with the class of\\nthe image rather than easily interpreted visual features. The information stored in the deeper\\nfeature maps is still highly useful in making predictions, but it is harder to visually interpret for\\nhumans (Dertat 2017).\\nFigure 2.4: Feature maps at different network depths (Dertat 2017)\\n2.1.2 Pooling Layers\\nTypically, directly after convolutional layers, there are Pooling Layers. The objective of the\\npooling layers is explained by V oulodimos et al. (2018) is to downsample (or subsample) the in-\\nput dimensions for the next convolutional layer. The depth of each feature map is not affected;\\n11'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 23}, page_content='however, the height and width are reduced in dimension leading to an inherent loss of infor-\\nmation. Interestingly, this loss of information is often advantageous as it prevents the network\\nfrom overﬁtting the training data, increases the networks ability to generalise to unseen data,\\nand reduces the computational and memory costs in later stages of the network. There are many\\ncommon pooling layer variants. However, the most common are max and average pooling. It\\nhas been demonstrated in the literature that max-pooling can lead to faster convergence than\\naverage pooling and is also more beneﬁcial in preventing overﬁtting in a CNN (Scherer et al.\\n2010). There are also additional variations of pooling layers that can be applied to different\\napplications, such as stochastic pooling and spatial pyramid pooling, which is used in object\\ndetection (He et al. 2015).\\nSee Figure 2.5 for an example of max-pooling in which the LHS 4 ×4 grid is a feature\\nmap outputted from the previous convolutional layer in a hypothetical network. The way that\\nmax-pooling works is simple, as this example uses a 2 ×2 window and a stride of 2, the feature\\nmap is split into a total of four 2 ×2 sub-grids indicated by the red, green, yellow, and blue boxes\\non the LHS of Figure 2.5. The maximum value of each sub-grid is taken and stored in the output\\nof the pooling layer. For example, in the red sub-grid in the LHS of Figure 2.5 the values are 1,\\n1, 5, 6, of which the maximum value of this set is 6. Therefore the number 6 is displayed in red\\nin the output matrix on the RHS of Figure 2.5.\\nFigure 2.5: Max pooling operation example (Dertat 2017)\\n2.1.3 Fully Connected Layers\\nThere are often several blocks of convolution and pooling layers; however, the amount of these\\nblocks can vary between networks. Some networks have fully connected layers as the ﬁnal\\nlayer for both regression and classiﬁcation tasks. V oulodimos et al. (2018) highlights that fully\\nconnected networks provide high-level reasoning based on the learned visual features in the\\ninput image to make the ﬁnal outputted prediction. Firstly, the multiple 2D feature maps are\\nﬂattened into 1D feature vectors, which are then passed as inputs to the fully connected layer.\\nNeurons in the fully connected layer are connected to every pixel (or neuron) in the feature\\nmap, which is why they are called fully connected layers. The inputted 1D feature vector is\\nmultiplied by the weights of the fully connected network, and a bias term is added. The purpose\\n12'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 24}, page_content='of the bias term is to offset the activation function by translating the activation function to the\\nleft or right, which increases the learning capacity of the network. The weights and bias are the\\nlearnable parameters in these layers, which are updates iteratively through back-propagation.\\nThe output of a fully connected layer can be passed as the input for deeper fully connected\\nlayers, or it can be the ﬁnal output of the model. If a fully connected layer is the ﬁnal output\\nlayer of a given model, the number of neurons in the ﬁnal layer equals the number of predictions\\nthe network will make. This makes the number of neurons in the ﬁnal layer a crucial parameter\\nwhen designing a network architecture (V oulodimos et al. 2018).\\n2.1.4 CNN for multi-class classiﬁcation example\\nA brief explanation has been given on convolutional, pooling, and fully connected layers. A\\nCNN used for multi-class classiﬁcation is shown in Figure 2.6. An image of a zebra is inputted\\ninto the network, and the visual features of the image are extracted as it is passed through the\\nthree convolution and pooling layers. The ﬁnal feature maps outputted by the last pooling layer\\nis ﬂattened to a 1D feature vector which is then passed into 4 fully connected layers. As this is\\na multi-class classiﬁcation problem, the ﬁnal fully connected layer is passed through a softmax\\nactivation function which outputs a ﬁnal probabilistic distribution predicting which animal was\\nin the input image. The ﬁnal fully connected layer has 3 neurons indicated by the 3 dark blue\\ndots in FIGURE, as this represents the number of pre-deﬁned classes. As shown in the peach\\ncoloured box in Figure 2.6, the class ‘zebra’ was predicted with a probability of 0.7, ‘horse’ has\\nan associated probability of 0.2, and dog has an associated probability of 0.1. As zebra has the\\nhighest probability, this is the ﬁnal prediction outputted from the network. It is interesting to\\nnote at this stage that ‘horse’ has a slightly higher probability than the class dog, which is likely\\ndue to horses and zebras typically having more similar visual features than zebras and dogs. The\\nvisual features have been extracted in the feature extraction stage of the network. This network\\narchitecture is speciﬁcally for the multi-class classiﬁcation of the three mentioned animals, and\\nthe architecture will differ depending on the application of the network.\\nFigure 2.6: CNN example for mutli-class classiﬁcation (Swapna 2021)\\n13'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 25}, page_content='2.1.5 Activation Functions\\nArtiﬁcial neural networks are inspired by biological neural networks, which comprise a large\\nnumber of interconnected neurons that may or may not ﬁre in response to a stimulus. In arti-\\nﬁcial neural networks, activation functions are the mathematical tool that is used to mimic the\\nthreshold activation in biological neural networks that determine if the input to a given neuron is\\nsufﬁcient to cause the neuron to ‘ﬁre’. In this section, a brief summary will be given to outline\\nthe most commonly used activation functions. Activation functions perform a mathematical\\noperation on the weighted sum of inputs to a given neuron (plus a bias term). The operation\\noften transforms the weighted sum to a value that is between some upper and lower limits. Such\\ntransformations are most often non-linear as this allows ANNs to learn functions that are also\\nnon-linear. In other words, activation functions enable ANNs to become universal function ap-\\nproximators. By applying non-linear activation functions to ANNs, in theory, they can learn any\\nfunction. ANNs being universal function approximators is the inherent quality of deep learn-\\ning methods that allows them to be so versatile and used in many applications such as machine\\ntranslation for language, object detection in computer vision, and simple linear regression.\\nThe Sigmoid Function\\nThe sigmoid function is an ‘S’ shaped function that maps any input value in the interval from\\n[−∞,+∞]to the range [0,1]. This is a non-linear function with a smooth derivative often used\\nin the ﬁnal layer of a neural network designed for binary classiﬁcation. The sigmoid function is\\ngiven displayed in Figure 2.7, and the equation of the sigmoid function is\\nσ(x) =1\\n1 +e−x(2.1)\\nFigure 2.7: The sigmoid function and its derivative\\nwhere x is the weighted sum of inputs to a given neuron plus a bias term. As shown in\\n14'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 26}, page_content='Figure 2.7, the sigmoid function will take most negative inputs and return a value that is close to\\nzero. Furthermore, for most positive inputs, the sigmoid function will return a value that is close\\nto one. This is the case for all inputs that lie outside of the range of [−5,5], which causes the\\nderivative indicated by the orange curve in Figure 2.7 to become very small and approximately\\nequal to zero. This causes an issue known as the vanishing gradient problem and results in\\na network not being able to reach a global optimum value, and this occurs when the sigmoid\\nfunction is used in hidden layers of an ANN. The vanishing gradient problem arises because the\\nweights associated with each layer of the network are iteratively updated based on their gradient.\\nAs the gradient of the sigmoid function becomes very small for large negative or positive values\\nofx, the weights are updated by tiny amounts each iteration. For randomly initialised weights\\nthat happen to have a large absolute magnitude, they are likely to remain close to their initial\\nvalue after thousands of epochs.\\nThe ReLU Function, and it’s variants\\nThe vanishing gradient problem in hidden layers is resolved by using the Rectiﬁed Linear Unit\\n(ReLU) function. The ReLU function, and its variants, are the most widely used among machine\\nlearning researchers and practitioners (Rasamoelina et al. 2020). The ReLU function is deﬁned\\nas\\nf(x) = max{0,x} (2.2)\\nThis function outputs zero for all negative values and therefore is non-differentiable at zero.\\nThis causes the gradients to be equal to zero for a negative input which means that no further\\nlearning occurs, which is a problem referred to as the dying ReLU problem (Lu et al. 2019).\\nThis problem is rectiﬁed by a variant known as the Leaky ReLU activation function. The Leaky\\nReLU activation function is deﬁned as\\nf(x) = max{0.01x,x} (2.3)\\nwhich ensures that the function remains differentiable for negative values of x. This activa-\\ntion function is used in the YOLOv4-tiny object detection algorithm used in this report. There\\nare also many other common activation functions such as Mish, which is a state-of-the-art non-\\nmonotonic activation function that is deﬁned as\\nf(x) =tanh×ln(1 +ex). (2.4)\\nThe Mish activation function is used in the YOLOv4 object detection algorithm used in this\\nstudy. Providing a full explanation of the details of this activation function falls outside the\\nscope of this study, and full details can be found in the original paper by Misra (2019).\\nAnother variant of the ReLU function is known as the SiLU function or Swish, which uses\\nthe sigmoid function and is deﬁned as\\n15'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 27}, page_content='f(x) =x×σ(x) (2.5)\\nin whichσis the sigmoid equation deﬁned in Equation 2.1. The Swish activation function\\nis used in EfﬁcientNet and is shown in Figure 2.8, also showing other common activation func-\\ntions. Further details on the Mish activation function can be found in a paper by Ramachandran\\net al. (2017).\\nFigure 2.8: Common activation functions (Kızrak 2020)\\n2.2 Loss Functions\\nLoss, error, or cost functions are used to calculate the loss associated with a given prediction by\\nthe model and the ground truth label for the same given training sample. The loss associated with\\nthe prediction is then backpropagated through the network by taking the derivative of the loss\\nwith respect to each layer of weights in the networks hierarchy. These gradients are then used to\\nupdate the weights to minimize an optimization algorithm allowing the network to learn to make\\nbetter predictions in the future. The loss function that is chosen will depend on the application of\\nthe neural network. For example, regression-based problems often use the Root Mean Squared\\nError (RMSE) or Mean Absolute Error (MAE). In contrast, classiﬁcation problems may use\\nCross-Entropy Loss or Hinge Loss. It is important to note that loss functions used in regressions\\nwould not be applicable to compute for a classiﬁcation problem and vice versa. Furthermore,\\nthere are task-speciﬁc loss functions such as Intersection over Union, which measure the loss\\nassociated with bounding box predictions. The exact loss function selected will depend on the\\napplication, the network and the data that is being used. The loss functions that are used in this\\n16'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 28}, page_content='report will be outlined and justiﬁed in this section.\\n2.2.1 Mean Square Error\\nThe Mean Square Error (MSE) is one of the most common loss functions used for regression-\\nbased problems. Speciﬁcally, it considers the error value of the Euclidean distance between the\\npredicted value and the ground truth value. The error is squared, summed over all samples (Neill\\n& Hashemi 2018). The MSE expressed as\\nMSE =1\\nnn∑\\ni=1(yi−ˆyi)2(2.6)\\nwhereyiis the ground truth value for a given sample, ˆyiis the predicted value for a given\\nsample, and n is the total number of samples (in a batch or data set). As the Euclidean distance\\nis squared between the ground truth and predicted values, large errors are penalised heavily.\\nThis makes this loss function particularly useful for applications in which accurate predictions\\nare essential, such as key point estimation. The ﬁnal validation loss for the key point regression\\nmodel in this report will represent the MSE for the validation set. The square root of the MSE\\ncan be taken, giving the Root Mean Square Error (RMSE), which gives a value for the mean\\nerror associated with the model, which will give the mean pixel error associated with the ﬁnal\\nmodel.\\n2.2.2 Intersection over Union (IOU)\\nTo evaluate the quality of object localization for object detection tasks, the most frequently used\\nmetric is Intersection over Union (IOU), also known as the Jaccard Index. The IOU is deﬁned in\\nFigure 2.9. The two boxes in the numerator and denominator of the formula have been included\\nto illustrate the terms ‘intersection’ and ‘union’, and one box can be thought of as the predicted\\nbounding box and the other as the ground truth annotated bounding box for a given object in\\nan image. The term ‘intersection’ refers to the overlap between the two boxes. Speciﬁcally, the\\nintersection is the area of pixels of overlap between the predicted bounding box and the ground-\\ntruth bounding box label. The term ‘union’ refers to the total area of the predicted bounding box\\nand the ground truth bounding box.\\nFigure 2.9: Intersection over Union (IOU)\\nTypically an IOU threshold is also set, which determines how tightly the predicted bounding\\n17'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 29}, page_content='box encapsulates the speciﬁed object or not. A ‘loose’ bounding box will contain a greater\\nnumber of background pixels than a ‘tight’ bounding box. Researchers most frequently set this\\nthreshold to be equal to 0.5. This means that if the IOU is above 0.5, it does produce a ﬁnal\\npredicted bounding box. If it is less than 0.5, it does not produce a ﬁnal bounding box.\\n2.2.3 Mean average Precision (mAP)\\nThe mean average precision is a metric that is commonly used to compare object detection\\nalgorithms. The precision of a model is the ratio of true positive predictions over the total\\nnumber of positive predictions and is given by\\nPrecision =TP\\nTP + FP, (2.7)\\nin which TP is True Positive, and FP is False Positive. So, for example, if we have a data set\\nwith 120 images of hands, with only 1 hand in each image. This data set has been inputted to\\nan object detection model that has predicted 100 hands and associated bounding box coordinate\\nvalues and conﬁdence values. Of the 100 total predicted bounding boxes, 20 of them have a\\nconﬁdence value below the threshold of 0.5, meaning that they must be considered False Posi-\\ntives. This leaves a total of 100−20 = 80 True Positives. Therefore the precision of the model\\nin predicting the class ‘hand’ would be Precision = 80/(80 + 20) = 0 .8.\\nWhen considering precision alone, it does not consider the total number of objects present\\nin the data. Continuing on from the previous example, there were 120 images, each containing\\none hand. Consider if a model predicts 80 correct bounding boxes and 20 incorrect bounding\\nboxes for 10,000 images of hands, the precision of the model would still be 0.8. Therefore, it is\\nimportant to consider the relationship between the number of correct predicted bounding boxes\\nand the total number of hands present in the data set. This is achieved by a metric called Recall\\nwhich is deﬁned as\\nRecall =TP\\nTP + FN, (2.8)\\nwhere TP is True Positives and FN is False Negatives. For the example considered with 120\\nimages of single hands, the recall would be given by Recall = 80/(80 + 40) = 0 .667.\\nObject detection models are usually evaluated by considering different IOU thresholds. Dif-\\nferent IOU thresholds will often result in different TP predictions. A way to compare models\\namong all considered IOU thresholds is to take the Mean Average Precision (mAP) of each\\nmodel. The mAP of a single model will also give an indication of how well it performs across\\ndifferent IOU thresholds for all classes considered for a given model. It is worth highlighting\\nthat the mAP for a single class object detection model is the same value as the model’s Aver-\\nage Precision (AP). To calculate the average precision, the average value of 11 points on the\\n18'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 30}, page_content='precision-recall curve for each possible IOU threshold for the same class (Bochkovskiy et al.\\n2020). As the model in this report considers only one class for object detection (hand), the mAP\\nof the model is equal to the AP of the only class ‘hand’.\\n2.3 Optimization\\nOptimization algorithms are the protocols used to change the parameters of a neural network,\\nsuch as the weights and bias terms, to enable the network to make better predictions in the future.\\nOptimization algorithms update the weights and bias terms with the objective of minimizing the\\nloss or error associated with the network. This occurs by passing input into the network to ob-\\ntain a predicted output, known as a forward pass. A loss function is used to quantify the error\\nbetween the predicted output and the ground truth label associated with that training sample.\\nThe error is then backpropagated through the network by taking the derivative of the error with\\nrespect to the weights at each stage of the network from the output layer to the input layer. The\\ncalculated gradients are then used in conjunction with a learning rate to update each weight and\\nbias term for each iteration. An iteration refers to the number of batches that the model has\\nseen. Often a single batch will contain 32 or 64 training samples. This process requires a high\\nnumber of iterations to reach a global minimum of the optimization algorithm. Furthermore,\\nthis process may require multiple epochs to converge to the global minimum. An epoch refers\\nto the number of times the model has seen the entire data set (Doshi 2020).\\nThe learning rate is an important parameter to select to ensure that the optimization algo-\\nrithm can converge to a local minimum. If the learning rate selected is too small, the weights\\nwill be updated in small steps, and learning can occur too slowly. This causes issues as it in-\\ncreases the time it takes for the model to converge, and therefore the learning rate needs to be\\nsufﬁciently large to meet the time constraints of the task at hand. On the other hand, if the learn-\\ning rate is set too large, the steps taken can be too large, and the algorithm may miss the global\\nminimum. Furthermore, if the learning rate is sufﬁciently large, it can cause a phenomenon\\nknown as divergence in which the loss will increase at each step rather than decrease. There-\\nfore, an appropriate learning rate needs to be selected to ensure the steps are sufﬁciently large so\\nthat training does not take too long and that the steps are sufﬁciently small to ensure the model\\nreaches a global minimum and does not diverge.\\nVarious popular optimisation algorithms are used in a convolutional neural network, such\\nas Gradient Descent, Stochastic Gradient Descent, and Adam. The optimisation algorithm used\\nin the key point regression model for this report is Adam as it has been shown to outperform\\ntraditional gradient descent algorithms (Kingma & Ba 2014). Further details can be found in a\\npaper by Kingma & Ba (2014), but most importantly, it is an extension of the gradient descent\\nalgorithm explained in this section but is more computationally efﬁcient and deals better with\\n19'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 31}, page_content='sparse gradients experienced in computer vision projects (Kingma & Ba 2014, Brownlee 2021).\\n2.3.1 Generalization Errors\\nGeneralization error refers to the measure of how accurately an algorithm can predict values\\nfor previously unseen data. This error can be caused by a poorly ﬁt model, sampling error,\\nand noise in the data. Three classiﬁcations can summarise the ﬁt of a model to a given data\\nset: underﬁtting, balanced, overﬁtting. Underﬁtting occurs when the model is not able to make\\naccurate predictions on the training data. This occurs because the model is not sophisticated\\nenough to capture the relationship between the inputs and outputs. A model that underﬁts the\\ndata has high bias and low variance. On the other hand, overﬁtting occurs when the model\\nperforms well on training data but makes poor predictions on validation or testing data. This\\noccurs because the model is learning the trend in the data and the noise present in the data. A\\nmodel that overﬁts the data has low bias and high variance. A balanced model will make good\\npredictions on training and unseen data. This is when the model has been able to learn the\\nrelationship between the inputs and outputs without learning the noise that was present in the\\ntraining data. A balanced model will have low bias and low variance and ideally should be able\\nto perform the prediction task with the same accuracy as a human or better.\\nAn example of the three model ﬁts discussed is shown in the context of a regression problem\\nin Figure 2.10. Consider that the data points to be a low order polynomial function such as a\\nquadratic. A balanced model is shown in the middle plot, in which the plotted line representing\\nthe model’s predictions accurately captures the trend of the data. The trend line does not pass\\nthrough every data point. However, this is due to noise in the data, which can be present for a\\nvariety of reasons. The left plot shows an underﬁtting model, which makes predictions based on\\nthe equation of a straight line or a ﬁrst-order polynomial. This model will make poor predictions\\non training data and unseen data. The right plot shows a model that is overﬁtting the training\\ndata as it has learned the noise. This is likely to make poor predictions on data that is out of the\\nsample, such as at a later time than is plotted on the axis in Figure 2.10 right.\\nFigure 2.10: Model Fit: Underﬁtting, Balanced, Overﬁtting (Bhande 2018)\\n20'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 32}, page_content='2.3.2 Early stopping\\nAn example of the three model ﬁts discussed is shown in the context of a regression problem\\nin Figure 2.10. Consider that the data points to be a low order polynomial function such as a\\nquadratic. A balanced model is shown in the middle plot, in which the plotted line representing\\nthe model’s predictions accurately captures the trend of the data. However, the trend line does\\nnot pass through every data point. Noise in the data can be present for various reasons, but it\\nis not desirable for a model to learn. The left plot shows an underﬁtting model, which makes\\npredictions based on the equation of a straight line or a ﬁrst-order polynomial. This model will\\nmake poor predictions on training data and unseen data. The right plot shows a model that is\\noverﬁtting the training data as it has learned the noise. This is likely to make poor predictions\\non data out of the sample, such as at a later time than is plotted on the axis in Figure 2.10 right.\\nFigure 2.11: Early Stopping Point (PapersWithCode n.d.)\\n21'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 33}, page_content='22'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 34}, page_content='Chapter 3\\nComputer Vision: Review of methods\\nSeeking more precise and resilient movement measurement methods has a multitude of appli-\\ncations in a variety of industries. These methods include measurement devices such as GPS\\ntrackers, video camera footage, microphones using ultrasound technologies, and various be-\\nspoke or tailored sensors. Using video camera footage is the most widely available technique as\\nthe ownership of smartphones able to capture high-quality videos is so high (e.g. 82% in the Uk\\nand 77% in the USA), allowing for high resolutions observations of behaviour (Newzoo 2021,\\nZhao et al. 2020). Furthermore, video camera footage allows for high-resolution observations of\\nbehaviour. Motion can be captured in video footage with high levels of accuracy using special-\\nist equipment such as ‘Mo-cap’ suits (Riecick `y et al. 2018). However, recent breakthroughs in\\ndeep learning technologies have allowed for advances in measuring movement in video footage\\nwithout markers (Wu et al. 2020, Mathis et al. 2020).\\nThe key beneﬁt of markerless key-point based pose estimation with deep learning is that it\\nreduces the need for specialist equipment, such as ‘Mo-cap’ suits or electronic sensors. Marker-\\nbased motion capture is achieved through physically creating contrast on points of interest.\\nThis can be achieved through the use of colours, LEDs, or reﬂective markers, which allows\\nfor existing computer vision technologies to track these key points with ease (Mathis et al.\\n2020). On the other hand, markerless motion capture requires no specialist equipment to be\\nworn when recording the video. However, it does require some kind of ﬁt-for-purpose deep\\nlearning network that has been trained based on annotated example images (Mathis et al. 2020).\\nFurthermore, a key beneﬁt of deep learning based algorithms is that they can be tailored for\\ndifferent tasks allowing different types of animals or body parts to be tracked (Mathis et al.\\n2020).\\n3.1 Pose Estimation\\nIn this section, current pose estimation methods will be outlined and reviewed. Based on this, a\\npreferred method will be selected for use in the automatic labelling system for DeepLabCut.\\n23'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 35}, page_content='As highlighted by Mathis et al. (2020), numerous pose estimation algorithms have been de-\\nveloped, which are comprehensively reviewed by Moeslund et al. (2006), Poppe (2007). How-\\never, algorithms based on neural networks (or deep learning) offer the greatest accuracy when\\nevaluated on data for human pose estimation (Insafutdinov et al. 2016, Xiao et al. 2018). Gen-\\nerally speaking, deep learning based pose estimation algorithms typically consist of two stages,\\nan encoder and decoder network (also often referred to as backbone and output heads, respec-\\ntively). The role of the encoder is to learn and extract visual features from each image in a\\nvideo. These learned features are used by a decoder network in predicting each relevant body\\npart along with the location in the frame (and usually returns image coordinates of key points).\\nPose estimation algorithms essentially fall under the umbrella of object detection, which is a\\nﬁeld of research that is advancing rapidly (Mathis et al. 2020). The ongoing development of\\nnew algorithms and techniques improves both both the accuracy and efﬁciency of deep learning\\nalgorithms to perform tasks in object detection and pose estimation (Wu et al. 2020).\\nDeep learning systems consist of four core components, including a data set (including\\nground truth annotations), a model architecture, a loss function, and an optimisation algo-\\nrithm (Goodfellow et al. 2016). Mathis et al. (2020) highlights that the data set deﬁnes the\\nrelationship between the inputs and outputs that the model architecture must learn. For pose\\nestimation, the model must learn to take an image as the input and predict a given pose as the\\noutput. Learning occurs by updating the model’s weights by the optimisation algorithm that\\nminimises the loss function over multiple iterations. In each iteration, the quality of the pre-\\ndicted pose is evaluated by comparing them with the ground truth annotated pose. The weights\\nare adjusted over numerous iterations until the predicted pose is as close to the ground truth\\npose as possible. There are multiple options or variations to consider for each of the four key\\ncomponents of a deep-learning-based pose estimation system. The selected combination of\\ncomponents will affect the performance of the model. These will be discussed in detail in the\\nfollowing sections.\\n3.1.1 Data sets and Data Augmentation\\nVarious data sets are to pre-train computer vision models used in deep learning based pose\\nestimation systems through a process known as ‘transfer learning’. Two types of data sets are\\ntypically relevant for training pose estimation systems. The ﬁrst type of data set are those used\\nfor image classiﬁcation tasks, for example, ImageNet (Deng et al. 2009) or the CIFAR data\\nsets (Krizhevsky et al. 2009). For example, the ImageNet data set contains 14.2 million images\\nof 21 thousand classes. Therefore, there are many images with a multitude of learnable image\\nfeatures present in this data. Such open-source data sets allow research teams to build state of\\nthe art models and allow for easy comparison between models. The accuracy of models has\\ndrastically improved in the last 12 years, as shown in Figure 3.1. Training networks on data\\n24'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 36}, page_content='sets such as ImageNet can take several days. For example, it took 17 days to train the widely\\nused DNN ‘ResNet’ on ImageNet (James 2017). Using datasets such as ImageNet for transfer\\nlearning allows researchers to save days of training by downloading pre-trained weights for\\nrobust ﬁlters that have learned to identify many image features. Therefore, networks pre-trained\\non these data sets are extremely useful in pose detection for saving on training time and obtaining\\ntrained and robust convolution ﬁlters (Mathis et al. 2020). Comparison of models trained on\\nImageNet can be achieved easily through ‘Top-1’ Accuracy, which refers to the number of\\ncorrectly predicted instances by a given model where the single class with the highest conﬁdence\\nvalue is the ground truth class. Further, ‘Top-5’ Accuracy refers to the number of correctly\\npredicted instances by a given model where any one of the top 5 highest conﬁdence predictions\\nmatch the ground truth class (Krizhevsky et al. 2012).\\nFigure 3.1: Image Classiﬁcation on ImageNet, ( PaperswithCode 2021 a)\\nThe second type of data sets that are relevant to pose estimation are those that have been\\nmade for purpose in pose estimation. These can be general-purpose pose estimation data sets\\nsuch as MS COCO (Lin et al. 2014) or MPII pose (Andriluka et al. 2014) which are used for\\nbenchmarking pose estimation models. Alternatively, there are pose estimation data sets spe-\\nciﬁc to particular applications, including NYU hands which is speciﬁcally for benchmarking\\nhand pose estimation models (Tompson et al. 2014).\\nA common issue when building pose estimation models in laboratory conditions is hav-\\ning insufﬁcient data to train the model. This issue can be addressed by implementing ‘Data\\nAugmentation’ techniques which artiﬁcially increase the amount of training data by creating\\nmodiﬁed copies of previously existing data. These techniques involve applying various transfor-\\n25'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 37}, page_content='mations, including geometric transformations (such as ﬂip/crop/rotate/translate), colour trans-\\nformations (altering the RGB colour channels, greyscale, intensifying a given colour), kernel\\nﬁlters (sharpen/blur an image). Data Augmentation techniques increase the model’s ability to\\ndeal with noise in the data, making it more robust and less likely to overﬁt the data that the model\\nis trained on. When designing a deep learning pose estimation system, the adopted data augmen-\\ntation techniques must not negatively affect the semantic information of the image. Different\\ntechniques can be applied depending on what the intended purpose of the model is. Further-\\nmore, certain features within the dataset may encourage the use of a speciﬁc data augmentation\\ntechnique.\\n3.1.2 Model Architectures\\nDeep learning based markerless pose estimation systems typically follow variations of the same\\nkey network architecture. As highlighted by Mathis et al. (2020), DL pose estimation systems\\nare typically composed of an encoder (backbone) network and one or multiple decoder (heads)\\nnetworks. The encoder network architecture is typically based on generic CNN models used in\\nobject detection or classiﬁcation. Speciﬁcally, which encoder architecture is speciﬁed is often\\nthe most inﬂuential decision that impacts various properties of the overall system. These factors\\ninclude computational requirements, inference speeds, and the amount of training data required.\\nTypical encoder network architectures used in DL pose estimation systems include variations of\\nstacked hourglass networks (Newell et al. 2016), ResNets (He et al. 2016), MobileNetV2s (San-\\ndler et al. 2018), or EfﬁcientNets (Tan & Le 2019). A key beneﬁt of using these architectures\\nfor pose estimation systems is that pre-trained weights are often open-source on one or more\\nlarge scale benchmark datasets such as ImageNet (Ridnik et al. 2021). There is evidence in\\nthe current literature to suggest that pre-trained encoder networks have proven to be a beneﬁt\\nto pose estimation for small-scaled experiments as they are more robust, have shorter training\\ntimes, better performance, and less training data required (He et al. 2019, Mathis et al. 2018,\\n2020, 2021).\\nThe performance of encoder network architectures is a highly active area of research (Mathis\\net al. 2020). There have been multiple breakthroughs in the last few years in not only the ac-\\ncuracy of network architectures in object classiﬁcation benchmark data sets (e.g. ImageNet),\\nbut also the speed of computation (Kornblith et al. 2019, Goodfellow et al. 2016). Such break-\\nthroughs are largely dictated by factors such as the number of trainable parameters. Due to the\\nimportance of the ImageNet benchmark data set, the amount of active research in this area, and\\nthe current rate of improved performance in image classiﬁcation, there will likely be frequent\\nperformance improvements for the foreseeable future. This continued progress necessitates that\\ndevelopers stay up to date with the latest breakthroughs in object detection technologies, which\\nis made easy through sources such as PapersWithCode (2021 b) which compare the performance\\nof state-of-the-art network architectures in all computer vision and natural language processing\\n26'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 38}, page_content='areas, and also provide links to relevant research articles and source code.\\nThere are also a variety of decoder network architectures that are used in pose estimation. In\\ntypical convolutional encoders, input images are passed through a series of convolution ﬁlters\\nand pooling layers. Encoders gradually downsample the original high-resolution input image\\nand produce feature maps with learned image representations of the original visual features in\\nthe input images. In regression-based systems, these downsampled feature maps can be di-\\nrectly to make predictions for key points of interest by passing the downsampled representation\\nthrough a ﬂatten layer, followed by fully connected layers (Mathis et al. 2020). However, when\\nthe downsampled feature maps can be upsampled to increase the resolution again using decon-\\nvolutional layers (Insafutdinov et al. 2016, Xiao et al. 2018). This upsampling process is referred\\nto as the decoder (head) network and typically involves taking feature maps from multiple stages\\nin the encoder network and upsampling them to a given resolution. There are also various other\\nstate-of-the-art approaches to pose estimation network architectures. These include stacked\\nhourglass networks, which do not implement transfer learning but instead use skip connections\\nto transfer visual information from the encoder network to the decoder network at the same\\nrelative position in the convolution hierarchy, which supports upsampling feature maps (Newell\\net al. 2016).\\nDeepLabCut is the pose estimation tool that has been demonstrated to be effectively used\\nin quantifying Parkinson’s bradykinesia (Zhao et al. 2020). Currently, DeepLabCut requires\\nan automatic labelling system to increase usability for widespread deployment. It is essential\\nto explain the network architecture of this pose estimation tool as it is a primary motivation\\nbehind this paper. DeepLabCut was create using DeeperCut, a state-of-the-art pose estimation\\nalgorithm and was developed by Insafutdinov et al. (2016) and adopts a ResNet-50 encoder ar-\\nchitecture (He et al. 2016). DeeperCut was then adapted to produce DeepLab by Chen et al.\\n(2017) which the ﬁnal convolutional layer in the encoder network was removed and replaced by\\ndilated deconvolutional layers with a stride of 16. Dilated layers were used to larger receptive\\nﬁelds with higher spatial information whilst using less computational resources and therefore\\ntaking less time to complete these operations (Mathis et al. 2020). A ﬁnal single deconvolu-\\ntional layer (with a stride of 8) was then used for upsampling to produce output heat maps that\\npredicted the location of the key points. The DeepLab network architecture has inspired many\\nother pose estimation algorithms, and various backbone architectures can be used to give a dif-\\nfering performance in speed and accuracy (Xiao et al. 2018, Mathis et al. 2020). These networks\\nwere adapted to produce DeepLabCut, a (multi) animal pose estimation tool that is open-source\\nand includes a GUI. This tool easily allows non-programmers to extract frames from videos,\\nlabel key points in the frames, create training data sets, initialise training, and evaluate the net-\\nwork (Nath et al. 2019).\\n27'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 39}, page_content='3.1.3 Loss Functions\\nAny key point of interest in pose estimation can be represented by (x,y) coordinates on an im-\\nage (Mathis et al. 2020). There are two underlying approaches in making predictions on these\\nkey points, which result in two core loss functions that can be adopted as part of the ML sys-\\ntem. Firstly, regression-based loss functions can be implemented where the coordinates are\\ntargets that must be predicted the model. On the other hand, classiﬁcation-based loss functions\\ncan be implemented, which is more widely applied in practice. When treating pose estima-\\ntion as a classiﬁcation problem, the coordinates of a key point are mapped onto a grid which is\\nthe same dimensions as the original input image, and the model produces a heatmap of pixel-\\nbased probabilities for each key point considered. The critical difference between regression\\nand classiﬁcation is that a regression loss function is deﬁned by sets of (x,y) coordinates and\\nclassiﬁcation problems are deﬁned by images (of ground truth pixels and pixel heatmap predic-\\ntions). The key beneﬁt of classiﬁcation is that it is a fully convolutional process and can make\\nseveral predictions for the same body part in the same image. For example, when making pre-\\ndictions for the tip of multiple index ﬁngers from different hands in a single video frame. The\\ndata set used in this study only contains a single hand in each frame, and therefore this beneﬁt\\nof classiﬁcation-based approaches is redundant. Furthermore, the performance of classiﬁcation-\\nbased approaches has been demonstrated to be lower than regression-based approaches when\\nusing low-resolution images (Li et al. 2021).\\nThe mean squared error (MSE) is a standard loss function used to assess the quality of\\npredictions in both regression and classiﬁcation based pose estimation algorithms. The MSE\\nmeasures the euclidean distance between predicted and ground truth pixels. Alternatively, the\\npercentage of correct key points (PCK) can be used to refer to the proportion of predicted key\\npoints that fall within a distance of the ground truth. An example user-deﬁned distance could be\\nthe human labelling variability within the training data if this value is known.\\n3.1.4 Optimization Algorithms\\nPose estimation tools typically adopt a multi-stage training approach in which a pre-trained\\nencoder is pre-trained for an unrelated task such as image classiﬁcation on data sets such\\nas ImageNet or CIFAR. Encoder networks are typically pre-trained using stochastic gradient\\ndescent (SGD) or other popular variants of this core method, such as Adam (Kingma & Ba\\n2014). The encoders weights are downloaded and frozen so that they can not be trained in\\nlater stages (Mathis et al. 2021). The decoder, top, or head of the network is then trained using\\nSGD variants such as the Adam optimizer to calibrate the network for a given pose estimation\\napplication (Mathis et al. 2020). Using a pre-trained encoder and a ﬁne-tuned decoder can in-\\ncrease the model’s ability to generalize to unseen images as the pre-trained weights are more\\nrobust (Mathis et al. 2021).\\n28'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 40}, page_content='3.1.5 Chosen method for this study\\nTransfer learning will be utilised in the methods for this study as it has been demonstrated that\\npre-trained encoder networks are more robust, offer better performance, and require less train-\\ning data and time than training from scratch (Mathis et al. 2021). The selected encoder will\\nbe trained on ImageNet as this is the primary data set used to pre-train models, and it contains\\na high amount of classes and training data. A regression-based approach has been selected as\\nonly two key points are being predicted: the tip of the index ﬁnger and thumb for a single hand.\\nTherefore, the core beneﬁt of classiﬁcation-based approaches, which is their ability to predict\\nthe same body part for multiple animals in the same frame, would not be utilised. Furthermore,\\nregression-based approaches have been demonstrated to exhibit better performance when mak-\\ning predictions on low-resolution images (Li et al. 2021). Many pre-trained networks such as\\nResNet and EfﬁcientNet take low-resolution images as inputs such as 224 ×224, further justi-\\nfying a regression-based approach (He et al. 2016, Tan & Le 2019). Lastly, data preprocessing\\nis more simple for regression-based approaches as there is no need to convert key point coor-\\ndinates to pixel-based heatmaps. Therefore implementing a regression-based method will take\\nless time. The regression-based approach will involve extracting downsampled feature maps to\\ndirectly predict key points using a ﬂatten layer followed by fully connected layers. Data aug-\\nmentation will not be implemented in this study as the original images are already labelled, and\\ntherefore applying augmentation techniques such as stretching, rotating or ﬂipping the images\\nmay negatively impact the quality of the semantic information between the images and the labels\\nas the labelled coordinates would be on the scale of the original image. The chosen pre-trained\\nencoder used as the feature extractor will be discussed in the next section.\\n3.2 A brief history of encoder network architectures\\nIn this section, a brief history of inﬂuential convolutional neural networks encoders will be\\ngiven. Based on this review, pre-trained encoder networks will be selected as feature extractors\\nin the key point estimation model for use in the automatic labelling system.\\n3.2.1 LeNet\\nArguably the most inﬂuential CNN is the ﬁrst one that used backpropagation, or gradient-based\\nlearning, to learn ﬁlter weights in convolutional layers as this is the key underpinning concept\\nthat serves as the foundation for all state-of-the-art networks today. This network was designed\\nby LeCun et al. (1998), and named ‘LeNet’. It was released in 1998 designed for optical\\ncharacter recognition (OCR), which took in 28 ×28 grey-scale images of hand-written digits\\nclassiﬁed from 0-9 using the famous MNIST data set. The signiﬁcance of LeNet was it high-\\nlighted that automatically learned visual feature extractors could outperform handcrafted feature\\nextractors (LeCun et al. 1998). See Figure 3.2 highlighting the network architecture for LeNet\\n29'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 41}, page_content='with two convolution and pooling layers followed by multiple dense layers, also known as fully\\nconnected layers.\\nFigure 3.2: LeNet and AlexNet network architectures\\n3.2.2 AlexNet\\nA more recent inﬂuential CNN is similarly named after the designer ‘AlexNet’ and was released\\nby Krizhevsky et al. (2012). AlexNet took nearly six days to train on two NVIDIA GTX 580\\n3GB GPUs, and the author did highlight that training would not be feasible for large scale\\nobject classiﬁcation would not be possible without training the networks on GPUs. AlexNet\\nwould not be feasible to train on a CPU because of the number of learnable parameters in the\\nnetwork. For example, LeNet had 802,816 weights whereas AlexNet had 62,378,344 weights.\\nThat means over 77 ×the number of learnable parameters that need to be iteratively updated\\nfor each backward pass through the network. Although AlexNet was not the ﬁrst network to\\nutilise a GPU for training, it is considered one of the most inﬂuential papers in computer vision\\nas it won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012 and, as\\na result, inspired many future designers to train CNNs for large scale object detection using\\nGPUs. Furthermore, the network architecture itself was well designed and therefore achieved a\\ntop-5 error of 15.3%, which outperformed the runner up with by more than 10.8% (Krizhevsky\\net al. 2012, Russakovsky et al. 2015).\\n3.2.3 VGG-16\\nShortly following AlexNet was the release of VGG-16, which was published by Simonyan &\\nZisserman (2014) from the Visual Geometry Group (VGG) at the University of Oxford. VGG-\\n30'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 42}, page_content='16 was primarily inspired by AlexNet and was entered into the ImagNet challenge in 2014, in\\nwhich it achieved a top-5 error rate of 7.32%, which is less than half that achieved by AlexNet\\nonly two years prior. VGG-16 was able to outperform AlexNet by reducing the kernel size\\nfrom 11×11 and 5×5 in the ﬁrst few layers of AlexNet to multiple 3 ×3 ﬁlters consecutively\\nshown in VGG models. The additional Conv layers resulted in 16 to 19 trainable layers, which\\ntook 2-3 weeks to train the network on NVIDIA Titan Black GPUs, as a result of 138 million\\nparameters (Simonyan & Zisserman 2014). VGG-16 indicated that increasing the depth of a\\nnetwork by having consecutive convolutional layers could result in better accuracy in large scale\\nimage classiﬁcation Simonyan & Zisserman (2014). However, the paper also indicated that\\nbeyond 20 layers, accuracy could decrease due to optimisation issues with SGD, with deeper\\nnetworks being outperformed by shallower CNNs. This issue is caused by vanishing/exploding\\ngradients He et al. (2016).\\n3.2.4 ResNet\\nResNet was proposed by He et al. (2016), and they identiﬁed an issue with training deeper net-\\nworks known as the vanishing/exploding gradient issue and a method to overcome it. As neural\\nnetworks are universal function approximators, in theory, by adding more layers to a DNN, they\\nshould learn progressively more complex features and offer better performance. He et al. (2016)\\nhighlights that most networks that are considered ‘deep’ contain roughly 16 to 30 layers and\\ndemonstrates how much deeper networks such as one with 56 layers can exhibit higher training\\nand test error than a shallower counterpart. Further, if this problem was caused by overﬁtting,\\nthen regularization techniques should combat this issue, but they do not. Therefore, this issue is\\nan optimization issue which was later called the vanishing/exploding gradient problem. When\\na network becomes too deep, the gradients used by the loss function in optimization tend to\\nzero after several iterations. This leads to weights no longer updating, and therefore no further\\nlearning occurs, causing demonstrated training and test loss not to reach the global theoretical\\nloss minimum (He et al. 2016).\\nHe et al. (2016) addresses the vanishing gradient problem by proposing the ‘residual block’,\\nwhich is demonstrated in Figure 3.3. The key characteristic of the residual block is the ‘skip\\nconnection’ identity mapping which adds the feature map outputted from a previous layer to\\na later layer as demonstrated in FIGURE. The previous feature map has to be multiplied by a\\nlinear projection to ensure that the dimensions of both feature maps are the same. These skip\\nconnections allow information to be passed between layers and prevent the vanishing gradient\\nproblem from occurring. Due to residual blocks, a network with 152 layers was produced with\\na top-5 error of 3.57%, and it won the ILSVRC 2015. Although this network is much deeper\\nthan previous networks such as VGG-16, it has less than half the parameters due to having far\\nless fully connected layers (He et al. 2016).\\n31'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 43}, page_content='Figure 3.3: The residual block (He et al. 2016)\\n3.2.5 EfﬁcientNet\\nIn the last few paragraphs in this section, various CNNs have been compared that all aim to\\nimprove accuracy on the ImageNet data set by increasing the network’s size whilst also trying\\nto improve the efﬁciency of the network. A key issue in designing CNNs is how to effectively\\nbalance the depth of the network (the number of layers), with the width of the network (the\\nnumber of ﬁlters or kernels), and the resolution of the network (the size of the input image).\\nIncreasing any of these dimensions will result in a greater computational overhead, which can\\nbe easily calculated and is measured in FLOPS, whilst also demonstrating further increases\\nin measured accuracy on data sets such as ImageNet. Continuously increasing the depth of\\nnetworks does not inﬁnitely improve the accuracy. It was demonstrated that ResNet-1000 has\\nsimilar accuracy as ResNet-101 despite having approximately 10 ×the amount of layers (Tan &\\nLe 2019). Researchers at Google, Tan & Le (2019), present a new state-of-the-art mobile sized\\nbaseline architecture accompanied by a compound scaling method to improve the accuracy of\\nthe baseline model whilst optimizing the efﬁciency of the up-scaled model. Compound scaling\\nrefers to simultaneously increasing at least two of the following: depth, width and resolution\\nnetwork (Tan & Le 2019).\\nThe ﬁrst signiﬁcant contribution of the paper by Tan & Le (2019) is that they propose\\nEfﬁcientNet-B0. The authors highlight that having a good baseline architecture is a critical\\ndesign decision for compound scaling, as making a given network larger does not change the\\nunderlying operators. Therefore they use a Neural Architecture Search, a technique for automat-\\ning the design of DNNs, that optimises by accuracy and FLOPS, resulting in EfﬁcientNet-B0.\\nThis network has only 5.3 million parameters and 0.39 billion FLOPS and achieves a top-5 ac-\\ncuracy of 93.3% on ImageNet (ResNet-152 achieved a top-5 accuracy of 93.8% with 60 million\\nparameters and 11 billion FLOPS.\\nEfﬁcientNet-B0 is underpinned by a type of short-cut connection, inspired by ResNet vari-\\nants, known as mobile inverted bottleneck connections (MBConv). MBCov uses the depthwise-\\nseparable convolution rather than the conventional convolution operation that is more widely\\nused. This concept essentially splits the convolutional operation into ﬁrstly a depthwise convo-\\n32'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 44}, page_content='lution, a spatial convolution performed on each input channel in isolation. This is followed by\\na pointwise convolution which is essentially a 1 ×1 convolution, which projects the outputted\\nchannels from the depthwise convolution into a new channel space chollet2017xception. This\\noperation results in a signiﬁcant reduction in parameters and makes convolutional layers far\\nmore computationally efﬁcient. However, it reduces the network’s learning capacity, which can\\nbe an issue in very small networks. MBConv blocks also use inverted residual blocks, which are\\nbased on ‘skip connections introduced for ResNet by He et al. (2016). In the original residual\\nblocks, skip connections are used to connect layers with many channels, known as wide layers.\\nThe connected layers bypassed multiple narrow layers with a comparatively smaller number of\\nchannels. However, for inverted residual blocks, the skip connections are between narrow layers,\\nwhich bypass wider layers (Sandler et al. 2018). The last key method implemented in MBConv\\nblocks is referred to as ‘Squeeze and Excitation’ blocks. These essentially assign weights to\\neach output channel rather than weighting them all equally. Which allows the network to learn\\nhow to prioritize different channels rather than treating all extracted visual features with equal\\nimportance (Hu et al. 2018).\\nTan & Le (2019) also released a method for compound scaling of convolutional neural net-\\nworks. If an input image has a larger resolution, the network requires additional layers to in-\\ncrease the receptive ﬁeld size and additional channels to capture ﬁne-grained visual information\\non the larger image. Figure 3.4 shows the seven variants of the EfﬁcientNet models, where B0\\nrepresents the baseline model, which takes an input image sized 224 ×224 and B7 represents the\\nlargest and highest ImageNet top-1 accuracy model, which takes an input image sized 600 ×600.\\nThe position of the red curve is to the top left of every other plotted point on FIGURE. This high-\\nlights the importance of the compound scaling method and an optimized baseline architecture,\\nas no other model with the same number of parameters can offer similar performance to the\\nEfﬁcientNet variants (Tan & Le 2019).\\nFigure 3.4: EfﬁcientNet performance on ImageNet (Tan & Le 2019).\\n33'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 45}, page_content='EfﬁcientNet-B0 will be selected as the ﬁrst pre-trained encoder used for the key point re-\\ngression model in this report as it offers a higher top-1 accuracy on ImageNet than ResNet-50\\nbut contains signiﬁcantly fewer parameters. Additionally, EfﬁcientNet-B4 will be used for an\\nadditional key point regression model, as this model offers the best trade-off between the num-\\nber of parameters and ImageNet top-1 accuracy, as shown in Figure 3.4. This will allow for\\nremarks to be made about the compound scaling method in the context of pose estimation.\\n3.3 Object Detection Algorithms\\nIn this section, state-of-the-art object detection models will be reviewed. An object detection\\nmodel will be used to produce bounding box coordinates that enclose the hands. A cropped im-\\nage based on the bounding boxes will be used for key point estimation. This approach has been\\nselected as it reduces the number of background pixels that are being inputted into the model,\\nand therefore, the model will likely be able to make better predictions. Feasible methods will be\\nselected for comparison for use in the automatic labelling study proposed in this study.\\nHumans can observe images and the world around them instantly. As eloquently expressed\\nby Redmon et al. (2016), humans can identify what objects are around them, where these objects\\nare located, and how to interact with objects and their surrounding environment. The human bi-\\nological visual processing system allows them to process visual stimuli fast and accurately. This\\nenables the brain to process a response to visual stimuli and perform complex tasks with little\\nconscious input, such as driving a car. Algorithms that can detect and localize objects with the\\nsame inference speed and accuracy as humans would allow tasks such as driving a car to be un-\\ndertaken by computers. Further, accurate algorithms that can identify and locate visual stimuli\\nallow for a multitude of new technologies that could beneﬁt humankind through computer-based\\nor robotic systems (Redmon et al. 2016).\\nOne of the fundamental visual recognition problems in computer vision is referred to as\\n‘object detection’. Object detection predicts not only object categories but also the location of\\neach object in the image. Formally, this would be referred to as combining image classiﬁcation\\nand object localization within the ﬁeld of computer vision (Wu et al. 2020). The state-of-the-art\\nobject detection algorithms are currently underpinned by deep learning algorithms, divided into\\ntwo key structures. Firstly, there are two-stage object detectors such as Region-Based CNN (R-\\nCNN) and developments of this approach such as fast and faster R-CNN (Girshick et al. 2014,\\nGirshick 2015, Ren et al. 2015). Secondly, there are one-stage object detectors such as YOLO\\nand similar variants such as revisions of YOLO and single-shot multibox detectors (Redmon\\net al. 2016, Bochkovskiy et al. 2020, Liu et al. 2016). Multi-stage detectors typically offer bet-\\nter accuracy and are considered state-of-the-art on open-source benchmark data sets, whereas\\nsingle-stage detectors require less memory and can run object detection in real-time and there-\\n34'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 46}, page_content='fore have far greater inference speeds (Wu et al. 2020).\\n3.3.1 Two-stage detection models\\nAs explained by Wu et al. (2020) two-stage detection models split object detection into two dis-\\ntinct stages. A sparse set of proposal regions is generated in the ﬁrst stage, and feature vectors\\nare generated for object class predictions in the second stage using a CNN for each proposal\\nregion. The proposal regions are generated such that all objects within a given image belong\\nto at least one proposal region. Each proposed region is then passed into a deep learning algo-\\nrithm that classiﬁes each proposal as a predeﬁned class label or background in the image. In\\nsome models, there is also additional reﬁnement of the original proposal region (Wu et al. 2020).\\nThe most famous multi-stage detector is known as R-CNN, which has been improved since\\n2014 producing newer algorithms such as Fast R-CNN, Faster R-CNN and SPP-net (Wu et al.\\n2020). Explained by Girshick et al. (2014) in R-CNN’s release paper in 2014, this new method\\noffered state-of-the-art performance on a public benchmark data set (the Pascal VOC2010), of-\\nfering a mAP of 53.7% which beat the predecessor by 13.3%. The R-CNN pipeline has four\\ndistinct components. Firstly, for each image, approximately 2000 sparse proposal regions are\\ngenerated using ‘Selective Search’, which is an algorithm designed to reject regions of the im-\\nage which can easily be identiﬁed as background areas (Uijlings et al. 2013). Secondly, each\\nproposal region is cropped to a ﬁxed size and passed into a CNN that outputs a feature vector.\\nThirdly, each feature vector is passed into a separate support vector machine (SVM) classiﬁer to\\nclassify the learned visual features of each proposed region into one of the pre-deﬁned classes.\\nLastly, the original proposal regions were reﬁned to tightly formed bounding boxes around the\\nclassiﬁed objects using feature maps extracted from the CNN. R-CNN implemented the idea of\\ntransfer learning by using an encoder network that was pre-trained on the ImageNet data set but\\nre-training ﬁnal fully connected layers for the detection application. Utilising transfer learning\\noffered performance gains, faster training times, and more robust learned visual features for\\nclassiﬁcation by the SVM (Girshick et al. 2014).\\nR-CNN was a novel approach that achieved state-of-the-art performance by utilising learned\\nvisual features extracted using a CNN encoder, which was more robust and informative than vi-\\nsual features learned by hand-crafted feature extractors, as used in previous methods as detailed\\nby Wu et al. (2020). Despite offering state-of-the-art performance, there were several ﬂaws as-\\nsociated with this two-stage object detection pipeline. Firstly, there were issues associated with\\nrepeated computation as the proposal regions often overlapped, and the visual features for each\\nproposal region was extracted by the CNN encoder individually . This meant that many pixels\\nfrom the original input image were passed through the CNN multiple times, which massively\\nhinders inference speeds. Furthermore, the Selective Search algorithm operated using low-level\\nvisual cues, which meant that it struggled to produce proposals for images with complex back-\\n35'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 47}, page_content='grounds and was not able to beneﬁt from parallel computation beneﬁts offered by GPUs. Due\\nto each stage of the network being fully independent of one another, the entire pipeline could\\nnot be optimised end-to-end, making it far more challenging to reach a global optimum for the\\ntwo-stage system (Wu et al. 2020). Despite this algorithm inspiring many other object detec-\\ntion models with different stages, the inability to optimise two-stage models in an end-to-end\\nmanner remains an inherent limitation of models structured this way therefore, they will not be\\nconsidered for hand detection in the automatic labelling system in this study.\\n3.3.2 One-stage detection models\\nOne-stage detectors do not have separate proposal generation and region classiﬁcation, and they\\nperform both of these tasks using one network. This is achieved by considering all regions\\nin the image as potential objects. All regions are then classiﬁed as a pre-deﬁned class or as\\nbackground (Wu et al. 2020).\\nYOLO\\nYou Only Look Once (YOLO) was proposed by Redmon et al. (2016), and was unique to pre-\\nvious object detection algorithms in the sense that it attempted to solve object detection using\\na regression-based approach. This single regression problem took the pixels of an image as\\ninput and directly outputted bounding box coordinates and class probabilities, making it partic-\\nularly simple when compared with two-stage detectors. YOLO utilises a single CNN that passes\\neach input image through the network in a single forward pass to predict object classiﬁcation\\nand localisation, eliminating the repetition of computation often present in two-stage detectors.\\nFurthermore, multiple bounding boxes for multiple objects are predicted in an image simulta-\\nneously, making it suitable for real-time two-object detection. The YOLO framework allows\\nend-to-end training through the use of a single CNN and supports real-time detection whilst\\nretaining high accuracy and precision. YOLO uses a network architecture that was inspired by\\nGoogLeNet for image classiﬁcation (Szegedy et al. 2015). The network used in YOLO has 24\\nconvolutional layers followed by two fully connected layers. The network is implemented on\\n‘Darknet’, an open source neural network framework written in C and CUDA. The network was\\ntrained on ImageNet-1000 for approximately 1 week and the model achieved a top-5 accuracy\\nof 88% on the ImageNet 2012 validation set. Due to having a lightweight network architecture\\nand being implemented on the Darknet framework, YOLO can make predictions at 45 FPS with\\na yet faster ‘Fast YOLO’ being able to make predictions at 155 FPS as it had fewer convolu-\\ntional layers (9 Conv layers instead of 24) (Redmon et al. 2016).\\nAs explained in by Redmon et al. (2016), YOLO places an S ×S grid onto the input image\\n(this was a 7×7 grid in the ﬁrst version of YOLO as shown in Figure 3.5). If the centre of an\\nobject falls within a given grid cell, that cell is responsible for detecting that object. Each grid\\ncell makes predictions for B bounding boxes and conﬁdence scores based on how accurate the\\n36'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 48}, page_content='predicted box is based on the IOU between the predicted box, and the ground truth labelled box.\\nEach cell produces a vector containing a numerical representation of the conﬁdence that the cell\\ncontained an object, the coordinates of the centre of the bounding box, the height and width of\\nthe bounding box, and the class of the object. YOLO uses a multi-part loss function which op-\\ntimizes the sum of squared error (SSE) for localization loss, conﬁdence loss, and classiﬁcation\\nloss simultaneously (Redmon et al. 2016).\\nFigure 3.5: YOLO Method (Redmon et al. 2016).\\nThe design of YOLO poses high spatial constraints on predicted bounding boxes. As the\\nimage is split into an S ×S grid and each cell can only predict up to two objects from only one\\nclass, the model struggles to predict multiple small objects in close proximity to one another in\\nimage space. As highlighted by the author, the model struggles to detect an entire ﬂock of birds.\\nAnother issue is that only the last feature map is used for making predictions. This means that\\nonly relatively course features are used for predicting bounding box coordinates, and therefore\\nstruggles to make predictions on objects at different scales or aspect ratios that are not present\\nin the training data. A further issue is that the main source of error is localization. The author\\nspeculates that this is caused by treating errors the same in small and large bounding boxes, as\\na small error in a large bounding box has a much smaller impact on IOU than a small error in a\\nsmall bounding box (Redmon et al. 2016).\\nYOLOv2\\nRedmon & Farhadi (2017) later proposed an improved version of YOLO named YOLOv2,\\nwhich addressed the limitations of its predecessor. This model maintained real-time inference\\nspeeds but achieved signiﬁcantly improved accuracy. YOLOv2 exhibited state-of-the-art perfor-\\nmance on public benchmark data sets, which was a tremendous contribution to object detection\\n37'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 49}, page_content='and computer vision. This was achieved mainly by three core design changes from the previous\\nversion of YOLO.\\nAs highlighted by Redmon & Farhadi (2017), a regularisation technique called Batch Nor-\\nmalization (BatchNorm) was used in YOLOv2 on all convolutional layers. BatchNorm es-\\nsentially takes all the outputted weights and bias terms from a given convolutional layer and\\nnormalises the values so that they all lie on the same scale. This helps prevent overﬁtting and\\nthe exploding gradient problem too. Furthermore, dropout layers can be removed without the\\nmodel overﬁtting as BatchNorm is a regularisation technique. By using BatchNorm, the mAP\\nof YOLO improved by 2% (Redmon & Farhadi 2017).\\nAnother key improvement made in YOLOv2 was by training the backbone network archi-\\ntecture on higher resolution images. Redmon & Farhadi (2017) states that all previous state-\\nof-the-art object detection methods utilise a backbone that is trained on ImageNet for object\\nclassiﬁcation. After the release of AlexNet, most of these networks take an image input size of\\n256×256. For YOLOv2, the network is initially trained on input images that are 224 ×224 and\\nthen the classiﬁcation network is ﬁne-tuned for a further 10 epochs, which take input images at\\na resolution of 448 ×448. This allows the network to learn more ﬁne-grained, higher resolution\\nvisual information and results in the mAP of YOLO improving by nearly 4%.\\nLastly, as outlined by Redmon & Farhadi (2017) the ﬁrst version of YOLO extracts visual\\nfeatures from an image using a convolutional network and then makes coordinate predictions\\nfor the bounding box using fully connected layers after the feature extractor. YOLOv2 adopts a\\ntechnique that was used in Faster R-CNN in which rather than predicting coordinated directly,\\nthey predict bounding boxes by using hand-picked priors (Ren et al. 2015). Instead of fully con-\\nnected layers, the network can use convolutional layers to predict offsets and conﬁdence values\\nfor hand-picked priors (Redmon & Farhadi 2017). Since the network now contains no fully\\nconnected layers, only convolutional layers, and the network only has to predict prior anchor\\nbox offsets rather than full bounding box coordinates, the problem is simpliﬁed, and learning is\\neasier for the network (Redmon & Farhadi 2017). Extending this, Redmon & Farhadi (2017)\\nproposed a method for automatically ﬁnding more appropriate anchor box priors than hand-\\npicked priors by using k-means clustering on the training set to ﬁnd more appropriate priors,\\nhelping both optimization and localization (Redmon & Farhadi 2017, Wu et al. 2020).\\nAnother signiﬁcant contribution of YOLOv2 was the backbone feature extractor that is used.\\nRedmon & Farhadi (2017) puts particular emphasis on the applications for detection, such as\\nrobotics and driver-less cars, relying on low-latency predictions. This requires that the backbone\\narchitecture has to be accurate but also must have rapid inference speeds. Many object detection\\nframeworks rely on VGG-16 as the backbone architecture as a base feature extractor as it is\\n38'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 50}, page_content='highly accurate and performs a top-5 accuracy score of 90%. However, it is excessively complex\\nand requires 30.69 billion ﬂoating-point operations for a 224 ×224 image. Therefore YOLOv2\\nproposes the Darknet-19, which contains 19 convolutional layers and 5 max-pooling layers.\\nThis was inspired by GoogLeNet and is an improved backbone to the network used in the ﬁrst\\nversion of YOLO. It only requires 5.58 billion ﬂoating-point operations per image and achieves\\na top-5 accuracy of 91.2%.\\nYOLOv3\\nYOLOv3 was released by Redmon & Farhadi (2018) which included a few small design changes\\nand a new larger network that made system more accurate whilst still maintaining real-time\\ninference speeds. The backbone used in YOLOv3 takes the previous backbone used in YOLOv2\\nand implements shortcut connections proposed in ResNet by He et al. (2016). In YOLOv2,\\nDarknet-19 was used which consisted of 19 convolutional layers with 5 max pooling layers,\\nthe new network used in YOLOv3 incorporates successive 3 ×3 and 1×1 convolutional layers\\nwhich allow shortcut connections and a much deeper backbone to be used. The new backbone\\nhas a total of 53 convolutional layers and is therefore named Darknet-53. Table 3.1 compares\\nDarknet variants with Resnet variants.\\nBackbone Top-1 Accuracy Top-5 Accuracy Billion Ops BFLOP/s FPS\\nDarknet-19 74.1 91.8 7.29 1246 171\\nResNet-101 77.1 93.7 19.7 1039 53\\nResNet-152 77.6 93.8 29.4 1090 37\\nDarknet-53 77.2 93.8 18.7 1457 78\\nTable 3.1: Comparing ResNet and Darknet Variants (Redmon & Farhadi 2018)\\nTable 3 was originally produced by Redmon & Farhadi (2018) and each network was trained\\nwith the same parameters and settings, taking input images that were 256 ×256 on a Titan X\\nGPU. This shows that by implementing shortcut connections, Darknet-53 obtains accuracy on\\npar with ResNet-152, which was the state-of-the-art performance in classiﬁcation, with Darknet-\\n53 having inference speeds that are 2 ×faster as indicated by the ‘FPS’ column in Table 3.\\nFurthermore, Darknet-53 achieved the highest measured billion ﬂoating-point operations per\\nsecond at 1457 BFLOP/s, meaning that the structure of the network is more optimal for utilising\\na GPU for training when compared to state-of-the-art ResNet variants.\\nAnother critical change in YOLOv3 is that it makes predictions across scales, rather than\\njust based on the ﬁnal output of the feature extractor, and this method was inspired by feature\\npyramid networks (Lin et al. 2017). This involves extracting feature maps at different stages\\nin the network hierarchy and up-sampling them to the same dimension as the ﬁnal feature map\\nthat is outputted from the network. The three feature maps are concatenated to produce a 3-\\n39'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 51}, page_content='D tensor in which anchor box offsets are predicted for the feature map at each hierarchical\\nlayer in the network that is considered. The ﬁnal bounding box prediction beneﬁts from the\\nprior contribution made on predicting bounding boxes for previous feature maps. Furthermore,\\nthis method allows ﬁner-grained visual information to be retained from earlier layers and more\\nmeaningful semantic information to be up-sampled for making ﬁnal predictions (Redmon &\\nFarhadi 2018).\\nYOLOv4\\nAt the end of the YOLOv3 paper, the researcher and developer of the three versions of YOLO\\nRedmon & Farhadi (2018) stated that they would no longer be undertaking computer vision\\nresearch due to ethical concerns of privacy and military applications. This left a vital question\\nin the computer vision community - would there be new and improved versions of YOLO in the\\nfuture? Bochkovskiy et al. (2020) published YOLOv4, which once again improved the accu-\\nracy to achieve state-of-the-art performance in real-time. This paper offers many contributions\\nwith several design changes that were evaluated in an ablation study, such as a new backbone,\\nnew regularization techniques, new activation functions, and new data augmentation techniques.\\nFull detail of these techniques are available in the original paper, and considering that all afore-\\nmentioned design choices included in the three previous versions of YOLO are still included\\nin YOLOv4 (Bochkovskiy et al. 2020), detailing all the new design choices are not possible in\\nthis review due to project constraints. All of the core design decisions for the YOLO algorithm\\nhave already been outlined in this section which are still essential building blocks of YOLOv4.\\nHowever, some key beneﬁts and techniques will be explained in a high-level overview.\\nBochkovskiy et al. (2020) explains that the current issue with CNN-based object detection\\nsystems is that the most accurate systems do not operate in real-time (30 FPS or more), and they\\nrequire multiple GPUs for training with large mini-batch sizes. This key issue is addressed in\\nYOLOv4, in which they take YOLOv3 along with various other design decisions to produce a\\nsystem with state-of-the-art accuracy that can be trained on a single conventional GPU such as\\nan NVIDIA 1080 Ti or 2080 Ti. YOLOv4 was the ﬁrst representative one-stage object detec-\\ntion model released with state-of-the-art performance since the release of YOLOv3, in which\\nsigniﬁcant gains in detection accuracy had occurred. On the MS COCO data set with a 416 ×\\n416 input image size, YOLOv3 achieved an AP of 31% at 35 FPS whilst YOLOv4 achieved an\\nAP of 43% at 31 FPS. This is a signiﬁcant increase in performance whilst still maintaining an\\ninference speed of at least 30 FPS meaning and is therefore still considered a real-time detection\\nsystem Bochkovskiy et al. (2020). This method achieves state-of-the-art accuracy and achieves\\nreal-time inference speeds and is therefore, will be used to detect the presence of a hand in the\\nML pipeline.\\n40'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 52}, page_content='Chapter 4\\nMethods\\nThis chapter explains the methods used for the analysis of this project. Firstly, it is important to\\nhighlight at which stage of the existing workﬂow the labelling phase exists for DLC and where\\nprecisely the contributions of this project will be in the DLC work pipeline. See Figure 4.1,\\nwhich displays a high-level overview of the DeepLabCut workﬂow in the ﬁrst ﬂowchart. This\\ninvolves using the DLC GUI to label video frames which are then inputted into a deep neural net-\\nwork for training. The core contribution of this project is to fully automate the labelling phase,\\nmeaning that you do not have to manually label the frames using the GUI whatsoever. Therefore,\\nsee the second ﬂowchart that highlights a new proposed DLC workﬂow. This involves creating\\na DLC project and extracting the frames as usual but passing these into an ‘Automatic Labelling\\nSystem’ (stage 2.) to label key points. Each frame of video will be labelled automatically and\\nthen passed back into the DLC workﬂow as usual.\\nFigure 4.1: Current and proposed DeepLabCut Workﬂow\\n41'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 53}, page_content='4.1 Automatic Labelling System: Pipeline\\nIn this section, the design of the automatic labelling system pipeline will be detailed. The\\npipeline illustrated in Figure 4.2 shows example inputs and outputs to the right-hand side of the\\nﬂow chart, demonstrating data examples at each stage. This pipeline was inspired by Google’s\\nsolution MediaPipe Hands which was released by Zhang et al. (2020) which similarly uses a\\npalm detector called ‘BlazePalm’ to detect the presence of a hand. BlazePalm outputs bounding\\nbox coordinates, which is used to produce a cropped image of the hand. This cropped image is\\nthen passed into a hand landmark model, which predicts 21 coordinates of the main joints in the\\nhand (Zhang et al. 2020). Unfortunately, this solution is not suitable for the data set used in this\\nproject as the palms are not visible in any of the frames, and therefore, the presence of the hand\\nwould not easily be detected by the model.\\nThe ﬁrst stage of the system pipeline is the ‘Object Detection Model’ to detect the presence\\nof a hand. The Object Detection Model takes a video frame as input as demonstrated by the\\n1920×1080 resolution image to the right-hand side. The hand detection model will output the\\nobject class, which will always be 0 corresponding to ‘hand’, the coordinates of the centre of\\nthe bounding box and the boxes height and width for this object. The green dot in Figure 4.2\\ndemonstrates the absolute coordinates for the centre of the box. The green dot coordinates are\\nvisualised in the second item in the example column in Figure 4.2. See the red square in Figure\\n4.2 indicating the bounding box. Two object detection models will be used for this task and\\ncompared to identify a preferred model, which are YOLOv4 and YOLOv4-tiny.\\nThese bounding box coordinates then need to be passed into a ‘Cropping Function’. In\\nwhich the image is cropped so that it only contains pixels within the bounding box. The cropped\\nimage contains less visual noise as fewer background pixels are present. The image’s resolution\\nin the example in Figure 4.2 has been reduced from 1920 ×1080 to 768×768. Therefore, more\\nthan 72% of the redundant pixels have been removed for passing the image of the participant’s\\nhand into the key point regression model.\\nThe cropped image of the hand is inputted into the ‘Key Point Regression Model’. This\\nconvolutional neural network will output a 4-tuple of the (x,y) coordinates for the thumb and\\nthe index ﬁnger. Two backbone architectures will be used for this task and compared to identify\\na preferred model, which are EfﬁcientNet-B0 and EfﬁcientNet-B4.\\nThe key point coordinates have been predicted on the axis of the cropped image. To be\\ninputted into DeepLabCut, they must be converted back to coordinates for the full resolution\\nimage. This will be achieved by passing the key point coordinates on the axis of the cropped\\nimage into a ‘Coordinate Conversion Function’, which will use the bounding box coordinates\\nfor each image to convert the key point coordinates relative to the axis of the 1080p image.\\n42'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 54}, page_content='Figure 4.2: Proposed ML Pipeline for the Automatic Labelling System\\n43'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 55}, page_content='4.2 Requirements\\nThe automatic labelling system requires three core components in order to meet the aims and\\nobjectives of this project, as illustrated in Figure 4.3:\\n1. There will be a ‘Data pre-processing phase’ to ensure it is in the correct format speciﬁed\\nfor the system that has been designed or selected.\\n2. There will be a ‘Model Training phase’ as the pre-processed data will need to be passed\\ninto the labelling system to build and train the models. All relevant ﬁgures, diagrams and\\noutputs will be collected.\\n3. There will be an ‘Evaluation phase’ in which all ﬁgures, diagrams and outputs produced\\nby the model will need to be discussed and analysed.\\nMultiple models will be compared for the object detection and key point estimation models,\\nwith a preferred solution identiﬁed for each model. 5-fold cross-validation will be undertaken\\nto evaluate the robustness of the key point regression model results. In which ﬁve different\\ntraining and test splits will be taken. For each split, both models will be trained, and the MSE\\nof the models will be compared, and an error analysis will be undertaken.\\nThe Object Detection model will be acceptable if it can crop all the images to contain the\\ntwo key points of interest, the tip of the thumb and the index ﬁnger. The core focus of this\\nproject will be identifying a key point estimation model that is both lightweight and accurate\\nenough to produce key point coordinates for DeepLabCut automatically.\\nFigure 4.3: Workﬂow of this project\\n44'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 56}, page_content='4.3 Data Collection\\nThe data used in this project is a subset of a data set that was previously used in two previous\\npublications with authors from Leeds Institute of Health Sciences in partnership with researchers\\nfrom other universities (Zhao et al. 2020, Williams et al. 2020).The data that has been used for\\nthis project contains a total of 75 video recordings of human hands (right hands only). The\\nvideos were captured of 40 Parkinson’s patients, 20 control patients (typically the spouse of a\\ngiven Parkinson’s patient), and 15 additional control patients (either hospital staff or children).\\nEach of the participants had given written consent for their data to be used in the study. Two\\nvideos were rejected early in the project for all students using this data, as they were ﬁlmed at\\n30 FPS which would likely cause issues if they were to be used in training a DeepLabCut model.\\nEach of the 40 Parkinson’s Patients had previously been diagnosed by a consultant neurol-\\nogist specialising in movement disorders at Leeds Teaching Hospitals NHS Trust. The Parkin-\\nson’s Patients were invited to attend a research clinic appointment speciﬁcally to gather this\\ndata set. Each of the diagnosed patients was subjectively and objectively in an ‘on’ motor state,\\nwhich means that they met the following criteria:\\n1. The patients reported that they felt ‘on’. This is a widely accepted and understood term\\nthat refers to patients ability to have an awareness that their medications are working\\neffectively and that the symptoms of their PD have been reduced by these medications.\\n2. The consultant neurologist reported that the patient looked ‘on’. This term is clinically\\naccepted that a trained specialist can observe a response in symptoms as a result of a\\nmedication taken by a PD patient.\\n3. The PD patient was currently taking their medication as prescribed and no medication\\nhad been missed prior to recording.\\nControl participants were invited that were companions or family members of the PD pa-\\ntients, and these were typically the spouse of the patient. There were also additional control\\nparticipants that were staff members at Leeds General Inﬁrmary. All control patients had no\\nprevious neurological diagnosis and were not on any medication that could cause PD like symp-\\ntoms such as tremor, rigidity, bradykinesia or any other movement impairment.\\nThe remaining 73 videos used for this project were ﬁlmed using an integrated smartphone\\ncamera on an iPhone SE. The integrated video camera was conﬁgured to record at 60 FPS at\\n1920×1080 pixels (1080p). The smartphone was placed on a tripod, and no additional lighting\\nwas used to enhance the recording, only ambient lighting in the hospital room. Each of the\\nparticipants placed their elbow on the arm of a chair and lifted their forearms to a 45◦angle\\nfrom the ground. The patient’s hands and arms were not ﬁxed or held in any other way and were\\ncompletely free to move as advised by ‘Item 3.4 of the Movement Disorder Society revision of\\n45'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 57}, page_content='the Uniﬁed Parkinson’s Disease Rating Scale’ (MDS-UPDRS) (Goetz et al. 2008). The only\\nvisible body parts in the video frame were the hand and forearm. The distance between the\\nsmartphone camera and the patient’s arm was approximately 1m. However, this was not strictly\\ndeﬁned, and the distance does vary within the sample.\\nEach of the participants in the study was instructed to tap their index ﬁnger and thumb “as\\nquickly and as big as possible” for at least 10 seconds. Digits 1 and 2, the thumb and index\\nﬁnger, respectively, were closest to the camera to try and prevent self occlusions of the key\\npoints of interest. No explicit instructions were given to the patients about the required position\\nof digits 3 to 5. Although the researcher gave a brief demonstration to the patients in which digits\\n3 to 5 were fully extended, some patients performed the tapping exercise with digits 3 to 5 fully\\nretracted (as if digits 3 to 5 were forming part of a closed ﬁst). Each video was manually edited\\nto make the total length of each video 11 seconds, 1 second of the patients hands stationary prior\\nto initiating the tapping exercise, followed by 10 seconds of the patient completing the tapping\\nexercise.\\n4.4 Data Preparation\\nThe data preparation process used to create the data sets for the models used in this study will\\nbe outlined in this section.\\n4.4.1 Labelling key points for DLC\\nOf the remaining 73 videos in the data set for this project, 63 (86%) videos were allocated for\\ntraining and 10 (14%) were allocated for testing. The training data set needed key points an-\\nnotated, such as tips of the ﬁngers and thumb, which will be explained later in this section. As\\nthere were ﬁve students in the cohort completing independent research projects on the same data\\nset, the 64 training videos were split between the ﬁve students for labelling, as this meant the\\nlabelling phase of the project for each student could be accelerated.\\nEach video is approximately 11 seconds long and was ﬁlmed at 60 fps for 660 frames per\\nvideo. To reduce the labelling burden whilst retaining sufﬁcient spatial semantics of the ﬁnger-\\ntapping exercise, every 4thframe was labelled. The frame rate of the videos was reduced from\\n60 fps to 15 fps to reduce the total number of frames for labelling from 44,000 to 11,000 frames\\napproximately. Labelling was completed by each of the 5 students on their own personal devices\\nin a local environment using the DeepLabCut Graphical User Interface (GUI). The process took\\napproximately 30 person-hours to complete due to the repetitive nature of the task. The aims\\nand objectives between each of the student’s research projects varied, and not all students would\\nuse each of the 6 points labelled on the training data. However, all students labelled all 6 points\\nto ensure every individual had sufﬁcient points labelled for their task at hand. The position of\\n46'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 58}, page_content='the points labelled are summarised in Figure 4.4.\\nFigure 4.4: Hand Labelling Protocol\\nWritten instructions were given detailing the exact position of each of the six key points,\\nas explained in Table 4.1. The two points of paramount importance are Points 1 and 2, as the\\ndistance between these two points produces the time series data used to classify each participant\\non the MDS-UPDRS.\\nPoint Description\\n1 Tip of the index ﬁnger (the point where ﬁnger and background meet)\\n2 Tip of the thumb (the point where the thumb and background meet)\\n3 Wrist joint at the bottom of the thumb muscle (the point where joint and background meet)\\n4 Wrist joint on top side\\n5 First knuckle of the index ﬁnger (the top of the peak)\\n6 Second knuckle of the index ﬁnger (the top of the peak)\\nTable 4.1: Hand labelling protocol. Each key point is visualised in Figure 4.4.\\n4.4.2 Pitfalls of labelling approach and data quality issues\\nA common issue with deep learning based key point estimation can occur due to issues with the\\nlabelled training data. For example, when labelling pixel-based coordinates for key points on\\nframes of video that are 1920 ×1080 (1080p) resolution, there are 2,073,600 pixels in each of\\nthe frames. Therefore, there is always likely to be a level of human variability as often, despite\\nhaving a detailed labelling protocol, there are geometric limitations to the body parts that are\\nbeing labelled. Further, a point described as ‘the tip of the index ﬁnger (the point where the\\nﬁnger and background meet)’ as described in TABLE will be inherently ambiguous as there\\nis no clear vertex for an index ﬁnger as the ﬁngertips is an irregular curve. The exact ‘tip’\\nof the ﬁnger may change based on the angle from which it is observed as the ﬁnger moves in\\n47'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 59}, page_content='relation to the perspective of the smartphone video camera. Therefore, this makes labelling the\\ntip ambiguous. Similar statements apply to the other six key points labelled in this data set.\\nThe implications of human labelling variability were summarised in detail by Mathis et al.\\n(2018) in which the author explains how this variability serves as a lower limit to the accuracy of\\nthe model in predicting key points for unseen test data. This is shown in Figure 4.5 in which the\\nRMSE (pixels) sits below the human variability within the training data, showing that the model\\nis able to track the labelled data with extremely high accuracy. However, the RMSE (pixels) for\\nthe test data converges to the human variability within the training data. It is worth noting that\\nthe model can still predict the test data extremely accurately as an approximate 2.5 pixel RMSE\\nis for an image that contains over 2 million pixels.\\nFigure 4.5: Human variability in labelling (Mathis et al. 2018)\\nThe extent of human variability is likely to depend on many factors. Including the animal or\\nbody part that is being labelled, the proportion that the body part occupies in the video camera\\nﬁeld of view, and how careful the individual(s) are in the labelling process. The tracking ac-\\ncuracy using DeepLabCut on this data set has been demonstrated to be mean absolute error of\\n8.39 pixels when trained over 1 million iterations on approximately twice the amount of videos\\npresent in comparison to the data used in this study (Zhao et al. 2020). However, the data was\\nrelabelled by ﬁve students independently in this study, meaning there is likely to be further hu-\\nman variability between each individual. Therefore, it is speculated that a human variability of\\n8 or 9 pixels can be considered as a likely minimum variability in the training data used for this\\nproject. The human variability will likely be greater than this, and this will be considered when\\nevaluating the performance of the ﬁnal model.\\nA further issue with labelling key points for animal pose estimation is referred to as ‘soft-\\ntissue artefact’, which is a commonly used term amongst movement scientists and refers to the\\ndeformation of tissues lying beneath the skin such as muscle or fat, which serves as a common\\n48'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 60}, page_content='obstacle to tracking accurate skeletal kinematics (Camomilla et al. 2017). In the data used for\\nthis project, for example, it may be inherently difﬁcult to label the wrist joint on the top side as\\noften the joint may not be clearly visible if a participant in the study does not have pronounced\\nbones in this area due to the bones being covered by a sufﬁcient amount of fat or muscle. See\\nFigure 4.6 below, which highlights the ambiguity in the position of the wrist joint on the top\\nside due to soft tissue artefact as indicated by the annotated yellow box. Soft tissue artefact was\\nsomething that the author of this paper observed to be a potential issue for key points labelled\\n3,4,5,6 in Figure 4.4 and Table 4.1 in certain patients and is therefore speculated to increase the\\nhuman labelling variability present in the training data. These points are the upper and lower\\nwrist and the two labelled knuckles on the index ﬁnger. See the yellow box in Figure 4.6 to\\nvisualise how the exact pixel location is likely to vary for these key points.\\nFigure 4.6: Data quality issues. Left: Soft Tissue Artefact. Right: Self-occlusion.\\nAnother frequent data quality issue present in the data set were self-occluded key points,\\nas shown in Figure 4.6, in which the thumb occludes the tip of the index ﬁnger. As per the\\nguidance of the DeepLabCut developer, these labels were skipped and not labelled (Nath et al.\\n2019). This was only an issue for the tip of the index ﬁnger in the data which means that these\\nframes should not be used in training the ﬁnger detector in this report. Therefore there are 156\\nframes that were omitted from the data set. Only the frames with self-occlusions were omitted,\\nnot the entire video allowing any frame with complete labels to be utilised for training.\\nSome data quality issues arose from the video camera quality itself. As previously stated,\\nthe video was captured using an integrated smartphone video camera (iPhone SE) which was\\nconﬁgured to record video at 60 fps at 1080p. The videos were taken in ambient lighting con-\\nditions with no additional specialist lighting used to capture the video recordings. Figure 4.7\\nshows that the position of key point 6 (the second knuckle of the index ﬁnger ) is hard to distin-\\nguish against the position of the second knuckle of the middle ﬁnger as it is difﬁcult to make out\\nthe ﬁnger boundaries due to general blur in the image. This blur results in the camera not being\\nfocused on the hand, which is likely due to the poor lighting in this participants video recording.\\nIn Figure 4.7, it is clear that the lighting incident on the hand in the left image is of a much lower\\nluminosity than the light rays incident on the hand in the right image. General blur caused by\\n49'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 61}, page_content='the integrated camera not being properly focused on the hand is likely to further increase human\\nvariability within the training data.\\nFurthermore, see Figure 4.7 right, indicating a frame example in the data set in which the\\nindex ﬁnger is more blurry than the rest of the hand in the image. This phenomenon is re-\\nferred to as ‘motion blur’ and is a common issue when capturing fast-moving objects in video\\nfootage (Dai & Wu 2008). This is a prevalent data quality issue in the data set, particularly\\nwith control participants rather than PD participants. This is because the control participants\\ncan perform the ﬁnger-tapping exercise at a much faster speed/frequency than the PD patients,\\nresulting in more frames with motion blur occurring. As this issue occurs in multiple frames,\\nit may not be a massive issue as the key point estimation model produced in this project may\\nhave the learning capacity to identify motion blur with the tip of the index ﬁnger. However, in\\ncases of extreme motion blur where the tip is not visible in the frame at all, the model is likely\\nto struggle to label the index ﬁnger. Motion blur is likely to increase the human variability of\\nlabelling this key point in the training data as the exact position of the index ﬁnger is not clear\\nin the frame, as shown in Figure 4.7 right.\\nFigure 4.7: Additional data quality issues. Left: Blur. Right: Motion Blur.\\nFigure 4.8 shows a frequency distribution of the Euclidean distance between the labelled\\ncoordinates of the tip of the index ﬁnger and thumb in the data used in this project. The mean\\nEuclidean distance between the index ﬁnger and thumb tip is 191.67, and the standard devi-\\nation is 152.05. The distribution peak is around 50 px, with over 1000 video frames having\\na Euclidean distance of 50 px. PD patients cannot reach the same peak amplitude as non PD\\npatients, which contributes to the largest peak being below 150 px. Videos of all patients con-\\ntain frames in the ‘closed position’, where the index ﬁnger and thumb are touching. Therefore,\\nthe ‘closed position’ is oversampled. Furthermore, the key point estimation model’s accuracy\\nwill likely be overestimated because of this. The majority of samples in the data set are in the\\nclosed position, and the ground truth location of the tip of the index ﬁnger and thumb occupy a\\n50'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 62}, page_content='very small amount of pixels. Therefore, it is likely that the model will easily predict key point\\nlocations for the closed position, but the majority of the error associated with predictions will\\nbe in the ‘open position’ at maximum amplitude.\\nFigure 4.8: Frequency distribution of Euclidean distance between index ﬁnger and thumb\\n4.4.3 Converting DLC Keypoints to Bounding Box Labels\\nThere are approximately 10,000 images of hands labelled with key point coordinates that were\\nlabelled by ﬁve students for DeepLabCut as outlined in Section 4.4.1. In order to make a bound-\\ning box model for object detection, ground truth bounding box labels were created for the train-\\ning data. There are several methods to manually label bounding boxes onto images using GUIs\\nas programmes such as ‘v7labs’ recommended by the developer of YOLOv4 (Bochkovskiy et al.\\n2020). However, this would be very time consuming, especially as the labelling phase for the\\napproximate 10k images took ﬁve people a total of approximately 29 person-hours (of constant\\nlabelling at 100% efﬁciency of a highly repetitive task). Due to the time constraints of this\\nproject and also concerns of labelling accuracy if the task were to be rushed, a decision was\\nmade to use the existing key point labels and convert these into bounding box coordinates for\\nthe object detection model.\\nSeveral possible approaches can convert the existing key point labels from Section 4.4.1 to\\nground-truth bounding box labels. The objective of the bounding box model is to crop the hand\\ncontained in the image to reduce the input dimensions for the regression model. A lower resolu-\\ntion image will make the regression model more lightweight (and therefore suitable for mobile\\napplications) while retaining as much spatial visual information about the hand and ﬁngers. The\\nkey point regression model will take an input image with a ﬁxed size and a square aspect ratio\\n(i.e. 224×224×3). Therefore, a decision was made to deﬁne the bounding box labelled coordi-\\nnates to the same dimensions for all images. The beneﬁt in doing this is that when the cropped\\nimage is outputted from the bounding box model, they are all the same dimension.\\n51'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 63}, page_content='Therefore, when all cropped images that have been outputted from the bounding box model\\nare resized from 768 ×768 to 224×224 for the key point regression model, they are all resized\\nby the same scaling factor. This means that the pixel error of the model can be analysed and dis-\\ncussed with greater conﬁdence that any given error is directly caused by the model, rather than\\nbeing caused by different scale factors for images of different image resolutions. For example,\\nif some cropped images were 448 ×448×3, and others were 672 ×672×3, the images would be\\nreduced by a scale factor of 2 and 3, respectively. Furthermore, due to the different scaling fac-\\ntors, the human labelling error in the original key point labels would also be resized by different\\nscale factors. If the bounding box model were designed to output bounding boxes that cropped\\nthe images of hands to several different sizes, then it would be more challenging to make ac-\\ncurate conclusions about the pixel error of the key point regression model. A further beneﬁt to\\ndeﬁning all the bounding boxes to a ﬁxed size is that the proportion of the ﬁeld of view occupied\\nby the hand and ﬁngers will vary between images. The distance between the camera and the\\nparticipant’s hand varied when the data was collected, despite being instructed to sit 1m from\\nthe camera. Therefore, the hands occupy a different proportion of the cropped images, as shown\\nin Figure 4.9, two 1080p images from the original data. If two boxes of the same dimension\\nwere drawn around each hand, the hand in the left ﬁgure would occupy a large proportion of the\\ncropped image, whereas the hand in the right ﬁgure will occupy a much smaller proportion of\\nthe cropped image.\\nFigure 4.9: Different proportion of original image occupied by hand: Left: Large hand relative\\nto 1080p image space. Right: Small hand relative to image space.\\nOn the other hand, if a dynamic bounding box were used, it would perfectly crop all hands\\nwith no redundant background pixels. When this was resized to the original input image, the\\nhand would always occupy the vast majority of the cropped region, with a similar proportion of\\nthe ﬁeld of view occupied in image space for all examples in the training data. This could make\\nthe key point regression model less robust to labelling key points at different scales. Further-\\nmore, the variability in the proportion of the cropped image occupied by the hand may enhance\\nthe model’s ability to associate the spatial information of the hands with the labelled coordinates\\ndue to more variability in the training data.\\n52'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 64}, page_content='YOLOv4 label format\\nA speciﬁc bounding box label format is required for the YOLOv4 object detection algorithm.\\nThe developer states that for each image in the training data, a ‘.txt’ ﬁle must be created con-\\ntaining ground truth labelled annotations with the same ﬁlename preﬁxing the ‘.txt’ and ‘.png’,\\nlabel and image pair@(Bochkovskiy et al. 2020). The ‘.txt’ ﬁle must contain each of the values\\nin Table 4.2 for each object in the image, with a new line for each object in the image.\\nValue Description\\nobject class integer object number from 0 to (classes-1)\\nxcentre x coordinate of the centre of the bounding box, normalised to the width of the image\\nycentre y coordinate of the centre of the bounding box, normalised to the height of the image\\nwidth the width of the bounding box, normalised to the width of the image\\nheight the height of the bounding box, normalised to the height of the image\\nTable 4.2: Bounding box ground truth annotation format\\nAll normalised values were calculated by dividing the absolute value by the total image\\nwidth in each plane. For example, the normalised x coordinate of the centre of the bounding\\nbox is given by;⟨xcentre⟩=⟨xabsolute⟩/⟨image width⟩. So in summary, for a hypothet-\\nical training image named ‘img1.png’ there would be a corresponding ﬁle called ‘img1.txt’\\ncontaining the values shown in Figure 4.10. For absolute clarity the columns represent the\\n⟨object-class⟩⟨xcentre⟩⟨ycentre⟩⟨width⟩⟨height⟩but these labels must notbe in the ﬁle.\\nThis example has one line for one bounding boxes. For the data used in this project, there is\\nonly one class which is ‘hand’, and there is only one object per image. Therefore, there is only\\n1 line per ‘.txt’ ﬁle.\\nFigure 4.10: Example bounding box ground truth annotation ‘.txt’ ﬁle\\nIn order to convert the existing six key points that were labelled for use in DeepLabCut to\\nan appropriately sized bounding box that is annotated in the format required for YOLOv4, the\\nfollowing steps were undertaken.\\n1.Gather DLC key point coordinates into a single data frame - Each of the 63 videos\\nhad an h5ﬁle containing approximately 165 rows of coordinates for the 6 key points,\\none row for each frame of video. All of the h5ﬁles were read in using the Python library\\npathlib into a pandas data frame. A ﬁnal data frame with 10,371 entries was assigned\\nto a variable named master df.\\n53'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 65}, page_content='2.Image inspection to determine bounding box dimensions - Using the Python PIL\\nImage library, video frames were inspected for the largest hand to determine an appro-\\npriate bounding box size and position for bounding boxes. A decision was made that the\\ncentre point of each box would be deﬁned as the mid-point between the thumb coordinate\\nand the coordinate of the ﬁrst knuckle of the index ﬁnger. After visualising different box\\ndimensions such as 672 ×672 and so on, it was decided that for the YOLOv4 model, the\\nmost appropriate dimension was 768 ×768 as this dimension contained all key points of\\nimage whilst not going outside of the dimensions of the original image in the majority of\\nphotos. An example of the bounding box coordinates has been displayed in Figure 4.11,\\nin which the green point is the box centre based off the midpoint between the tip of the\\nthumb and the ﬁrst knuckle of the index ﬁnger. The red box is a 768 ×768 bounding box\\nwith the green point at its centre. Also, the red bounding box clearly contains the two key\\npoints of interest for this project which are the tip of the index ﬁnger and the tip of the\\nthumb.\\nFigure 4.11: Deﬁning bounding box dimensions using existing DLC key point labels\\n3.Renaming all image ﬁles to have unique values - For each patient, there were approxi-\\nmately 165 images that all were named in a sequence from ‘img000.png’ to ‘img165.png’\\nor similar. Therefore, as there were a total of 63 participants, there were also 63 images\\nnamed ‘img000.png’ which would not be appropriate for YOLOv4 as each image needed\\nto be renamed to have a unique value using the pathlib library in Python. Therefore,\\neach image was renamed to contain the Participant ID preﬁxing the image frame number.\\nFor example, this would be ‘P52 R10slowfps img068.png’ in which this is the image\\nfor the PD patient 52, video frame number 068.\\n4.Normalising the bounding box values and adding a class value for each image - Each\\nof the coordinates and the height and width of the bounding box dimensions were deﬁned\\n54'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 66}, page_content='on an absolute scale of the original input image. These needed to be normalised to be the\\ncorrect format for YOLOv4. Therefore the height value was normalised by computing\\n768/1080 = 0.711and the width value was normalised by 768/1920 = 0.4and the\\n(x,y) coordinates for the centre of the box were computed similarly taking 1080 for x\\ncoordinates and 1920 for y coordinates. New columns were added to the master df\\ndata frame using the pandas library in Python for the normalised values. Furthermore,\\na class value was appended to master dfwith a value of ‘0’ for all images, as there is\\nonly one class for this data (hand) and only one hand per image. The class value must be\\na integer value. If this is a ﬂoat value (i.e. 0.0), then YOLOv4 will interpret the second 0\\nas the x dimension for the centre of the bounding box and the model will systematically\\nproduce bounding boxes on the far left side of the image.\\n5.Creating new data frame containing only relevant values - A new data frame was\\ncreated using the pandas library in Python and was named yolo dfthis contained\\nthe bounding box annotations only. The object class, (x,y) coordinates of the centre of\\nthe box and height and width of the bounding box were merged into one column called\\n‘txt’, which contained the relevant values in the format that YOLOv4 requires. All the\\nindividual columns were dropped as they were no longer relevant. The ﬁnal yolo dfis\\nshown in Figure 4.12. The drop.na() method was used to remove all rows with empty\\nvalues, reducing the data frame from 10,371 to 10,367. These were due to self-occluded\\nindex ﬁngers.\\nFigure 4.12: Pandas dataframe named ‘yolo df’ containing formatted bounding box values\\n6.Creating ‘.txt’ ﬁles - Finally, a ‘.txt’ ﬁle was created for each frame using a Python\\nﬁlehandle, which took the ‘patient ID’ and ‘img no’ columns displayed in Figure 4.12 for\\nthe ﬁle name and the ‘txt’ column for the contents of the annotation ﬁle.\\n7.Creating training and validation data - All of the images and annotation ﬁles were then\\nadded to a single folder and split into training and validation data. This was done using\\nthepathlib andshutil libraries in Python. The data was split so that there was 80%\\nof the data used for training and 20% of the data used for validation. These folders were\\n55'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 67}, page_content='compressed into ‘.zip’ ﬁles to be uploaded to a Google Drive for use in Google Colab.\\nResulting in a total of 8268 training samples and 2102 validation samples.\\n4.4.4 Converting DLC Keypoints to data set for Key Point Regression model\\nOnce the DLC labelled data was converted into a data set with labelled bounding box coordinates\\nto build the object detection model, an additional data set was required to build the key point\\nregression model. As the bounding box model has been designed to output cropped images that\\nare ﬁxed in dimension, these could be cropped prior to building the object detection model using\\nPython to enable both the object detection model and the key point regression model to be built\\nsimultaneously. Therefore, the ground truth bounding box coordinates were used to crop all the\\noriginal images.\\nCropping the images\\nAn issue was discovered with the data set for the object detection model, shown in Figure 4.13.\\nThe 768×768 bounding box ground truth annotations did not contain the tip of the index ﬁnger\\nin all frames. Therefore, to produce the data set for the key point regression model, the ground\\ntruth bounding box dimensions were increased to 1120 ×1120 to ensure that the cropped images\\ncontained both the tip of the index ﬁnger and thumb for all video frames.\\nFigure 4.13: Issue caused by 768 ×768 bounding box: Index ﬁnger outside of box\\nThe images were cropped in Python using the following steps:\\n1.Copy original dataset and create dataframe of all coordinates - All original images\\nfrom the DLC data set were copied to a new directory to be cropped using the shutil.copy()\\nmethod. Also, similarly to the ﬁrst step taken in when creating the YOLOv4 data set the\\nh5ﬁles were all read into a data frame assigned to the variable master dfusing the\\npathlib andpandas libraries in Python.\\n56'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 68}, page_content='2.Determining appropriate box size for cropping - The PIL.Image.crop() method\\nwas used for cropping the images from their full resolution to a reduced resolution for the\\nlightweight key point regression model. When attempting to crop the images using the\\nground truth bounding box annotations, it was discovered that the (768 ×768) bounding\\nboxes would not contain the coordinate of the index ﬁnger. This is an issue as this is\\nan essential point for the regression model. Therefore, a ﬁnal bounding box size was\\n1120×1120 as this resulted in the tip of the index ﬁnger and thumb fully enclosed by the\\nbounding box in all frames.\\n3.Cropping the images - The images are being cropped based on ground truth bounding\\nbox annotations. This produced a data set for the key point regression model without re-\\nquiring the YOLOv4 model to be ﬁnalised for this project. The PIL.Image.crop()\\nmethod was used which takes a 4-tuple of the left, right, top, and bottom boundary lines of\\nthe desired crop region relative to the image space of the original input image. This can be\\nexplained when visualising FIGURE in which the red box represents the desired cropped\\nregion. The top boundary line for the cropped region lies at approximately y=300, the\\nright boundary line lies at x = 1250. These were calculated using the following computa-\\ntions for all images:\\n(a) Left = Centre x - (Box width / 2)\\n(b) Right = Centre x + Box width / 2)\\n(c) Top = Centre y - (Box height / 2)\\n(d) Bottom = Centre y + (Box height / 2)\\nThis was converted into a 4-tuple which was then used to crop the images with the afore-\\nmentioned PIL.Image.crop() method.\\nConverting the key point coordinates\\nOnce all the images had been cropped, the coordinates of the key points also needed to be up-\\ndated. The original labelled coordinates are based on the origin of the 1080p image, represented\\nby the black dot in Figure 4.14. As the relative position of each hand varied between participants\\nand also different frames for the same participant, the coordinates needed to be updated relative\\nto the new origin of the cropped image. See Figure 4.14 in which the key points of the hand\\nhave been deﬁned and the dimensions of the bounding box used to crop the image.\\n57'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 69}, page_content='Figure 4.14: Converting key points coordinates from original image to cropped image\\nThe following steps were undertaken to convert the coordinates of the tip of the index ﬁnger\\nand thumb from their position on the original 1920 ×1080 image to their relative position on the\\ncropped image:\\n1.Finding the origin of the cropped region - Firstly, the coordinates of the cropped region\\nneed to be calculated. The origin of the cropped region is represented by the yellow point\\nin Figure 4.14, which have the coordinates of (Ox2,Oy2)on the axis of the original input\\nimage. These values can be calculated using the coordinates of the centre of the cropped\\nregion as deﬁned in FIGURE, and the dimensions of the cropped region as follows:\\n(a)Ox2=Cx−(W/2)\\n(b)Oy2=Cy−(H/2)\\n2.Updated coordinates for the tip of the index ﬁnger - Now that the origin of the new\\ncropped region has been calculated on the coordinate axis of the input image, the updated\\ncoordinates can be calculated for the tip of the index ﬁnger relative to the origin of the\\nnew cropped region. This is given by:\\n(a)Fx2=Fx1−Ox2\\n(b)Fy2=Fy1−Oy2\\nWhereFx1is the original x-coordinate of the tip of the index ﬁnger on the image axis of\\nthe original 1920×1080 image and Fx2is the updated x-coordinate for the tip of the index\\nﬁnger on the image axis of the cropped region, based on the new origin of the cropped\\nregion.\\n3.Updated coordinates for the tip of the thumb - The updated coordinates of the tip of\\nthe thumb are calculated similarly to the tip of the index ﬁnger:\\n58'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 70}, page_content='(a)Tx2=Tx1−Ox2\\n(b)Ty2=Ty1−Oy2\\nWhereTy1is the original y-coordinate of the tip of the thumb on the image axis of the\\noriginal 1920×1080 image and Ty2is the updated x-coordinate for the tip of the thumb\\non the image axis of the cropped region, based on the new origin of the cropped region.\\nSee Figure 4.15, which shows the plotted coordinates for the tip of the index ﬁnger and\\nthumb and on the image axis of the cropped image. This is the cropped image that was shown in\\nFigure 4.14, and when comparing the two images you can observe that the axis for the original\\nimage and the cropped image has changed. It is clear that the tip of the middle ﬁnger has\\nbeen cropped out of image space by using this method. The model may struggle to learn visual\\nfeatures associated with this class if in some training instances certain digits are contained within\\nthe ground truth bounding box label and in other instances they are not contained within the\\nground truth bounding box label. However, it was decided that as long as long as the bounding\\nbox contained the two key points required for the regression model in this project (the tip of the\\nthumb and the tip of the index ﬁnger), that this was not an issue for this study.\\nFigure 4.15: New coordinates relative to origin of cropped image\\n4.5 Bounding Box models\\nThis section of the report outlines the network architectures for models proposed for the ‘Object\\nDetection Model’ in the ML pipeline to produce bounding box coordinates as a crop region\\nfor the image of a hand. The two models will be trained to detect a single class (hand), and\\nthe two models will be compared with a preferred model identiﬁed. The two proposed mod-\\nels are YOLOv4 and YOLOv4-tiny, which share similar structures and methods; however, the\\ntiny version contains signiﬁcantly fewer parameters. Both networks methods were released\\nby Bochkovskiy et al. (2020) and YOLOv4 achieves state-of-the-art results on the MS COCO\\n59'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 71}, page_content='data set 43.5% AP (65.7% AP50) at 65 FPS on a Tesla V100. Also on the MS COCO data\\nset, YOLOv4-tiny achieves 22.0% AP (42.0% AP50) at a speed of 443 FPS on an RTX 2080\\nTi (Bochkovskiy et al. 2020). Clearly, YOLOv4 achieves an accuracy that is far higher than\\nYOLOv4-tiny, however, it has an inference speed that is nearly 7 times slower, which requires a\\nhigh-end GPU. Furthermore, YOLOv4-tiny has been demonstrated to achieve 25 FPS inference\\nspeeds using CPUs, making it feasible for deployment on mobile and embedded devices (Jiang\\net al. 2020). Despite YOLOv4-tiny showing lower AP metrics on the MS COCO data set, is it\\naccurate enough to produce bounding box coordinates for a single-class model?\\n4.5.1 Environment\\nYOLOv4 is underpinned by Darknet, which was published by the developer of YOLOv1-3,\\nRedmon (2013–2016). Darknet is an open-source neural network framework written in C and\\nCUDA. It is fast and easy to install using the Linux command line and supports CPU and GPU\\ncomputation. It has only two dependencies, OpenCV for image processing and CUDA for GPU\\nsupport.\\nA virtual environment called ‘Google Colab Pro’ was used for the object detection models\\nused in this project. The data set and YOLOv4 conﬁguration ﬁles were uploaded to a Google\\nDrive, which was mounted to a Colab virtual machine (VM). Subsequently, the Darknet git\\nrepository was cloned and built to the same VM. Weights to the relevant model were down-\\nloaded that were pre-trained on the MS-COCO data set. A key beneﬁt of using these pre-trained\\nweights is that the MS-COCO is already trained on the class ‘person’ and therefore, the model\\nis likely to generalize quickly as the visual features associated with the class ‘hand’ will be a\\nsubset of the class ‘person’ of which the weights can already classify. Google Colab Pro gives\\nshared allocation to a Tesla P100-PCIE-16GB GPU and 26GB of RAM, allowing for much\\nfaster training times than if undertaken on a laptop.\\n4.5.2 YOLOv4: Network Architecture\\nYOLOv4 consists of three core stages; a backbone, neck, and head. The backbone is CSPDarknet-\\n53 which is pre-trained on ImageNet and can extract visual features for object detection. The\\nDarknet variant used in this model has 137 pre-trained convolutional layers with ‘Mish’ acti-\\nvation functions. The necks used in this system are Spatial Pyraming Pooling (SPP) and Path\\nAggregation Network (PAN), which extract feature maps at various stages of the backbone and\\nallow for the use of ﬁne and coarse visual feature maps be used for the ﬁnal predictions in object\\ndetection. The SPP block is added since it signiﬁcantly increases the receptive ﬁeld size of the\\nnetwork, which separates the most important spatial features whilst having minimal impact on\\ninference speeds. Lastly, the head in this system is the anchor box method used in YOLOv3,\\nwhich is used to predict the class, conﬁdence scores, and ﬁnal bounding boxes for objects.\\n60'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 72}, page_content='4.5.3 YOLOv4-tiny: Network Architecture\\nYOLOv4-tiny is the compressed version of the YOLOv4 architecture designed by Bochkovskiy\\net al. (2020). The network has been simpliﬁed and the number of parameters have been reduced\\nto make it viable for mobile and embedded devices (Jiang et al. 2020). The Darknet variant\\nused in this model only has 29 pre-trained convolutional layers compared with 137 in the full\\nYOLOv4 model. Furthermore, the ‘Mish’ activation function was replaced with ‘Leaky ReLU’\\nactivation functions to reduce computational overhead further. YOLOv4 tiny still uses PANet\\nfor the neck to extract feature maps at different scales and anchor-based YOLOv3 for the head of\\nthe network for making ﬁnal predictions. However, it does not use SPP, and therefore YOLOv4-\\ntiny has a smaller receptive ﬁeld than YOLOv4. One further key difference is that YOLOv4-tiny\\nuses only two heads, whereas YOLO-v4 uses a total of three heads. Therefore, YOLOv4-tiny\\nwill make fewer anchor box predictions per frame of video, which speeds up the network and\\ncontributes to a lower AP on MS COCO. A high-level overview of the network is given in Table\\n4.3, and for further details of the network architectures, please refer to papers the following cited\\npapers, as explaining all design decisions in detail is not possible due to the constraints of this\\nproject (Bochkovskiy et al. 2020, Jiang et al. 2020).\\nComponent YOLOv4 YOLOv4-tiny\\nBackboneCSP-Darknet\\n137 convolutional layers\\nMish activation functionsCSP-Darknet\\n29 convolutional layers\\nLeaky ReLU activation functions\\nNeck(s) SPP and PANet PANet only\\nHead 3×YOLOv3 (Anchor Based) 2 ×YOLOv3 (Anchor Based)\\nTable 4.3: Comparison of object detection models used in this study\\n4.5.4 Training\\nConﬁguration (cfg) ﬁles containing the network architectures and the training ﬁles were down-\\nloaded from the GitHub repository published by Bochkovskiy et al. (2020). These cfg ﬁles for\\neach network were downloaded for training object detection for a custom data set and adapted\\nusing speciﬁc instructions listed on the GitHub repository. These changes are summarised in\\nAppendix A.1.\\n4.5.5 Evaluation\\nBochkovskiy et al. (2020) instructs users that you usually 2000 iterations is sufﬁcient per class.\\nHowever, you should ideally train for at least as many iterations as you have training images\\nand not less than 6000 iterations in total. Furthermore, the model can stop being trained when\\nthe average training loss no longer decreases for many iterations. Additionally, a target training\\nloss value can be from 0.05 (for a small model with a simple an ‘easy’ data set) to 3.0 (for a\\n61'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 73}, page_content='big model with a ‘difﬁcult’ data set) (Bochkovskiy et al. 2020). As the data set in this class\\nonly contains one class, and as YOLOv4-tiny is a smaller model, if it has sufﬁcient learning\\ncapacity for this task it should achieve an average training loss of 0.05 as a target. For the\\nlarger YOLOv4, the target should be less than 3.0 as this study’s data set contains only one class\\nand could be considered more simple than the MS COCO data set, which contains 80 classes.\\nFurthermore, once training has ﬁnished, the mAP should be checked for weights that have been\\nsaved at different checkpoints throughout training. This means that the mAP will be compared\\nevery 1000 iterations, and there will be six checkpoints per model. The checkpoint with the\\nhighest mAP can have a higher training loss than other checkpoints as the model can overﬁt the\\ntraining data. By selecting the weights with the highest mAP, the ‘early stopping point’ can be\\nidentiﬁed: the point with the highest mAP but not necessarily the lowest average training loss.\\nTherefore, the ﬁnal model will be less likely to overﬁt the training data.\\n4.6 Key point regression models\\nThis section of the report outlines the network architectures for models proposed for the ‘Key\\nPoint Regression Model’ in the ML pipeline to produce key point coordinates of the tip of\\nthe index ﬁnger and thumb for the cropped images. The two models will be trained to pre-\\ndict a 4-tuple containing the coordinates ⟨thumb x,thumb y,finger x,finger y⟩for each frame\\nof video inputted to the model. The MSE of the two models will be compared, and a pre-\\nferred model will be identiﬁed, with example predictions shown. The two proposed models\\nare EfﬁcientNet-B0 and EfﬁcientNet-B4. These networks belong to the same family of net-\\nwork architectures that have been reviewed in SECTION. EfﬁcientNet-B4 is simply an up-\\nscaled version of EfﬁcientNet-B0. It is deeper, wider, and takes a higher resolution input image.\\nEfﬁcientNet-B4 achieves a higher top-5 accuracy on ImageNet than EfﬁcientNet-B0, but both\\nare considered mobile network architectures. EfﬁcientNet-B0 is the most lightweight version\\nof the EfﬁcientNet family, whereas EfﬁcientNet-B4 seems to offer the best tradeoff between the\\nnumber of parameters and accuracy. Both networks used in this project will be pre-trained on\\nImageNet, and the only trainable parameters will be in three fully connected layers that use the\\nlearned visual features to predict the coordinates of the key points. In other words, all weights\\nassociated with the feature extraction part of the network will be frozen. Backbone network\\narchitectures that have been pre-trained on ImageNet have been demonstrated to be robust and\\nto generalize well. By comparing EfﬁcientNet-B0 to EfﬁcientNet-B4, two similar state-of-the-\\nart backbones can be compared, and the impact of compound scaling can be identiﬁed in the\\ncontext of regression-based key point and pose estimation models.\\n4.6.1 Environment\\nThe source code for EfﬁcientNet-B0 was written in TensorFlow 2, an open-source machine\\nlearning platform written in a combination of C++ and CUDA, making it extremely fast and of-\\n62'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 74}, page_content='fers GPU support. Furthermore, it is very easy to access and operate TensorFlow 2 using Python\\nlibraries. Since EfﬁcientNet was published, there has been an implementation of EfﬁcientNet-\\nB0 released on a high-level TensorFlow 2 API for deep learning called Keras. Keras is writ-\\nten in Python using TensorFlow 2 and allows for models such as VGG-16, ResNet-152, and\\nEfﬁcientNet-B0 in various applications. Therefore, TensorFlow 2 will be used to build both the\\ncustom model and EfﬁcientNet-B0. The models were built using Jupyter notebooks on a local\\nmachine and saved in a local directory with the data set.\\nA virtual environment called ‘Vast.ai’ was used for the key point regression models in this\\nproject. The Vast.ai instance gave dedicated access to a NVIDIA RTX 3090 24.3GB GPU and\\n32 GB of RAM, allowing for faster training times than on Google Colab. Furthermore, image\\nprocessing is easier using vast.ai than Google Colab pro as you can open Jupyter notebooks\\nas if they were on your local machine without having to interact with your Google Drive and\\nVirtual Machine, which was discovered after building the object detection models in this report.\\nFurthermore, vast.ai offers a ‘pay as you go’ system at approximately 0.50 cents per hour with\\ndedicated resource allocation, which made it both cheaper and faster than using Colab Pro.\\nThe data set and Jupyter notebook ﬁles of the models were uploaded to a GitHub repository,\\nwhich were then cloned to a Vast.ai instance. This was the fastest way of accessing the ﬁles on\\nmultiples instances as the download speed of the instance was approximately 85 ×faster than\\nthe upload speed of the local internet connection.\\n4.6.2 EfﬁcientNet-B0: Network Architecture\\nEfﬁcientNet-B0 is the baseline architecture designed by a multi-objective architecture search to\\noptimize both FLOPS and accuracy (Tan & Le 2019). The model used in this study was pre-\\ntrained on ImageNet for the 1000 class classiﬁcation challenge. In this study, the ﬁnal layers, or\\n‘top’, was removed and replaced with three fully connected layers, with the ﬁnal layer having\\nfour neurons to produce a 4-tuple of the key point coordinates previously deﬁned. The network\\nis designed to take input images that have a resolution of 224 ×224 with 3 input channels. The 3\\ninput channels are for RGB channels, which is common for coloured images to be represented.\\nEfﬁcientNet-B0 has a total of 8 million parameters of which 4 million are trainable. This is\\nbecause the weights have been frozen for stage 1-9 as these are the pre-trained feature extraction\\nlayers that have been pretrained on ImageNet. The 4 million trainable parameters are in layers\\n10-13 which are being trained to ﬁne-tune the network for key point estimation. The network\\nhas a total of 241 layers, the majority of which come from mobile inverted blocks which use\\nsqueeze and excitation blocks and a Swish activation function which are explained in Section\\n3.2.5. These blocks are repeated multiple times as indicated by Table 4.4, in which a high-\\nlevel overview of the network architecture has been given. MBConv1 represents the ﬁrst type\\nof mobile inverted block, further details can be found in articles by Agarwal (2020) and Borad\\n(2021).\\n63'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 75}, page_content='Stage Operator Input Resolution (height, width) Output Channels No. Blocks\\n1 Conv2D block, k ×3 224, 224 32 1\\n2 MBConv1, k3 ×3 112, 112 16 1\\n3 MBConv6, k3 ×3 112, 112 24 2\\n4 MBConv6, k5 ×5 56, 56 40 2\\n5 MBConv6, k3 ×3 28, 28 80 3\\n6 MBConv6, k5 ×5 14, 14 112 3\\n7 MBConv6, k5 ×5 14, 14 192 4\\n8 MBConv6, k3 ×3 7, 7 320 1\\n9 Conv2D block, k1 ×1 7, 7 1280 1\\n10 Flatten - 62,720 1\\n11 Fully Connected (Dense) - 64 1\\n12 Fully Connected (Dense) - 64 1\\n13 Fully Connected (Dense) - 4 1\\nTable 4.4: EfﬁcientNet-B0 Network Architecture with custom top for regression.\\n4.6.3 EfﬁcientNet-B4: Network Architecture\\nEfﬁcientNet-B4 follows a similar architecture to B0. It is the same baseline architecture that\\nhas been upscaled for resolution, width and depth. The input resolution for EfﬁcientNet-B4\\nis 380×380, whereas B0 is 224 ×224. Furthermore, it is demonstrated in the output channels\\ncolumns in Tables 4.4 and 4.5 that at each stage through the feature extraction stages in the\\nnetwork (prior to the ‘Flatten’ operator) that there are signiﬁcantly more output channels in B4\\ncompared to B0, meaning that B4 is wider than B0. Furthermore, there are a total of 477 layers\\nin B4, which comprise the multiple blocks summarised in Table 4.4. There are a total of 34.2\\nmillion parameters in this network, of which 16.5 million are trainable. There are approximately\\n4×more trainable parameters in B4 compared to B0 due to the network dealing with a greater\\nresolution and more output channels being outputted from the feature extractor. This is shown\\nin Stage 10 in Table 4.5, in which the ‘Flatten’ stage has over 4 ×the number of channels.\\n64'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 76}, page_content='Stage Operator Input Resolution (height, width) Output Channels No. Blocks\\n1 Conv2D block, k3 ×3 380, 380 48 1\\n2 MBConv1, k3 ×3 190, 190 24 2\\n3 MBConv6, k3 ×3 95, 95 32 4\\n4 MBConv6, k5 ×5 48, 48 56 4\\n5 MBConv6, k3 ×3 24, 24 112 6\\n6 MBConv6, k5 ×5 24, 24 160 6\\n7 MBConv6, k5 ×5 12, 12 272 8\\n8 MBConv6, k3 ×3 12, 12 448 2\\n9 Conv2D block, k1 ×1 12, 12 1792 1\\n10 Flatten - 258,048 1\\n11 Fully Connected (Dense) - 64 1\\n12 Fully Connected (Dense) - 64 1\\n13 Fully Connected (Dense) - 4 1\\nTable 4.5: EfﬁcientNet-B4 Network Architecture with custom top for regression.\\n4.6.4 Training\\nThe data set of 10,239 cropped images were split into 90% training images (9213 instances) and\\n10% validation images (1026 instances). To prevent the models from overﬁtting the training\\ndata, the keras.callbacks.EarlyStopping() method was used to stop the model from training if the\\nmodel did not decrease within the last 10 epochs. This is a regularization technique called early\\nstopping which is used to prevent the validation loss from increasing as in explained in Section\\n2.3.2. The number of neurons where hand tuned in layers 11 and 12 for both networks, however\\nthe validation performance was given for the ﬁnal conﬁgurations demonstrated in Tables 4.4 and\\n4.5. The learning rate was also varied but the ﬁnal learning rate used was 2×10−5.\\n4.6.5 Evaluation\\nThe mean square error was calculated at each epoch of training for both the training and vali-\\ndation data to compare the models. This can be used to calculate the root mean squared error,\\nwhich represents the mean pixel error for each model. Furthermore, the inference speed was\\nmeasured for each model by measuring the time taken to make predictions on the entire valida-\\ntion batch and dividing by the number of images in this data set. There is also a time-dependent\\nobjective of carrying out k-fold cross-validation on these models to increase the chance that the\\nmodel has been trained on every data point. Also, to ensure that any conclusions made are not\\ndue to a sampling generalization error caused by the training and validation split in the data.\\n65'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 77}, page_content='66'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 78}, page_content='Chapter 5\\nResults\\nThis chapter will present the results from the object detection model comparison, and the key\\npoint regression model comparison. A preferred model will be identiﬁed for each model, along\\nwith optimal weights and a justiﬁcation as to why it is the preferred model for this application.\\n5.1 Object Detection models for bounding box coordinates\\nFigure 5.1 shows the training loss and validation mAP for the two object detection models built\\nfor this research project, the left-hand ﬁgure is for YOLOv4-tiny, and the right-hand ﬁgure is\\nfor the full YOLOv4. The overall trend for the training loss for both models follows very sim-\\nilar behaviour. The training loss decreases rapidly for the ﬁrst 200-300 iterations; after that,\\nthe training loss decreases gradually. The training loss for YOLOv4-tiny seems to follow a\\nmuch smoother curve, and therefore the model seems to be indicating that the model is learning\\nthe features of the training set with a sufﬁciently small learning rate throughout training. Fur-\\nthermore, the ﬁnal training loss is 0.0343, which is lower than the target value of 0.05 set by\\nthe developer of YOLOv4-tiny, which suggests the model has successfully learned the features\\nwithin the training set (Bochkovskiy et al. 2020). For YOLOv4, on the other hand, the training\\nloss curve decreases less smoothly, and there is some ﬂuctuation indicated by the zig-zag ap-\\npearance of the blue curve. The ﬁnal training loss for YOLOv4 is 0.4277, which is signiﬁcantly\\nlower than the target of 3.0, which was discussed in Section 4.5.5 (Bochkovskiy et al. 2020).\\nMore importantly, the red line in Figure 5.1 represents the validation mAP which shows\\nhow well the model can make predictions on unseen data. Both YOLOv4 and YOLOv4-tiny\\nachieve an mAP of 100% indicated in Figure 5.1. The red curve starts at 1000 iterations, as this\\nis when the ﬁrst mAP calculation is performed throughout the training schedule. YOLOv4-tiny\\nappears to reach a higher mAP at this point at 99%, whereas YOLOv4 only achieves 66%. This\\nis unsurprising as YOLOv4 has signiﬁcantly more training parameters and therefore requires\\nmore iterations of training to reach the highest mAP for this data set. Fortunately, the best\\nweights are saved, which maximise the mAP allowing the early stopping point to be identiﬁed.\\n67'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 79}, page_content='Figure 5.1: Object Detection Models: Training loss and validation mAP . Left: YOLOv4-tiny.\\nRight: YOLOv4.\\nInterestingly, both models achieve the exact same maximum mAP of 99.9979%, which\\nmeans they identify ∼100 % of the hands in images. However, the maximum mAP and, there-\\nfore, the early stopping point occurs at a different number of iterations. YOLOv4-tiny reaches\\nmaximum mAP at 4093 iterations, whereas YOLOv4 reaches the maximum mAP at 1515 itera-\\ntions. This is interesting as mAP increased initially for YOLOv4 slower initially than YOLOv4-\\ntiny, which was understandable because it had more parameters and required more iterations\\nto learn the features of the training data. Therefore, it could be expected that YOLOv4-tiny\\nwould reach the maximum mAP faster than YOLOv4. However, this could be caused by a given\\ntraining image not being seen until later on in training by YOLOv4-tiny due to random shufﬂing\\nprior to training. Although the early stopping point has been identiﬁed for each model through\\nthe iteration with the highest mAP, the mAP remains to be really high and rounded to 100% for\\nboth models after this point. Therefore, it could be argued that early stopping is not essential\\nfor this data. It would be really useful if the developers of YOLOv4 plotted validation loss as\\nin their in-built functions, as this could allow further discussion about overﬁtting. However, it\\nis unlikely the model is overﬁtting as the mAP remains stable. YOLOv4 achieves a slightly\\nhigher average IOU of 85.06%, whereas YOLOv4-tiny achieves an average IOU of 84.07%,\\nwhich is a really small difference and is unlikely to impact detection performance largely. The\\ndifference in bounding box size for a given image is unlikely to be largely different, and both\\nmodels should be able to predict bounding box coordinates for hands exceptionally well. The\\nIOU is a metric for measuring the overlap between ground truth and predicted bounding boxes,\\n68'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 80}, page_content='whereas the AP considers the number of TP and FP as discussed in Section 2.2.\\nBoth models achieve very similar performance in maximum mAP and average IOU at the\\nearly stopping point as discussed, which means they should both make excellent predictions.\\nTherefore, to decide the preferred model for the ﬁnal pipeline, considerations need to be made\\nof the models’ inference speeds, memory requirements, and example predictions. YOLOv4\\nmakes predictions with a mean inference speed of 20.25 milliseconds per image which means\\nit can make predictions at 49.4 FPS on a Tesla P-100 16GB GPU. YOLOv4-tiny is much faster,\\nwith a mean inference speed of 3.03 milliseconds per image which means it can make predic-\\ntions at 330 FPS on the same GPU. Both models achieve real-time performance on a GPU, but\\nYOLOv4-tiny is signiﬁcantly faster whilst maintaining the same mAP and similar IOU. Based\\non this, YOLOv4-tiny is the preferred model. Predictions are shown in Figure 5.2, in which the\\nbounding boxes produced by both models seem very similar. Figure 5.2 left shows the prediction\\nmade by YOLOv4-tiny, which appears to be a tighter ﬁt than YOLOv4 on the right. Further-\\nmore, YOLOv4-tiny predicted this example with a conﬁdence of 0.99, whereas YOLOv4 has a\\nslightly lower conﬁdence of 0.97.\\nIn summary, both models perform a maximum mAP of 100% on validation data with a\\nsimilar average IOU of approximately 85%. However, YOLOv4-tiny has inference speeds ap-\\nproximately 7 times faster than YOLOv4. Therefore, the preferred model is YOLOv4-tiny.\\nFigure 5.2: Predictions made by object detection models. Left: YOLOv4-tiny. Right: YOLOv4.\\n5.2 Key Point Regression Models\\nFigure 5.3 shows the training loss and validation loss for the two key point regression mod-\\nels built for this research project, the left-hand plot is from training EfﬁcientNet-B0 and the\\nright-hand plot is from training EfﬁcientNet-B4. EfﬁcientNet-B0 takes a total of 73 epochs\\nto converge, which took approximately 45 minutes to train. Both the training and validation\\nloss curves decrease rapidly initially until about 25 epochs and then more gradually after this\\npoint. The model exhibits some variance indicated by the orange line following the same trend\\nas the blue line in the left plot. The ﬁnal training loss is 127.24 (MSE) which gives a pixel\\n69'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 81}, page_content='error of 11.28 by taking the square root of the MSE. Similarly, the ﬁnal validation loss was\\n179.76, which equates to a pixel error of 13.41. It was speculated that the human labelling\\nvariability was approximately 8 or 9 pixels and was justiﬁed in Section 4.4.2. Therefore, given\\nthat the model is able to make predictions on unseen validation data with a mean pixel error of\\n13.41, the model is performing well. Furthermore, the resolution of the image was resized from\\n1120×1120 down to 224×224, which means that the original image has been downsized by a\\nscale factor of 5. The labels on the other hand, were left on the original scale of 1120 ×1120\\nas this allows them to easily be converted back to the axis of the full 1080p image by the Co-\\nordinate Conversion Function. However, this means that every 1 pixel inputted into the image\\nrepresents a 5×5 pixel grid on the original image axis. Therefore, a labelled coordinate on the\\ndownsized image can represent 1 of 25 coordinates on the full-sized image. This is likely to\\nhave contributed to the training and validation error present in Figure 5.3, as the model cannot\\nlearn associate visual features in a lower resolution with coordinates on a higher resolution that\\nit has not been trained on.\\nFigure 5.3: Key Point Regression Models: Training loss and validation loss. Left: EfﬁcientNet-\\nB0. Right: EfﬁcientNet-B4.\\nSimilarly to B0, B4 begins to converge at approximately 25 epochs. However, for B4, both\\nloss curves continue to decrease until the early stopping point at 137 epochs. The ﬁnal training\\nloss is 49.30, which gives a mean pixel error of 7.02 px and the ﬁnal validation loss is 99.00,\\nwhich gives a mean pixel error of 9.95 px. As the training loss is 7.02 px, it is unlikely that\\nthe human variability in the data is approximately 8 or 9 pixels, as previously speculated. The\\ntrue human variability is likely to be a value ≤7.02 px as some additional loss is likely to be\\ncaused by the model and by rescaling the input image dimensions. The resolution of the image\\nwas resized from 1120 ×1120 to 380×380 as this is the input size deﬁned by the developers\\nfor EfﬁcientNet-B4, which means the original image has been downsized by a factor of ∼2.95.\\n70'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 82}, page_content='Therefore each pixel inputted to the model represents an approximate 3 ×3 grid on the axis of\\nthe original image and a labelled coordinate on the downsized image can represent 1 of 9 on the\\nfull-size image.\\nThe mean inference speed of B0 was 2.27 milliseconds per image, which is over 439 FPS\\non an NVIDIA RTX 3090 24.3GB GPU. For B4, on the other hand, the mean inference speed\\nwas 51.77 milliseconds per image, which is over 19 FPS on the same GPU. Therefore, B0 can\\nbe considered real-time, but B4 cannot be considered real-time on this GPU. The requirement\\nfor a model to be considered ‘real-time’ is having an FPS greater than 30 FPS. Furthermore, the\\nFPS is likely to be much slower for both models on a CPU, which may be an issue for mobile\\ndevices deployment.\\n5.2.1 Discussion\\nIt has been previously demonstrated that the mean pixel error using DeepLabCut on the same\\ndata set was 8.39 pixels using 1920 ×1080 resolution images (Zhao et al. 2020). EfﬁcientNet-B0\\nand B4 achieve a mean pixel error of 13.41 px and 9.95 px, respectively is quite remarkable con-\\nsidering that the models are making predictions using resized images. In summary, 1920 ×1080\\nresolution images of hands have been cropped to 1120 ×1120 images to remove excess back-\\nground pixels. The cropped image was downsized by a scale factor of 5 for EffcientNet-B0 and\\n∼2.95 for Efﬁcient-B4. Therefore, the quality of the data being inputted to the EfﬁcientNet mod-\\nels in this report contains less semantic visual details as multiple pixels have been condensed\\ninto single pixels. Further, given that the pixel error on validation data of the EfﬁcientNet mod-\\nels is not signiﬁcantly worse than using DeepLabCut due to making predictions on lower quality\\nimages suggests that EfﬁcientNet pre-trained on ImageNet is an excellent backbone for human\\nhand pose estimation tasks.\\nEfﬁcientNet-B0 achieves real-time inference speeds (439 FPS) on a GPU, whereas EfﬁcientNet-\\nB4 does not (19 FPS). EfﬁcientNet-B4 being nearly 23 ×slower is due to a large number of\\nweights in the fully-connected layers which are much slower to compute than fully convolu-\\ntional operations. The more accurate B4 model is slower, and the less accurate B0 model is\\nfaster. Labelling accuracy is likely to be the more important factor than inference speed at this\\nstage, as being able to produce an accurate time-series plot for the euclidean distance between\\ntip of the index ﬁnger and thumb is essential for classifying patients on the MDS-UPDRS. If the\\nkey points are not tracked accurately, then measurable tapping metrics such as rhythm, ampli-\\ntude and speed are likely to be negatively impacted by labelling error caused by the automatic\\nlabelling system. Therefore, the preferred key regression model for the ﬁnal ML pipeline is\\nEfﬁcientNet-B4 as it achieves a lower mean pixel error on the validation set of 9.95 px. Pre-\\ndictions on training and unseen data have been given in Appendix A.2 to allow for multiple\\npredictions to be visualised. On the vast majority of predictions, the predictions look very good.\\nSome predictions are slightly poorer when a hand is not in the ‘closed’ position.\\n71'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 83}, page_content='72'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 84}, page_content='Chapter 6\\nConclusion\\n6.1 Limitations of Study and Suggestions for Future Work\\nYOLOv4-tiny achieves a mean IOU of 84.07%, which is extremely good and certainly good\\nenough for localising hands in this application with a sufﬁcient level of accuracy. However,\\nthis could likely be improved by using higher quality data. The bounding box coordinate la-\\nbels were produced by adapting existing labelled key points using methods discussed in Section\\n4.4.3. As a result, often the bounding box labels did not fully enclose all digits of the hand\\nfor certain training samples. For other training samples, there were excess background pixels,\\ni.e. the bounding box ﬁt the hand ‘loosely’. Therefore, the performance of this model could be\\nimproved by manually labelling each frame of video to ensure that each bounding box encloses\\nthe entire hand and to ensure that each box does not contain excessive background pixels. Poor\\nbounding box labels likely served as a limiting factor for this project as they were not consistent\\nbetween frames.\\nFurthermore, YOLOv4 instructs users to train the model for at least as many iterations as\\ntraining images (Bochkovskiy et al. 2020). The training data set contained 8268 images but\\nwas only trained for 6000 iterations, which does not follow the instructions of Bochkovskiy\\net al. (2020). However, given the performance of a model, it is unlikely that further iterations\\nwould have improved the model’s performance. Lastly, the bounding box training labels were\\n768×768 in dimension, which was later discovered was too small to fully enclose all key points.\\nTherefore, the model should be retrained for bounding boxes with a larger dimension. Alter-\\nnatively, a 200-pixel buffer could be added to the height and the width of the bounding box\\ndimensions before passing the images into the cropping function.\\nThe key point regression model using EfﬁcientNet-B4 achieved a good mean pixel error\\nof 9.95 px however, this could be improved by potentially using a model that takes an input\\nimage of higher resolution. EfﬁcientNet-B4 downsized 1120 ×1120 images down to 380 ×380;\\ntherefore, each pixel in the input image represented an approximate 3 ×3 grid on the original\\n73'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 85}, page_content='image. This serves as a physical limit to how accurately the model can predict pixel coordi-\\nnates on the original image as much of the visual semantic information is lost by downsizing\\nthe image. Backbones that take a larger input, such as EfﬁcientNet-B7 (which takes an input\\nof 600×600), should be considered to investigate the proportion of the pixel error that is due to\\nan error caused by downsizing the image. However, this was demonstrated to some extent by\\ncomparing EfﬁcientNet-B0 and EfﬁcientNet-B4. Furthermore, if EfﬁcientNet-B7 did not result\\nin a signiﬁcantly lower pixel error, then further comments could be made about the contribution\\nof error due to the variability of human labelling.\\nFurthermore, the key point estimation model that used EfﬁcientNet-B4 achieved an infer-\\nence speed of 19 FPS which did not achieve real-time performance on an Nvidia RTX 3090.\\nTherefore would not be suitable for real-time deployment on mobile or embedded devices. The\\ninference speed of the network was limited by a bottleneck in the ‘top’ of network that is re-\\nsponsible for regression. As the convolutional layers were ﬂatted to a feature vector of 258,048\\nneurons which was passed on to the fully connected ‘top’ of the network, there were over 16\\nmillion operations performed to produce the 4-tuple of coordinates. Alternatively, the same pre-\\ntrained backbone could be used, but the problem could be treated as a classiﬁcation problem\\nrather than a regression problem. This would involve removing the fully connected layers that\\ncause the bottleneck in inference speed and replacing it with deconvolutional layers compared\\nwith ground truth heat maps. This would reduce the number of operations required in a forward\\npass and likely reduce the inference speed.\\nAdditionally, due to the constraints of this project, k-fold cross-validation was not carried\\nout. Therefore, a time-dependent objective was not met. K-fold cross-validation should be car-\\nried out in future studies to ensure the results in this study are robust and repeatable.\\nLastly, it is possible that the 9.95 mean pixel error associated with the ﬁnal model is likely\\nto be an overestimate of how accurate the model can predict key points. This is due to the\\n‘closed’ position, where the index ﬁnger and thumb are touching, being oversampled. Further,\\nas the two key points being predicted are touching, they occupy a very small area of pixels in\\nthe image space. Due to the proximity of the two key points, the mean squared error is likely to\\nbe lower for the ‘closed‘ position than when the index ﬁnger are not touching or at maximum\\namplitude. It is likely that the mean pixel error is higher for any open position compared to the\\nclosed position which was explored in Section 4.4.2.\\n6.2 Conclusion\\nThis project aimed to automate the labelling stage of the DeepLabCut workﬂow. Which for\\nthis application, takes an image of a hand as input and output labels for the coordinates of the\\n74'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 86}, page_content='tip of the index ﬁnger and thumb. The automatic labelling system was designed by reviewing\\nstate-of-the-art deep-learning-based computer vision algorithms which were selected for the ﬁ-\\nnal pipeline, to try and optimise accuracy and efﬁciency. YOLOv4 and YOLOv4-tiny were the\\nselected object detection algorithms used to detect and localise a hand in a 1080p frame. Both\\nYOLOv4 and YOLOv4-tiny achieved an mAP of 99.9979%. However, YOLOv4-tiny was se-\\nlected as the preferred model as the inference speed was nearly 7 ×faster. YOLOv4-tiny had a\\nmean IOU of 84.07%, which was less than 1% lower than YOLOv4. However, this was not an\\nissue as the model was able to produce bounding boxes that fully enclosed the hands containing\\nall key points of interest for tracking.\\nOnce the hand had been localized in the original image space using YOLOv4-tiny, a cropped\\nimage was passed into a key point estimation model. A regression-based approach was adopted\\nthat predicted a 4-tuple containing the (x,y) coordinates for the index ﬁnger and thumb tip.\\nEfﬁcientNet-B0 and EfﬁcientNet-B4 were compared as feature extractors, followed by two fully\\nconnected layers containing 64 neurons each and a ﬁnal fully connected layer with 4 neurons to\\nproduce the 4-tuple of predicted coordinates. On validation data, the mean pixel error was 13.41\\npx and 9.95 px for EfﬁcientNet-B0 and EfﬁcientNet-B4, respectively. A large proportion of this\\nerror is likely due to variability in human labelling in the training data and downsizing the input\\nimage. Downsizing the image reduces the visual semantic information present in the data, which\\nserves a physical constraint that even a perfect model could not overcome. As accuracy is of pri-\\nmary importance for the key point estimation model, EfﬁcientNet-B4 was the preferred model\\nand had an inference speed of 19 FPS which could not be considered ‘real-time’; however, it was\\nonly 11 FPS too slow. EfﬁcientNet-B0 achieved 439 FPS which is excellent; however, the er-\\nror of the model was greater, and therefore the EfﬁcientNet-B4 was selected for the ﬁnal system.\\nCode has not been explicitly given for the ﬁnal ‘Cropping Function’ and ‘Coordinate Con-\\nversion Function’ for the ﬁnal ML pipeline. However, a similar approach can be adopted that\\nwas used to create the data sets as explained in Section 4.4. The primary focus of this paper\\nwas to build the models for an automatic labelling system to label to coordinates of the tip of\\nthe index ﬁnger and thumb. The ﬁnal automatic labelling system pipeline for this report uses\\nYOLOv4-tiny for object detection and an EfﬁcientNet-B4 backbone pre-trained on ImageNet\\nwith a customized top for key point regression to predict coordinates for the tip of the index\\nﬁnger and thumb. The ﬁnal mean pixel error from the pipeline is 9.95 px which is excellent con-\\nsidering it has been demonstrated that DeepLabCut achieves 8.39 px on the same data set (Zhao\\net al. 2020).\\n75'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 87}, page_content='76'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 88}, page_content='Appendix A\\nAppendix\\nA.1 YOLOv4 and YOLOv4-tiny conﬁguration instructions\\nThe following changes need to be made to the cfg ﬁles named ‘yolov4-custom.cfg’ and ‘yolov4-\\ntiny.cfg’ that were available to download from the GitHub repository:\\n(https://github.com/AlexeyAB/darknet/tree/master/cfg) (Bochkovskiy et al. 2020).\\nThree colour channels were used inputted to both models, and both had a learning rate decay\\nof 0.0005 by default. There were some default differences in the cfg ﬁles that were not altered\\nas there were no instructions to do so, such as learning rate was 0.001 for YOLOv4 and 0.00261\\nfor YOLOv4-tiny. Furthermore, momentum was 0.949 for YOLOv4 and 0.9 for YOLOv4-tiny.\\nThe following instructions applied to both models however:\\n• batch=64,\\n• subdivisions=16,\\n• max batches=([number of]classes ×2000) but not less than 6000 or the number of training\\nimages. f.e. max batches=6000,\\n• change line steps to 80% and 90% of max batches, f.e. steps=4800, 5400\\n• set network size width =416, height =416\\n• change number of classes, f.e. classes=1\\n• change ﬁlters=255 to ﬁlters=(classes+5) ×in the [convolutional] layer before each [yolo]\\nhead. f.e. for 1 class, ﬁlters = 18.\\n77'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 89}, page_content='A.2 EfﬁcientNet-B4 Key point regression model predictions\\nFigure A.1: EfﬁcientNet-B4 Key point regression model: Predictions on Training Data\\n78'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 90}, page_content='Figure A.2: EfﬁcientNet-B4 Key point regression model: Predictions on Test Data\\n79'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 91}, page_content='80'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 92}, page_content='Bibliography\\nAgarwal, V . (2020), ‘Complete architectural details of all efﬁcientnet models’.\\nURL: https://towardsdatascience.com/complete-architectural-details-of-all-efﬁcientnet-\\nmodels-5fd5b736142\\nAlbawi, S., Mohammed, T. A. & Al-Zawi, S. (2017), Understanding of a convolutional neural\\nnetwork, in‘2017 International Conference on Engineering and Technology (ICET)’, Ieee,\\npp. 1–6.\\nAndriluka, M., Pishchulin, L., Gehler, P. & Schiele, B. (2014), 2d human pose estimation: New\\nbenchmark and state of the art analysis, in‘Proceedings of the IEEE Conference on computer\\nVision and Pattern Recognition’, pp. 3686–3693.\\nAyodele, T. O. (2010), ‘Types of machine learning algorithms’, New advances in machine learn-\\ning3, 19–48.\\nBhande, A. (2018), ‘What is underﬁtting and overﬁtting in machine learning and how to deal\\nwith it.’.\\nURL: https://medium.com/greyatom/what-is-underﬁtting-and-overﬁtting-in-machine-\\nlearning-and-how-to-deal-with-it-6803a989c76\\nBochkovskiy, A., Wang, C.-Y . & Liao, H.-Y . M. (2020), ‘Yolov4: Optimal speed and accuracy\\nof object detection’, arXiv preprint arXiv:2004.10934 .\\nBorad, A. (2021), ‘Image classiﬁcation with efﬁcientnet: Better performance with computa-\\ntional efﬁciency’.\\nURL: https://datamonje.medium.com/image-classiﬁcation-with-efﬁcientnet-better-\\nperformance-with-computational-efﬁciency-f480fdb00ac6\\nBrownlee, J. (2021), ‘Gentle introduction to the adam optimization algorithm for deep learning’.\\nURL: https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-\\nlearning/\\nCamomilla, V ., Dumas, R. & Cappozzo, A. (2017), ‘Human movement analysis: The soft tissue\\nartefact issue’, Journal of biomechanics 62, pp–1.\\n81'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 93}, page_content='Chen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K. & Yuille, A. L. (2017), ‘Deeplab: Se-\\nmantic image segmentation with deep convolutional nets, atrous convolution, and fully con-\\nnected crfs’, IEEE transactions on pattern analysis and machine intelligence 40(4), 834–848.\\nDai, S. & Wu, Y . (2008), Motion from blur, in‘2008 IEEE Conference on Computer Vision and\\nPattern Recognition’, IEEE, pp. 1–8.\\nDeepLizard (2021), ‘Convolutional neural networks (cnns) explained’.\\nURL: https://deeplizard.com/learn/video/YRhxdVk sIs\\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K. & Fei-Fei, L. (2009), Imagenet: A large-\\nscale hierarchical image database, in‘2009 IEEE conference on computer vision and pattern\\nrecognition’, Ieee, pp. 248–255.\\nDertat, A. (2017), ‘Applied deep learning - part 4: Convolutional neural networks’.\\nURL: https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-\\nnetworks-584bc134c1e2\\nDoshi, S. (2020), ‘Various optimization algorithms for training neural network’.\\nURL: https://towardsdatascience.com/optimizers-for-training-neural-network-\\n59450d71caf6\\nEsteva, A., Robicquet, A., Ramsundar, B., Kuleshov, V ., DePristo, M., Chou, K., Cui, C.,\\nCorrado, G., Thrun, S. & Dean, J. (2019), ‘A guide to deep learning in healthcare’, Nature\\nmedicine 25(1), 24–29.\\nGirshick, R. (2015), ‘Fast r-cnn. arxiv 2015’, arXiv preprint arXiv:1504.08083 .\\nGirshick, R., Donahue, J., Darrell, T. & Malik, J. (2014), Rich feature hierarchies for accu-\\nrate object detection and semantic segmentation, in‘Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition’, pp. 580–587.\\nGoetz, C. G., Tilley, B. C., Shaftman, S. R., Stebbins, G. T., Fahn, S., Martinez-Martin, P.,\\nPoewe, W., Sampaio, C., Stern, M. B., Dodel, R. et al. (2008), ‘Movement disorder society-\\nsponsored revision of the uniﬁed parkinson’s disease rating scale (mds-updrs): scale presen-\\ntation and clinimetric testing results’, Movement disorders: ofﬁcial journal of the Movement\\nDisorder Society 23(15), 2129–2170.\\nGomez, L. F., Morales, A., Orozco-Arroyave, J. R., Daza, R. & Fierrez, J. (2021), Improving\\nparkinson detection using dynamic features from evoked expressions in video, in‘Proceed-\\nings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition’, pp. 1562–\\n1570.\\nGoodfellow, I., Bengio, Y . & Courville, A. (2016), Deep learning , MIT press.\\n82'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 94}, page_content='Gu, J., Wang, Z., Kuen, J., Ma, L., Shahroudy, A., Shuai, B., Liu, T., Wang, X., Wang, G.,\\nCai, J. et al. (2018), ‘Recent advances in convolutional neural networks’, Pattern Recognition\\n77, 354–377.\\nHe, K., Girshick, R. & Doll ´ar, P. (2019), Rethinking imagenet pre-training, in‘Proceedings of\\nthe IEEE/CVF International Conference on Computer Vision’, pp. 4918–4927.\\nHe, K., Zhang, X., Ren, S. & Sun, J. (2015), ‘Spatial pyramid pooling in deep convolutional\\nnetworks for visual recognition’, IEEE transactions on pattern analysis and machine intelli-\\ngence 37(9), 1904–1916.\\nHe, K., Zhang, X., Ren, S. & Sun, J. (2016), Deep residual learning for image recognition, in\\n‘Proceedings of the IEEE conference on computer vision and pattern recognition’, pp. 770–\\n778.\\nHeldman, D. A., Giuffrida, J. P., Chen, R., Payne, M., Mazzella, F., Duker, A. P., Sahay, A.,\\nKim, S. J., Revilla, F. J. & Espay, A. J. (2011), ‘The modiﬁed bradykinesia rating scale for\\nparkinson’s disease: reliability and comparison with kinematic measures’, Movement Disor-\\nders 26(10), 1859–1863.\\nHu, J., Shen, L. & Sun, G. (2018), Squeeze-and-excitation networks, in‘Proceedings of the\\nIEEE conference on computer vision and pattern recognition’, pp. 7132–7141.\\nInsafutdinov, E., Pishchulin, L., Andres, B., Andriluka, M. & Schiele, B. (2016), Deepercut: A\\ndeeper, stronger, and faster multi-person pose estimation model, in‘European Conference on\\nComputer Vision’, Springer, pp. 34–50.\\nJames, M. (2017).\\nURL: https://www.i-programmer.info/news/202-number-crunching/11140-imagenet-\\ntraining-record-24-minutes.html\\nJiang, Z., Zhao, L., Li, S. & Jia, Y . (2020), ‘Real-time object detection method based on im-\\nproved yolov4-tiny’, arXiv preprint arXiv:2011.04244 .\\nJordan, M. I. & Mitchell, T. M. (2015), ‘Machine learning: Trends, perspectives, and prospects’,\\nScience 349(6245), 255–260.\\nKhan, T., Nyholm, D., Westin, J. & Dougherty, M. (2014), ‘A computer vision framework for\\nﬁnger-tapping evaluation in parkinson’s disease’, Artiﬁcial intelligence in medicine 60(1), 27–\\n40.\\nKingma, D. P. & Ba, J. (2014), ‘Adam: A method for stochastic optimization’, arXiv preprint\\narXiv:1412.6980 .\\n83'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 95}, page_content='Kornblith, S., Shlens, J. & Le, Q. V . (2019), Do better imagenet models transfer better?, in\\n‘Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition’,\\npp. 2661–2671.\\nKrizhevsky, A., Hinton, G. et al. (2009), ‘Learning multiple layers of features from tiny images’.\\nKrizhevsky, A., Sutskever, I. & Hinton, G. E. (2012), ‘Imagenet classiﬁcation with deep convo-\\nlutional neural networks’, Advances in neural information processing systems 25, 1097–1105.\\nKızrak, A. (2020), ‘Comparison of activation functions for deep neural networks’.\\nURL: https://towardsdatascience.com/comparison-of-activation-functions-for-deep-neural-\\nnetworks-706ac4284c8a\\nLeCun, Y ., Bengio, Y . & Hinton, G. (2015), ‘Deep learning’, nature 521(7553), 436–444.\\nLeCun, Y ., Boser, B., Denker, J., Henderson, D., Howard, R., Hubbard, W. & Jackel, L. (1989),\\n‘Handwritten digit recognition with a back-propagation network’, Advances in neural infor-\\nmation processing systems 2.\\nLeCun, Y ., Bottou, L., Bengio, Y . & Haffner, P. (1998), ‘Gradient-based learning applied to\\ndocument recognition’, Proceedings of the IEEE 86(11), 2278–2324.\\nLeCun, Y . & Cortes, C. (2010), ‘MNIST handwritten digit database’.\\nURL: http://yann.lecun.com/exdb/mnist/\\nLee, C. Y ., Kang, S. J., Hong, S.-K., Ma, H.-I., Lee, U. & Kim, Y . J. (2016), ‘A validation study\\nof a smartphone-based ﬁnger tapping application for quantitative assessment of bradykinesia\\nin parkinson’s disease’, PloS one 11(7), e0158852.\\nLi, Y ., Yang, S., Zhang, S., Wang, Z., Yang, W., Xia, S.-T. & Zhou, E. (2021), ‘Is 2d heatmap\\nrepresentation even necessary for human pose estimation?’, arXiv preprint arXiv:2107.03332\\n.\\nLin, T.-Y ., Doll ´ar, P., Girshick, R., He, K., Hariharan, B. & Belongie, S. (2017), Feature pyramid\\nnetworks for object detection, in‘Proceedings of the IEEE conference on computer vision and\\npattern recognition’, pp. 2117–2125.\\nLin, T.-Y ., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll ´ar, P. & Zitnick, C. L.\\n(2014), Microsoft coco: Common objects in context, in‘European conference on computer\\nvision’, Springer, pp. 740–755.\\nLiu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y . & Berg, A. C. (2016),\\nSsd: Single shot multibox detector, in‘European conference on computer vision’, Springer,\\npp. 21–37.\\n84'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 96}, page_content='Liu, Y ., Chen, J., Hu, C., Ma, Y ., Ge, D., Miao, S., Xue, Y . & Li, L. (2019), ‘Vision-based\\nmethod for automatic quantiﬁcation of parkinsonian bradykinesia’, IEEE Transactions on\\nNeural Systems and Rehabilitation Engineering 27(10), 1952–1961.\\nLu, L., Shin, Y ., Su, Y . & Karniadakis, G. E. (2019), ‘Dying relu and initialization: Theory and\\nnumerical examples’, arXiv preprint arXiv:1903.06733 .\\nMathis, A., Biasi, T., Schneider, S., Yuksekgonul, M., Rogers, B., Bethge, M. & Mathis, M. W.\\n(2021), Pretraining boosts out-of-domain robustness for pose estimation, in‘Proceedings of\\nthe IEEE/CVF Winter Conference on Applications of Computer Vision’, pp. 1859–1868.\\nMathis, A., Mamidanna, P., Cury, K. M., Abe, T., Murthy, V . N., Mathis, M. W. & Bethge,\\nM. (2018), ‘Deeplabcut: markerless pose estimation of user-deﬁned body parts with deep\\nlearning’, Nature neuroscience 21(9), 1281–1289.\\nMathis, A., Schneider, S., Lauer, J. & Mathis, M. W. (2020), ‘A primer on motion capture with\\ndeep learning: principles, pitfalls, and perspectives’, Neuron 108(1), 44–65.\\nMcCulloch, W. S. & Pitts, W. (1943), ‘A logical calculus of the ideas immanent in nervous\\nactivity’, The bulletin of mathematical biophysics 5(4), 115–133.\\nMiller, J. L. (2002), ‘Parkinson’s disease primer’, Geriatric nursing 23(2), 69–75.\\nMisra, D. (2019), ‘Mish: A self regularized non-monotonic neural activation function’, arXiv\\npreprint arXiv:1908.08681 4, 2.\\nMoeslund, T. B., Hilton, A. & Kr ¨uger, V . (2006), ‘A survey of advances in vision-based human\\nmotion capture and analysis’, Computer vision and image understanding 104(2-3), 90–126.\\nNath, T., Mathis, A., Chen, A. C., Patel, A., Bethge, M. & Mathis, M. W. (2019), ‘Using\\ndeeplabcut for 3d markerless pose estimation across species and behaviors’, Nature protocols\\n14(7), 2152–2176.\\nNeill, S. P. & Hashemi, M. R. (2018), ‘Ocean modelling for resource characterization’, Funda-\\nmentals of ocean renewable energy pp. 193–235.\\nNewell, A., Yang, K. & Deng, J. (2016), Stacked hourglass networks for human pose estimation,\\nin‘European conference on computer vision’, Springer, pp. 483–499.\\nNewzoo (2021), ‘Top countries by smartphone users’.\\nURL: https://newzoo.com/insights/rankings/top-countries-by-smartphone-penetration-and-\\nusers/\\nPaperswithCode (2021 a).\\nURL: https://paperswithcode.com/sota/image-classiﬁcation-on-imagenet\\n85'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 97}, page_content='PapersWithCode (2021 b), ‘Papers with code - browse the state-of-the-art in machine learning’.\\nURL: https://paperswithcode.com/sota\\nPapersWithCode (n.d.), ‘Papers with code - early stopping explained’.\\nURL: https://paperswithcode.com/method/early-stopping\\nPoppe, R. (2007), ‘Vision-based human motion analysis: An overview’, Computer vision and\\nimage understanding 108(1-2), 4–18.\\nRamachandran, P., Zoph, B. & Le, Q. V . (2017), ‘Searching for activation functions’, arXiv\\npreprint arXiv:1710.05941 .\\nRasamoelina, A. D., Adjailia, F. & Sin ˇc´ak, P. (2020), A review of activation function for artiﬁcial\\nneural network, in‘2020 IEEE 18th World Symposium on Applied Machine Intelligence and\\nInformatics (SAMI)’, IEEE, pp. 281–286.\\nRedmon, J. (2013–2016), ‘Darknet: Open source neural networks in c’,\\nhttp://pjreddie.com/darknet/.\\nRedmon, J., Divvala, S., Girshick, R. & Farhadi, A. (2016), You only look once: Uniﬁed, real-\\ntime object detection, in‘Proceedings of the IEEE conference on computer vision and pattern\\nrecognition’, pp. 779–788.\\nRedmon, J. & Farhadi, A. (2017), Yolo9000: better, faster, stronger, in‘Proceedings of the IEEE\\nconference on computer vision and pattern recognition’, pp. 7263–7271.\\nRedmon, J. & Farhadi, A. (2018), ‘Yolov3: An incremental improvement’, arXiv preprint\\narXiv:1804.02767 .\\nRen, S., He, K., Girshick, R. & Sun, J. (2015), ‘Faster r-cnn: Towards real-time object detection\\nwith region proposal networks’, Advances in neural information processing systems 28, 91–\\n99.\\nRidnik, T., Ben-Baruch, E., Noy, A. & Zelnik-Manor, L. (2021), ‘Imagenet-21k pretraining for\\nthe masses’, arXiv preprint arXiv:2104.10972 .\\nRiecick `y, A., Madaras, M., Piovarci, M. & Durikovic, R. (2018), Optical-inertial synchroniza-\\ntion of mocap suit with single camera setup for reliable position tracking., in‘VISIGRAPP\\n(1: GRAPP)’, pp. 40–47.\\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A.,\\nKhosla, A., Bernstein, M., Berg, A. C. & Fei-Fei, L. (2015), ‘ImageNet Large Scale Visual\\nRecognition Challenge’, International Journal of Computer Vision (IJCV) 115(3), 211–252.\\nRˇziˇcka, E., Krupi ˇcka, R., Z ´arubov ´a, K., Rusz, J., Jech, R. & Szab ´o, Z. (2016), ‘Tests of manual\\ndexterity and speed in parkinson’s disease: Not all measure the same’, Parkinsonism & related\\ndisorders 28, 118–123.\\n86'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 98}, page_content='Samuel, A. L. (1959), ‘Some studies in machine learning using the game of checkers’, IBM\\nJournal of research and development 3(3), 210–229.\\nSandler, M., Howard, A., Zhu, M., Zhmoginov, A. & Chen, L.-C. (2018), Mobilenetv2: Inverted\\nresiduals and linear bottlenecks, in‘Proceedings of the IEEE conference on computer vision\\nand pattern recognition’, pp. 4510–4520.\\nSano, Y ., Kandori, A., Shima, K., Yamaguchi, Y ., Tsuji, T., Noda, M., Higashikawa, F., Yokoe,\\nM. & Sakoda, S. (2016), ‘Quantifying parkinson’s disease ﬁnger-tapping severity by extract-\\ning and synthesizing ﬁnger motion properties’, Medical & biological engineering & comput-\\ning54(6), 953–965.\\nScherer, D., M ¨uller, A. & Behnke, S. (2010), Evaluation of pooling operations in convolu-\\ntional architectures for object recognition, in‘International conference on artiﬁcial neural\\nnetworks’, Springer, pp. 92–101.\\nShahid, A. H. & Singh, M. P. (2020), ‘A deep learning approach for prediction of parkinson’s\\ndisease progression’, Biomedical Engineering Letters 10(2), 227–239.\\nSibley, K. G., Girges, C., Hoque, E. & Foltynie, T. (2021), ‘Video-based analyses of parkinson’s\\ndisease severity: A brief review’, Journal of Parkinson’s Disease (Preprint), 1–11.\\nSimonyan, K. & Zisserman, A. (2014), ‘Very deep convolutional networks for large-scale image\\nrecognition’, arXiv preprint arXiv:1409.1556 .\\nSwapna (2021), ‘Convolutional neural network: Deep learning’.\\nURL: https://developersbreach.com/convolution-neural-network-deep-learning/\\nSzegedy, C., Liu, W., Jia, Y ., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke,\\nV . & Rabinovich, A. (2015), Going deeper with convolutions, in‘Proceedings of the IEEE\\nconference on computer vision and pattern recognition’, pp. 1–9.\\nTan, M. & Le, Q. (2019), Efﬁcientnet: Rethinking model scaling for convolutional neural net-\\nworks, in‘International Conference on Machine Learning’, PMLR, pp. 6105–6114.\\nTompson, J. J., Jain, A., LeCun, Y . & Bregler, C. (2014), ‘Joint training of a convolutional\\nnetwork and a graphical model for human pose estimation’, Advances in neural information\\nprocessing systems 27, 1799–1807.\\nUijlings, J. R., Van De Sande, K. E., Gevers, T. & Smeulders, A. W. (2013), ‘Selective search\\nfor object recognition’, International journal of computer vision 104(2), 154–171.\\nV oulodimos, A., Doulamis, N., Doulamis, A. & Protopapadakis, E. (2018), ‘Deep learning for\\ncomputer vision: A brief review’, Computational intelligence and neuroscience 2018 .\\n87'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf', 'page': 99}, page_content='Williams, S., Relton, S., Fang, H., Alty, J., Qahwaji, R., Graham, C. D. & Wong, D. C. (2020),\\n‘Supervised classiﬁcation of bradykinesia in parkinson’s disease from smartphone videos’,\\nArtiﬁcial Intelligence in Medicine 110, 101966.\\nWu, X., Sahoo, D. & Hoi, S. C. (2020), ‘Recent advances in deep learning for object detection’,\\nNeurocomputing 396, 39–64.\\nXiao, B., Wu, H. & Wei, Y . (2018), Simple baselines for human pose estimation and tracking,\\nin‘Proceedings of the European conference on computer vision (ECCV)’, pp. 466–481.\\nYin (2018), ‘A summary of neural network layers’.\\nURL: https://medium.com/machine-learning-for-li/different-convolutional-layers-\\n43dc146f4d0e\\nZhang, F., Bazarevsky, V ., Vakunov, A., Tkachenka, A., Sung, G., Chang, C.-L. & Grund-\\nmann, M. (2020), ‘Mediapipe hands: On-device real-time hand tracking’, arXiv preprint\\narXiv:2006.10214 .\\nZhao, Z., Williams, S., Hafeez, A., Wong, D., Relton, S., Fang, H. & Alty, J. (2020), ‘The\\ndiscerning eye of computer vision: Can it measure parkinson’s ﬁnger tap bradykinesia?’,\\nJournal of the Neurological Sciences 416, 117003.\\n88')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "chunk_size = 1000\n",
    "chunk_overlap = int(0.15*chunk_size)"
   ],
   "metadata": {
    "id": "8OTo6TqlVaDa",
    "ExecuteTime": {
     "end_time": "2024-08-16T06:31:21.719095Z",
     "start_time": "2024-08-16T06:31:21.699096Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=chunk_size,\n",
    "  chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "splits = splitter.split_documents(docs)"
   ],
   "metadata": {
    "collapsed": true,
    "id": "v1ShlG7yVcJu",
    "ExecuteTime": {
     "end_time": "2024-08-16T06:31:24.382272Z",
     "start_time": "2024-08-16T06:31:24.356238Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "from rag.ingestion import create_vector_store, storage_path\n",
    "from langchain_openai.chat_models import AzureChatOpenAI\n",
    "\n",
    "vector_store = create_vector_store(splits)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o4AZLmDgVlzY",
    "outputId": "8cc87e9e-a347-4a1b-afe2-19609ce11847",
    "ExecuteTime": {
     "end_time": "2024-08-16T06:35:18.079436Z",
     "start_time": "2024-08-16T06:34:51.607915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 281 chunks to chroma db\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "from rag.retriever import contextualCompressionRetriever, multiQueryRetriever\n",
    "\n",
    "retriever = multiQueryRetriever(db_storage_path=storage_path, embedding_function=embedding_function, llm=llm)"
   ],
   "metadata": {
    "id": "pcdUt5jeV41B",
    "ExecuteTime": {
     "end_time": "2024-08-16T06:47:01.138711Z",
     "start_time": "2024-08-16T06:47:01.117714Z"
    }
   },
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": [
    "retriever.get_relevant_documents(\"Parkinson\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5v0iIJsFXg4q",
    "outputId": "1175414d-6fe0-4655-de99-39c594ea5d96",
    "ExecuteTime": {
     "end_time": "2024-08-16T06:47:05.168050Z",
     "start_time": "2024-08-16T06:47:03.552630Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 56, 'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf'}, page_content='Each of the 40 Parkinson’s Patients had previously been diagnosed by a consultant neurol-\\nogist specialising in movement disorders at Leeds Teaching Hospitals NHS Trust. The Parkin-\\nson’s Patients were invited to attend a research clinic appointment speciﬁcally to gather this\\ndata set. Each of the diagnosed patients was subjectively and objectively in an ‘on’ motor state,\\nwhich means that they met the following criteria:\\n1. The patients reported that they felt ‘on’. This is a widely accepted and understood term\\nthat refers to patients ability to have an awareness that their medications are working\\neffectively and that the symptoms of their PD have been reduced by these medications.\\n2. The consultant neurologist reported that the patient looked ‘on’. This term is clinically\\naccepted that a trained specialist can observe a response in symptoms as a result of a\\nmedication taken by a PD patient.\\n3. The PD patient was currently taking their medication as prescribed and no medication'),\n",
       " Document(metadata={'page': 12, 'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf'}, page_content='shortage of trained neurologists, resulting in a decreased supply of clinicians to accurately di-\\nagnose PD. Furthermore, since the COVID-19 pandemic, many patients have been unable to\\ncarry out routine in-person assessments as PD patients are typically older and considered ‘high\\nrisk’ and have been formally advised to shield (Sibley et al. 2021). Therefore, there is an urgent\\nneed to develop new tools to objectively measure PD to enable effective widespread diagnosis,\\nmonitoring and future research into the disease (Zhao et al. 2020).\\nFor diagnosing PD, Williams et al. (2020) explains that a principal method used in current\\nclinical practice involves trained neurologists visually observing patients performing a tapping\\nexercise with their index ﬁnger and thumb. This method assesses for bradykinesia, which is\\na cardinal motor feature of the condition that is associated with the slowness of movement,'),\n",
       " Document(metadata={'page': 12, 'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf'}, page_content='Chapter 1\\nIntroduction\\nParkinson’s Disease (PD) is a progressive, complex neurodegenerative disorder that inhibits\\nmovement. Speciﬁcally, it is characterized by several primary motor symptoms, including\\nbradykinesia (slowness of movement), muscle rigidity, a resting tremor, postural instability.\\nWhile the exact cause of the disease is typically unknown, it can be caused by carbon monox-\\nide poisoning, strokes or even drug-induced. The disease is a result of genetic mutations that\\ncause brain cell degeneration, primarily in the midbrain region (Miller 2002). Approximately\\n10 million patients are affected by PD globally, and that the prevalence of this disease is in-\\ncreasing, resulting in an increased demand for trained neurologists to carry out ongoing motor\\nassessments of patients (Zhao et al. 2020, Shahid & Singh 2020). Additionally, there is a global\\nshortage of trained neurologists, resulting in a decreased supply of clinicians to accurately di-'),\n",
       " Document(metadata={'page': 56, 'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf'}, page_content='medication taken by a PD patient.\\n3. The PD patient was currently taking their medication as prescribed and no medication\\nhad been missed prior to recording.\\nControl participants were invited that were companions or family members of the PD pa-\\ntients, and these were typically the spouse of the patient. There were also additional control\\nparticipants that were staff members at Leeds General Inﬁrmary. All control patients had no\\nprevious neurological diagnosis and were not on any medication that could cause PD like symp-\\ntoms such as tremor, rigidity, bradykinesia or any other movement impairment.\\nThe remaining 73 videos used for this project were ﬁlmed using an integrated smartphone\\ncamera on an iPhone SE. The integrated video camera was conﬁgured to record at 60 FPS at\\n1920×1080 pixels (1080p). The smartphone was placed on a tripod, and no additional lighting\\nwas used to enhance the recording, only ambient lighting in the hospital room. Each of the'),\n",
       " Document(metadata={'page': 15, 'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf'}, page_content='Parkinson’s bradykinesia. Manual labelling serves as a barrier to the widespread clinical deploy-\\nment of DeepLabCut. Producing an automatic labelling system may result in the deployment of\\na quantiﬁable, deep-learning-based system for objectively measuring Parkinson’s bradykinesia.\\nCurrent clinical methods involve visually observing patients performing the exercise, which is\\ntherefore not quantiﬁed and highly subjective. Different clinicians will often classify the same\\n4'),\n",
       " Document(metadata={'page': 56, 'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf'}, page_content='4.3 Data Collection\\nThe data used in this project is a subset of a data set that was previously used in two previous\\npublications with authors from Leeds Institute of Health Sciences in partnership with researchers\\nfrom other universities (Zhao et al. 2020, Williams et al. 2020).The data that has been used for\\nthis project contains a total of 75 video recordings of human hands (right hands only). The\\nvideos were captured of 40 Parkinson’s patients, 20 control patients (typically the spouse of a\\ngiven Parkinson’s patient), and 15 additional control patients (either hospital staff or children).\\nEach of the participants had given written consent for their data to be used in the study. Two\\nvideos were rejected early in the project for all students using this data, as they were ﬁlmed at\\n30 FPS which would likely cause issues if they were to be used in training a DeepLabCut model.\\nEach of the 40 Parkinson’s Patients had previously been diagnosed by a consultant neurol-'),\n",
       " Document(metadata={'page': 57, 'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf'}, page_content='the Uniﬁed Parkinson’s Disease Rating Scale’ (MDS-UPDRS) (Goetz et al. 2008). The only\\nvisible body parts in the video frame were the hand and forearm. The distance between the\\nsmartphone camera and the patient’s arm was approximately 1m. However, this was not strictly\\ndeﬁned, and the distance does vary within the sample.\\nEach of the participants in the study was instructed to tap their index ﬁnger and thumb “as\\nquickly and as big as possible” for at least 10 seconds. Digits 1 and 2, the thumb and index\\nﬁnger, respectively, were closest to the camera to try and prevent self occlusions of the key\\npoints of interest. No explicit instructions were given to the patients about the required position\\nof digits 3 to 5. Although the researcher gave a brief demonstration to the patients in which digits\\n3 to 5 were fully extended, some patients performed the tapping exercise with digits 3 to 5 fully\\nretracted (as if digits 3 to 5 were forming part of a closed ﬁst). Each video was manually edited'),\n",
       " Document(metadata={'page': 0, 'source': 'C:\\\\Users\\\\Hammer\\\\PycharmProjects\\\\llm_adons\\\\data\\\\upload\\\\MACPHERSON, Callum - MSc Dissertation.pdf'}, page_content='Fully automated hand tracking for Parkinson’s Disease\\nDiagnosis\\nCallum Macpherson\\nStudent ID: 201022895\\nSupervised by Luisa Cutillo, Samuel Relton, and Hui Fang\\nSubmitted in accordance with the requirements for the\\nmodule MATH5872M: Dissertation in Data Science and Analytics\\nas part of the degree of\\nMaster of Science in Data Science and Analytics\\nThe University of Leeds, School of Mathematics\\nSeptember 2021\\nThe candidate conﬁrms that the work submitted is his/her own and that appropriate\\ncredit has been given where reference has been made to the work of others.')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "s8s2TPwvXw_J"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
